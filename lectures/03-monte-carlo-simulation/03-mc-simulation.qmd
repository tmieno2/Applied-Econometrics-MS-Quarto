---
title: "03: Monte Carlo Simulation"
format: 
  revealjs: 
    theme: [default, ../custom.scss]
    fontsize: 1.2em
    callout-icon: false
    scrollable: true
    echo: true
    fig-dpi: 400
    footer: "[back to course website with lecture slides](https://tmieno2.github.io/Applied-Econometrics-MS-Quarto/lectures/)"
execute:
  echo: true
  out-width: 80%
webr:
  packages: ['fixest', 'dplyr', 'data.table', 'ggplot2', 'modelsummary']
  cell-options:
    editor-font-scale: 0.8
filters:
  - webr
---

```{r}
#| include: false
library(dplyr)
library(data.table)
library(ggplot2)
```

# Monte Carlo Simulation: Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Monte Carlo Simulation: Introduction

::: {.panel-tabset}

### What is ti?

It is a way to test econometric theories via simulation.

<br>

**How is it used in econometrics?**

+ confirm ecoometric theory numerically
    - OLS estimators are unbiased if $E[u|x]=0$ along with other conditions (theory)
    - I know the above theory is right, but let's check if it is true numerically
+ You kind of sense that something in your data may cause problems, but there is no proven econometric theory about what's gonna happen (I used MC simulation for this purpose a lot)
+ assist students in understanding econometric theories by providing actual numbers instead of a series of Greek letters

### Question

Suppose you are interested in checking what happens to OLS estimators if $E[u|x]=0$ (the error term and $x$ are not correlated) is violated. 

:::{.callout-note title="Question"}
Can you use the real data to do this?
:::

<br>

<br>
<details>
  <summary>Answer</summary>
No because you will never observe either error term or true value of $\beta$s.
</details>


### Key part of MC simulations

<span style = "color: blue;"> You </span> generate data (you have control over how data are generated)

+ You know the true parameter unlike the real data generating process
+ You can change only the part that you want to change about data generating process and econometric methods with everything else fixed

:::
<!--end of panel-->

## Generating data

::: {.panel-tabset}

### RNG

**Pseudo random number generators (Pseudo RNG)**

Algorithms for generating a sequence of numbers whose properties <span style = "color: blue;"> approximate </span> the properties of sequences of random numbers

<br>

**Examples** 

Draw from a uniform distribution:

```{webr-r}
runif(5) # default is min=0 and max=1
```

<br>

```{webr-r}
x <- runif(10000)
hist(x)
```

### Pseudo?

Numbers drawn using pseudo random number generators are not truly random

+ What numbers you will get are pre-determined
+ What numbers you will get can be determined by setting a <span style = "color: red;"> seed </span>

**Demonstration**

```{webr-r}
set.seed(2387438)
runif(5)
```

<br>

**Question**

What benefits does setting a seed have?

### Normal Distribution

::: {.columns}

::: {.column width="50%"}
$x \sim N(0, 1)$
```{webr-r}
# default is mean = 0,sd = 1
x <- rnorm(10000)
hist(x)
```
:::
<!--end of the 1st column-->

::: {.column width="50%"}
$x \sim N(2, 2)$
```{webr-r}
# mean = 2, sd = 2
x <- rnorm(10000, mean = 2, sd = 2)
hist(x)
```

:::
<!--end of the 2nd column-->

:::
<!--end of the columns-->

:::
<!--end of panel-->

## R functions for often-used distributions  

::: {.panel-tabset}

### Distribution types

::: {.columns}

::: {.column width="50%"}
+ Normal
+ Uniform
+ Beta
+ Chi-square
+ F
+ Logistic
+ Log-normal
+ many others
:::
<!--end of the 1st column-->
::: {.column width="50%"}
For each distribution, you have four different kinds of functions:

+ <span style = "color: red;"> `d`</span>`norm`: density function
+ <span style = "color: red;"> `p`</span>`norm`: distribution function
+ <span style = "color: red;"> `q`</span>`norm`: quantile function
+ <span style = "color: red;"> `r`</span>`norm`: random draw
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->



### `dnorm()`

`dnorm(x)` gives you the height of the density function at $x$.

`dnorm(-1)` and `dnorm(2)`

```{webr-r}
#| context: output

x <- seq(-3, 3, length = 1000)

pdf <- dnorm(x)
plot_data <- data.table(y = pdf, x = x)

x <- seq(-3, 3, length = 1000)
pdf <- dnorm(x)
plot_data <- data.table(y = pdf, x = x)
ggplot() +
  geom_line(data = plot_data, aes(y = y, x = x), color = "red") +
  geom_point(
    data = data.table(y = dnorm(2), x = 2),
    aes(y = y, x = x),
    color = "blue", size = 2
  ) +
  annotate(
    "text",
    label = paste0("dnorm(-1) = ", round(dnorm(-1), digits = 2)),
    y = 0.25, x = -2,
    size = 3
  ) +
  geom_line(
    data = data.table(y = seq(0, dnorm(-1), length = 20), x = -1),
    aes(y = y, x = x),
    linetype = 2
  ) +
  geom_point(
    data = data.table(y = dnorm(-1), x = -1),
    aes(y = y, x = x),
    color = "blue", size = 2
  ) +
  annotate(
    "text",
    label = paste0("dnorm(2) = ", round(dnorm(2), digits = 2)),
    y = 0.1, x = 2.5,
    size = 3
  ) +
  geom_line(
    data = data.table(y = seq(0, dnorm(2), length = 20), x = 2),
    aes(y = y, x = x),
    linetype = 2
  ) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_bw()
```

### `pnorm()`

`pnorm(x)` gives you the probability that a single random draw is <span style = "color: red;"> less </span> than $x$.

::: {.panel-tabset}
#### pnorm(-1)

```{webr-r}
#| context: output
ggplot() +
  geom_line(data = plot_data, aes(y = y, x = x), color = "red") +
  geom_ribbon(
    data = plot_data[x <= -1, ],
    aes(ymax = y, ymin = 0, x = x),
    fill = "blue",
    alpha = 0.4
  ) +
  annotate(
    "text",
    label = paste0("pnorm(-1) = ", round(pnorm(-1), digits = 2)),
    y = 0.05, x = 0,
    size = 3
  )
```

#### pnorm(2)

```{webr-r}
#| context: output
ggplot() +
  geom_line(data = plot_data, aes(y = y, x = x), color = "red") +
  geom_ribbon(
    data = plot_data[x <= 2, ],
    aes(ymax = y, ymin = 0, x = x),
    fill = "blue",
    alpha = 0.4
  ) +
  annotate(
    "text",
    label = paste0("pnorm(2) = ", round(pnorm(2), digits = 2)),
    y = 0.05, x = 0,
    size = 3
  )
```

#### Exercise

What is the probability that a single random draw from a Normal distribution with `mean = 1` and `sd = 2` is less than 1?

<br>

**Work here**

```{webr-r}

```

<br>

**Answer**

```{r}
#| eval: false
pnorm(1, mean = 1, sd = 2) 
```

:::
<!--end of panel-->

### `qnorm()`

::: {.panel-tabset}

#### What is it?

`qnorm(x)`, where $0 < x < 1$, gives you a number $\pi$, where the probability of observing a number from a single random draw is less than $\pi$ with probability of $x$. 

We call the output of `qnorm(x)`, $x%$ quantile of the standard Normal distribution (because the default is `mean = 0` and `sd = 1` for `rnorm()`). 

#### qnorm(0.95)

```{webr-r}
#| context: output
 
x <- seq(-3, 3, length = 1000)
pdf <- dnorm(x)
plot_data <- data.table(y = pdf, x = x)
ggplot() +
  geom_line(data = plot_data, aes(y = y, x = x), color = "red") +
  geom_ribbon(data = plot_data[x < 1.64, ], aes(ymax = y, ymin = 0, x = x), fill = "green", alpha = 0.3) +
  annotate(
    "text",
    label = "qnorm(0.95)=1.64",
    x = 2.2, y = 0.3,
    size = 3
  ) +
  geom_point(aes(y = 0, x = 1.64)) +
  annotate(
    "text",
    label = "1.64",
    x = 1.64, y = 0.03,
    size = 3
  ) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0)
```

#### Exercise

What is the 88% quantile of Normal distribution with `mean = 0` and `sd = 9`?

::: {.panel-tabset}
### Work here
```{webr-r}

```

### Answer
```{r, eval = FALSE}
#| code-fold: true
qnorm(0.88, mean = 0, sd = 9)
```

:::

:::
<!--end of panel-->

:::
<!--end of panel-->

# Monte Carlo Simulation: Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Monte Carlo Simulation: Steps

+ specify the data generating process
+ generate data based on the data generating process
+ get an estimate based on the generated data (e.g. OLS, mean)
+ repeat the above steps many many times
+ compare your estimates with the true parameter

**Question**

Why do the steps $1-3$ many many times?

## Monte Carlo Simulation: Example 1

::: {.panel-tabset}

### Problem

:::{.callout-note title="Question"}
Is sample mean really an unbiased estimator of the expected value? 
:::

<br>

That is, is $E[\frac{1}{n}\sum_{i=1}^n x_i] = E[x]$, where $x_i$ is an independent random draw from the same distribution,

### Steps 1-3

```{webr-r}
#--- steps 1 and 2:  ---#
# specify the data generating process and generate data
x <- runif(100) # Here, E[x]=0.5

#--- step 3 ---#
# calculate sample mean
mean_x <- mean(x)
mean_x
```

### Sample Mean: Step 4

+ repeat the above steps many times
+ We use a <span style = "color: blue;"> loop </span> to do the same (similar) thing over and over again

### Loop: for loop 

**R code**

```{webr-r}
#--- the number of iterations ---#
B <- 1000

#--- repeat steps 1-3 B times ---#
for (i in 1:B) {
  print(i) # print i
}
```

<br>

**Verbally**

For each of $i$ in $1:B$ $(1, 2, \dots, 1000)$, do `print(i)`.

+ `i` takes the value of 1, and then `print(1)`
+ `i` takes the value of 2, and then `print(2)`
+ ...
+ `i` takes the value of 999, and then `print(999)`
+ `i` takes the value of 1000, and then `print(1000)`

### Step 4

::: {.columns}

::: {.column width="80%"}
```{webr-r}
#| autorun: true
 
#--- the number of iterations ---#
B <- 1000

#--- create a storage that stores estimates ---#
estimate_storage_mean <- rep(0, B)

#--- repeat steps 1-3 B times ---#
for (i in 1:B) {
  #--- steps 1 and 2:  ---#
  # specify the data generating process and generate data
  x <- runif(100) # Here, E[x]=0.5

  #--- step 3 ---#
  # calculate sample mean
  mean_x <- mean(x)
  estimate_storage_mean[i] <- mean_x
}
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


### Step 5

Compare your estimates with the true parameter

```{webr-r}
mean(estimate_storage_mean)
hist(estimate_storage_mean)
```

:::
<!--end of panel-->

## Monte Carlo Simulation: Example 2

::: {.panel-tabset}

### Question

:::{.callout-note title="Question"}
What happens to $\beta_1$ if $E[u|x]\ne 0$ when estimating $y=\beta_0+\beta_1 x + u$?
:::

### R code

::: {.columns}

::: {.column width="80%"}
```{webr-r}
#--- Preparation ---#
B <- 1000 # the number of iterations
N <- 100 # sample size
estimate_storage <- rep(0, B) # estimates storage

#--- repeat steps 1-3 B times ---#
for (i in 1:B) {
  #--- steps 1 and 2:  ---#
  mu <- rnorm(N) # the common term shared by both x and u
  x <- rnorm(N) + mu # independent variable
  u <- rnorm(N) + mu # error
  y <- 1 + x + u # dependent variable
  data <- data.frame(y = y, x = x)

  #--- OLS ---#
  reg <- fixest::feols(y ~ x, data = data) # OLS
  estimate_storage[i] <- reg$coefficient["x"]
}
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


### Compare

```{webr-r}
hist(estimate_storage)
```
:::
<!--end of panel-->


## Monte Carlo Simulation: Example 3 (optional)

::: {.panel-tabset}

### Question 

**Model**

\begin{aligned}
    y = \beta_0 + \beta_1 x + u \\
\end{aligned}

+ $x\sim N(0,1)$
+ $u\sim N(0,1)$
+ $E[u|x]=0$

<br>

**Variance of the OLS estimator**

True Variance of $\hat{\beta_1}$: $V(\hat{\beta_1}) = \frac{\sigma^2}{\sum_{i=1}^n (x_i-\bar{x})^2} = \frac{\sigma^2}{SST_X}$

Its estimator: $\widehat{V(\hat{\beta_1})} =\frac{\hat{\sigma}^2}{SST_X} = \frac{\sum_{i=1}^n \hat{u}_i^2}{n-2} \times \frac{1}{SST_X}$

<br>

**Question**

Does the estimator really work? (Is it unbiased?)

### R code

```{r}
#| include: false

# Note: this code is for generating SST_X, which is referred to in the presentation later
set.seed(903478)

#--- Preparation ---#
B <- 10000 # the number of iterations
N <- 100 # sample size
beta_storage <- rep(0, B) # estimates storage for beta
V_beta_storage <- rep(0, B) # estimates storage for V(beta)
x <- rnorm(N) # x values are the same for every iteration
SST_X <- sum((x - mean(x))^2)
```

::: {.columns}

::: {.column width="80%"}
```{webr-r}
set.seed(903478)

#--- Preparation ---#
B <- 10000 # the number of iterations
N <- 100 # sample size
beta_storage <- rep(0, B) # estimates storage for beta
V_beta_storage <- rep(0, B) # estimates storage for V(beta)
x <- rnorm(N) # x values are the same for every iteration
SST_X <- sum((x - mean(x))^2)

#--- repeat steps 1-3 B times ---#
for (i in 1:B) {
  #--- steps 1 and 2:  ---#
  u <- 2 * rnorm(N) # error
  y <- 1 + x + u # dependent variable
  data <- data.frame(y = y, x = x)

  #--- OLS ---#
  reg <- fixest::feols(y ~ x, data = data) # OLS
  beta_storage[i] <- reg$coefficient["x"]
  #* store estimated variance of beta_1_hat
  V_beta_storage[i] <- vcov(reg)["x", "x"]
}
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

### Check

**True Variance**

+ $SST_X = `r round(SST_X, digits = 2)`$
+ $\sigma^2 = 4$

$$V(\hat{\beta}) = 4/`r round(SST_X, digits = 2)` = `r round(4/SST_X, digits = 4)`$$

<br>

**Check**

Your Estimates of Variance of $\hat{\beta_1}$?

```{webr-r}
mean(V_beta_storage)
```

### Distribution

```{webr-r}
ggplot(data = data.frame(x = V_beta_storage)) +
  geom_density(aes(x = x)) +
  geom_vline(xintercept = round(4 / SST_X, digits = 4))
```
:::
<!--end of panel-->


## Exercise (optional)

::: {.panel-tabset}

### Problem

Using MC simulations, find out how the variation in $x$ affects the OLS estimators

<br>

**Model setup**

```{=tex}
\begin{align}
  y = \beta_0 + \beta_1 x_1 + u \\
  y = \beta_0 + \beta_1 x_2 + u
\end{align}
```

+ $x_1\sim N(0,1)$ and $x_2\sim N(0,9)$
+ $u\sim N(0,1)$
+ $E[u_1|x]=0$ and $E[u_2|x]=0$

### Solution

::: {.columns}

::: {.column width="80%"}
```{webr-r}
#--- Preparation ---#
B <- 1000 # the number of iterations
N <- 100 # sample size
estimate_storage <- matrix(0, B, 2) # estimates storage

for (i in 1:B) {
  #--- generate data ---#
  x_1 <- rnorm(N, sd = 1) # indep var 1
  x_2 <- rnorm(N, sd = 3) # indep var 2
  u <- rnorm(N) # error
  y_1 <- 1 + x_1 + u # dependent variable 1
  y_2 <- 1 + x_2 + u # dependent variable 2
  data <- data.table::data.table(y_1 = y_1, y_2 = y_2, x_1 = x_1, x_2 = x_2)

  #--- OLS ---#
  reg_1 <- fixest::feols(y_1 ~ x_1, data = data) # OLS
  reg_2 <- fixest::feols(y_2 ~ x_2, data = data) # OLS

  #--- store coef estimates ---#
  estimate_storage[i, 1] <- reg_1$coefficient["x_1"] # equation 1
  estimate_storage[i, 2] <- reg_2$coefficient["x_2"] # equation 2
}

#--- assign new names ---#
beta_1s <- estimate_storage[, 1]
beta_2s <- estimate_storage[, 2]

#--- mean ---#
mean(beta_1s)
mean(beta_2s)

#--- sd ---#
sd(beta_1s)
sd(beta_2s)
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


### Results visualization

```{webr-r}

plot_data_1 <- data.table(x = beta_1s, type = "Equation 1")
plot_data_2 <- data.table(x = beta_2s, type = "Equation 2")
plot_data <- rbind(plot_data_1, plot_data_2)
ggplot(data = plot_data) +
  geom_density(aes(x = x, fill = type), alpha = 0.5) +
  scale_fill_discrete(name = "") +
  xlab("Coefficient Estimate") +
  theme(
    legend.position = "bottom"
  )
```
:::
<!--end of panel-->




