---
title: "01-1: Univariate Regression: Introduction"
format: 
  revealjs: 
    theme: [default, ../custom.scss]
    fontsize: 1.2em
    callout-icon: false
    scrollable: true
    echo: true
    fig-dpi: 400
    footer: "[back to course website with lecture slides](https://tmieno2.github.io/Applied-Econometrics-MS-Quarto/lectures/)"
---

```{r knitr-setup, }
#| include: false
#| cache: false
library(data.table)
library(fixest)
library(flextable)
library(dplyr)
library(ggplot2)

theme_lecture <- 
  theme_bw() +
  theme(
    axis.text = element_text(size = 20),
    axis.title = element_text(size = 20),
    legend.text = element_text(size = 16),
    legend.title = element_text(size = 16)
  )
```

# Univariate Regression: Introduction

<html><div style='float:left'></div><hr color='#EB811B' size=2px width=1280px></html>

## Population, Sample, and Econometrics

::: {.panel-tabset}
### Population

:::{.callout-important title=Definition}
A set of $ALL$ individuals, items, phenomenon, that you are interested in learning about 
:::

<br>

**Example**

+ Suppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.
+ Suppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.

**Important**

Population differs depending on the scope of your interest

+ If you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world

+ If you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.

### Sample

:::{.callout-important title=Definition}
Sample is a subset of population that you observe
:::

::: {.panel-tabset}
### Case 1

+ Population: you are interested in the impact of education on wage 
+ Sample (example): data on education, income, and many other things for 300 individuals from each State 

:::{.callout-note title=Question}
Is the sample representative of the population?
:::

### Case 2

+ Population: you are interested in the impact of water price on irrigation by farmers in Nebraska
+ Sample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)

:::{.callout-note title=Question}
Is the sample representative of the population?
:::

:::
<!--end of panel-->

:::
<!--end of panel--> 

## Simple univariate model

::: {.panel-tabset}

### What is it?

Consider a phenomenon in the population that is correctly represented by the following model (<span style = "color: blue;"> This is the model you want to learn about using sample </span>):

```{=tex}
\begin{equation}
y=\beta_0+\beta_1 x + u 
\end{equation}
```

+ $y$: to be explained by $x$ (<span style = "color: blue;"> dependent variable</span>)
+ $x$: explain $y$ (<span style = "color: blue;"> independent variable </span>, <span style = "color: blue;"> covariate </span>, <span style = "color: blue;"> explanatory variable </span>)
+ $u$: parts of $y$ that cannot be explained by $x$ (<span style = "color: blue;"> error term </span>)
+ $\beta_0$ and $\beta_1$: real numbers that gives the model a quantitative meaning (<span style = "color: blue;"> parameters </span>)

::: {.fragment}
:::{.callout-important}
You will never know the **true** model. You can try estimating it using sample! That is what statistics is about. 
:::
:::
<!--end of the fragment-->

### What does $\beta_1$ measure?

```{=tex}
\begin{align}
y=\beta_0+\beta_1 x + u 
\end{align}
```

If you change $x$ by $1$ unit while holding $u$ (everything else) constant,


```{=tex}
\begin{align}
  y_{before} & = \beta_0+\beta_1 x + u \\
  y_{after} & = \beta_0+\beta_1 (x + 1) + u 
\end{align}
```

The difference in $y_{before}$ and $y_{after}$,

```{=tex}
\begin{align}
  \Delta y = \beta_1
\end{align}
```

That is, $y$ changes by $\beta_1$.

::: {.fragment}
:::{.callout-note title=So,}
+ $\beta_1$ is the change in $y$ when $x$ increases by 1
+ We call $\beta_1$ the <span style = "color: blue;"> ceteris paribus </span> (with everything else fixed) causal impact of $x$ on $y$.
:::
:::
<!--end of the fragment-->

### What does $\beta_0$ measure?

```{=tex}
\begin{align}
y=\beta_0+\beta_1 x + u 
\end{align}
```

When $x = 0$ and $u=0$,

```{=tex}
\begin{align}
y=\beta_0
\end{align}
```

So, $\beta_0$ represents the intercept.

### Visualized

::: {.columns}

::: {.column width="30%"}
<br>

+ $\beta_0$: intercept
+ $\beta_1$: coefficient (slope)

:::
<!--end of the 1st column-->
::: {.column width="70%"}
```{r out.width="100%"}
#| fig-height: 8
#| echo: false
x <- seq(0, 5, length = 1000)
y <- 1 + 2 * x
(
g_1 <- 
  ggplot(data = data.table(y = y, x = x)) +
  geom_line(aes(y = y, x = x)) +
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = 0) +
  theme_lecture
)
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

:::
<!--end of panel-->


```{r data-gen-1}
#| include: false
set.seed(9947398)
N <- 500
ab_A <- pmax(rnorm(N) * 1.5 + 7, 0)
ab_B <- pmax(rnorm(N) * 1.2 + 5, 0)
inc_A <- 60 + 10 * ab_A
inc_B <- 40 + 10 * ab_B
inc_A_e <- inc_A + 30 * rnorm(N)
inc_B_e <- inc_B + 30 * rnorm(N)
mean_inc_A <- mean(inc_A_e)
mean_inc_B <- mean(inc_B_e)
data_A <- data.table(x = ab_A, y = inc_A_e, yline = inc_A, University = "A")
data_B <- data.table(x = ab_B, y = inc_B_e, yline = inc_B, University = "B")
data <- rbind(data_A, data_B)
```


## Why do we want <span style = "color: blue;"> ceteris paribus </span> causal impact?

::: {.panel-tabset}

### Example 

**Quality of College**

You

+ have been admitted to University A (better, more expensive) and B (worse, less expensive)
+ are trying to decide which school to attend
+ are interested in knowing a boost in your future income to make a decision

**You have found the following data**

```{r}
#| echo: false
inc_table <-
  data[, .(
    `average income` = mean(yline) %>% round(digits = 2),
    `sample size` = .N
  ),
  by = University
  ]

flextable(inc_table) %>%
  autofit()
```

**Question**

Should you assume that the observed difference of `r round(mean(inc_A) - mean(inc_B), digits = 2)` is the expected boost **you** would get if you are to attend University A instead of B?

### Why ceteris paribus impact?

Let's say your ability score is $6$ out of $10$ (the higher, the better),

$$\mbox{(1)}\;\; E[inc|A,ability=9] -E[inc|B,ability=6]$$
$$\mbox{(2)}\;\; E[inc|A,ability=6] -E[inc|B,ability=6]$$

Which one would like you to know?

::: {.fragment}
:::{.callout-important title=""}
+ You want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A

+ You do not want the impact of school quality to be confounded with something else
:::
:::
<!--end of the fragment-->

:::{.callout-note title="Aside: Conditional Expectation"}
$E[Y|X]$ represents expected value of $Y$ conditional on $X$ (For a given value of $X$, the expected value of $Y$).
:::


### What do you observe?

:::{.callout-note title=""}
+ red line: $E[income|A, ability]$
+ blue line: $E[income|B, ability]$
:::

```{r cp, out.width = "100%"}
#| echo: false
g_CI <-
  ggplot(data = data) +
  geom_point(aes(x = x, y = y, color = University), size = 0.4) +
  geom_smooth(aes(x = x, y = yline, color = University), method = "lm", se = FALSE, size = 0.8) +
  geom_hline(yintercept = mean_inc_A, color = "red") +
  geom_hline(yintercept = mean_inc_B, color = "blue") +
  annotate("text", x = 1, y = 150, label = paste(round(mean_inc_A, 0), sep = ""), family = "Times", color = "red") +
  annotate("text", x = 1, y = 70, label = paste(round(mean_inc_B, 0), sep = ""), family = "Times", color = "blue") +
  xlab("ability") +
  ylab("Annual Income") +
  theme_lecture +
  theme(
    legend.position = "bottom"
  )

g_CI
```

:::
<!--end of panel-->

## Example: corn yield and fertilizer

::: {.panel-tabset}

### Model

**Corn yield and fertilizer**

```{=tex}
\begin{align}
  yield=\beta_0+\beta_1 fertilizer+u 
\end{align}
```

**Question**

What is in the error term?

### Estimate $\beta_1$

```{=tex}
\begin{align}
  yield=\beta_0+\beta_1 fertilizer+u 
\end{align}
```

+ you do not know $\beta_0$ and $\beta_1$, and would like to estimate them
+ you observe a series of $\{yield_i,fertilizer_i\}$ combinations $(i=1,\dots,n)$
+ you would like to estiamte $\beta_1$, the impact of fertilizer on yield, **ceteris paribus** (with everything else fixed)

**Question**

How could we possibly find the **ceteris paribus** impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?

### Crucial condition

It turns out we can identify the ceteris paribus causal impact of $x$ on $y$ as long as the following condition is satisfied:

:::{.callout-important title="Zero conditional mean"}
$E(u|x) = 0$
:::

This is satisfied when $E[u|x]=E[u]$ and $E[u] = 0$. Practically (and roughtly) speaking, this condition is satisfied if


:::{.callout-important}
+ <span style = "color: blue;"> the error term ($u$) is not correlated with $x$ </span>  
+ an intercept ($\beta_0$) is included in the model (which we almost always do by default)
:::

### Condition satisfied?

**Model**

```{=tex}
\begin{align}
  yield=\beta_0+\beta_1 fertilizer + u 
\end{align}
```

<br>

**Data**

You have collected farm-level yield-fertilizer data from 200 farmers in year 2023.

<br>

**Questions**

+ What's in $u$? (note that factors that do not affect yield are not part of $u$)
+ Is it correlated with fertilizer?


### Math asides

::: {.panel-tabset}
#### Mean independence

:::{.callout-note title="Definition: Mean Independence"}
$E[u|x]=E[u]$
:::

+ **verbally**: the average value of the error term (collection of all the unobservables) is the same at any value of $x$, and that the common average is equal to the average of $u$ over the entire population

+ **(almost) interchangeably**: the error term is not correlated with $x$

#### Correlation and mean independence

Mean independence of $u$ and $x$ implies no correlation. But, no correlation does not imply mean independence.

```{=tex}
\begin{aligned}
    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\
    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\
    = & E[ux] \\\\
    = & E_x[E_u[u|x]] \;\; \mbox{(iterated law of expectation)}
\end{aligned}
```

If zero conditional mean condition $(E(u|x)=0)$ is satisfied,

```{=tex}
\begin{aligned}
    Cov(u,x)= & E_x[0] = 0
\end{aligned}
```

#### $E(u)=0$

Expected value of the error term is 0 $(E(u)=0)$.

This is always satisfied as long as an intercept is included in the model:

$$y = \beta_0 + \beta_1 x + u_1,\;\; \mbox{where}\;\; E(u_1)=\alpha$$

Rewriting the model,

```{=tex}
\begin{aligned}
y & = \beta_0 + \alpha + \beta_1 x + u_1 - \alpha \\\\
  & = \gamma_0 + \beta_1 x + u_2
\end{aligned}
```

where, $\gamma_0=\beta_0+\alpha$ and $u_2=u_1-\alpha$. 

Now, $E[u_2]=0$.
:::
<!--end of panel-->

:::
<!--end of panel-->


```{r college_income_data}
#| echo: false
set.seed(38943)
N <- 1000
college_effect <- 20
ab_A <- pmax(rnorm(N) * 1.1 + 5, 0)
ab_B <- pmax(rnorm(N) * 1.1 + 5, 0)
inc_A <- 40 + college_effect + 10 * ab_A
inc_B <- 40 + 10 * ab_B
inc_A_e <- inc_A + 20 * rnorm(N)
inc_B_e <- inc_B + 20 * rnorm(N)
mean_inc_A <- mean(inc_A_e)
mean_inc_B <- mean(inc_B_e)
data_A <- data.table(x = ab_A, y = inc_A_e, yline = inc_A, University = "A")
data_B <- data.table(x = ab_B, y = inc_B_e, yline = inc_B, University = "B")
data_cI <- rbind(data_A, data_B)
```


## Going back to the college-income example

**The model**

$$
Income = \beta_0+\beta_1 College\;\; A + u
$$

where $College\;\; A$ is 1 if attending college A, 0 if attending college B, and $u$ is the error term that includes ability. $u$ includes ability.

<br>

**Zero conditional mean satisfied?**

$$
E[u(ability)|college A] = 0?
$$

That is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?

```{r}
#| echo: false 
g_CI
```

This is what it would like if college choice and ability are not correlated:

```{r}
#| echo: false
 
ggplot(data = data_cI) +
  geom_point(aes(x = x, y = y, color = University), size = 0.5) +
  geom_smooth(aes(x = x, y = yline, color = University), method = "lm", se = FALSE) +
  geom_hline(yintercept = mean_inc_A, color = "red") +
  geom_hline(yintercept = mean_inc_B, color = "blue") +
  annotate(
    "text",
    x = 1,
    y = 120,
    label = paste(round(mean_inc_A, 0), sep = ""),
    family = "Times",
    color = "red"
  ) +
  annotate(
    "text",
    x = 1, y = 70,
    label = paste(round(mean_inc_B, 0), sep = ""),
    family = "Times",
    color = "blue"
  ) +
  xlab("ability") +
  ylab("Annual Income") +
  theme_lecture +
  theme(
    legend.position = "bottom"
  )
```

## Exercise

+ consider a phenomenon you are interested in understanding 
  - dependent variable (variable to be explained) 
  - explanatory variable (variable to explain) 
+ construct a simple linear model
+ identify what is in the error term
+ check if they are correlated withe explanatory variable or not



