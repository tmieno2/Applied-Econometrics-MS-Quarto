---
title: "05: Hypothesis Testing"
format: 
  revealjs: 
    theme: [default, ../custom.scss]
    fontsize: 1.2em
    callout-icon: false
    scrollable: true
    echo: true
    fig-dpi: 400
    footer: "[back to course website with lecture slides](https://tmieno2.github.io/Applied-Econometrics-MS-Quarto/lectures/)"
execute:
  echo: true
  out-width: 80%
webr:
  packages: ['fixest', 'dplyr', 'data.table', 'ggplot2', 'wooldridge', 'broom', 'car']
  cell-options:
    editor-font-scale: 0.8
filters:
  - webr
---



# Review on Statistical Hypothesis Testing

```{r}
#| include: false
library(dplyr)
library(wooldridge)
library(data.table)
library(ggplot2)
library(fixest)
```

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Hypothesis Testing

::: {.panel-tabset}

### General steps

Here is the general step of any hypothesis testing:

+ Step 1: specify the null $(H_0)$ and alternative $(H_1)$ hypotheses

+ Step 2: find the distribution of the test statistic <span style = "color: red;"> if the null hypothesis is true </span> 

+ Step 3: calculate the test statistic based on the data and regression results

+ Step 4: define the significance level

+ Step 5: check how unlikely that you get the actual test statistic (found at Step 3) <span style = "color: red;"> if indeed the null hypothesis is true </span>

### Example

::: {.panel-tabset}

#### Setting

**Goal**

Suppose you want to test if the expected value of a normally distributed random variable $(x)$ is 1 or not.

<br>

**State of Knowledge**

We do know $x$ follows a normal distribution and its variance is 4 for some reason.

<br>

**Your Estimator**

Your estimator is the sample mean: $\theta = \sum_{i=1}^J x_i/J$

#### distirbution of $\theta$


:::{.callout-note title="Math Aside 1"}
$Var(ax) = a^2 Var(X)$
:::

So, we know that $\theta \sim N(\alpha, 4/J)$ (of course $\alpha$ is not known).

<br>

:::{.callout-note title="Math Aside 2"}
If $x \sim N(a, b)$, then, $x-a \sim N(0, b)$ (shift) 
:::

So, $\frac{x-a}{\sqrt{b}} \sim N(0, 1)$ (combined with Math Aside 1)

<br>

**This means,**

Since $\theta = \sum_{i=1}^J x_i/J$ and $x_i \sim N(\alpha, 4)$, 

+ $Var(\theta) = J \times \frac{1}{J^2}Var(x) = 4/J$
+ $\frac{\sqrt{J}}{2} \cdot (\theta - \alpha)\sim N(0, 1)$.


#### distirbution of $\theta$ if $H_0$ is true

::: {.columns}

::: {.column width="50%"}
We established that $\frac{\sqrt{J}}{2} \cdot (\theta - \alpha)\sim N(0, 1)$.

The null hypothesis is $\alpha = 1$. 

If $\alpha = 1$ is indeed true, then $\sqrt{J} \times (\theta - 1)/2 \sim N(0, 1)$.

In other words, if you multiply the sample mean by the square root of the number of observations and divide it by 2, then it follows the standard normal distribution like below.
:::
<!--end of the 1st column-->
::: {.column width="50%"}
```{r}
#| echo: false
x <- seq(-4, 4, length = 1000)
y <- dnorm(x, sd = 1)
plot_data <- data.table(x = x, y = y)
g_n <- 
  ggplot(data = plot_data) +
  geom_line(aes(x = x, y = y), color = "black") +
  theme_bw()
g_n
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

#### Case 1

::: {.columns}

::: {.column width="50%"}
Suppose you have obtained 100 samples $(J = 100)$ and calculated $\theta$ (sample mean), which turned out to be 2.

Then, your test statistic is $\sqrt{100} \times (2-1)/2 = 5$.

How unlikely is it to get the number you got (5) if the null hypothesis is indeed true?

:::
<!--end of the 1st column-->
::: {.column width="50%"}
```{r}
#| echo: false
g_n
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


#### Case 2

::: {.columns}

::: {.column width="50%"}
Suppose you have obtained 400 samples $(J = 400)$ and calculated $\theta$ (sample mean), which turned out to be 1.02.

Then, your test statistic is $\sqrt{400} \times (1.02-1)/2 = 0.2$.

How unlikely is it to get the number you got (0.2) if the null hypothesis is indeed true?
:::
<!--end of the 1st column-->
::: {.column width="50%"}
```{r}
#| echo: false
g_n
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


:::
<!--end of panel-->

### By the way

::: {.columns}

::: {.column width="40%"}
Note that you do not really need to use $\sqrt{J} \times (\theta - \alpha)/2$ as your test statistic. 

You could alternatively use $\theta - \alpha$. But, in that case, you need to be looking at $N(0, 4/J)$ instead of $N(0, 1)$ to see how unlikely you get the number you got. 

For example, when the number of observations is 100 $(J = 100)$, the distribution of $\theta-\alpha$ looks like the figure on the right.
:::
<!--end of the 1st column-->
::: {.column width="60%"}
```{r}
#| echo: false
x <- seq(-4, 4, length = 1000)
y <- dnorm(x, sd = 2 / 10)
plot_data <- data.table(x = x, y = y)
ggplot(data = plot_data) +
  geom_line(aes(x = x, y = y), color = "black") +
  theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


**Reconsider the case 1**

Suppose you have obtained 100 samples $(J = 100)$ and calculated $\theta$ (sample mean), which turned out to be 2.

Then, your test statistic is $2-1 = 1$. 

Is it unlikely for you to get 1 if the null hypothesis is true? 

The conclusion would be exactly the same as using $\sqrt{J} \times (\theta - \alpha)/2$ because the distribution under the null is adjusted according to the test statistic you use.

:::{.callout-note}
We always use normalize test statistic so that we can always look up the same distribution.
:::

### Summary

**What do we need?**

+ test-statistic of which we know the distribution (e.g., t-distribution, Normal distribution) assuming the null hypothesis

<br>

**What do we (often) do?**

+ transform (most of the time) a raw random variable (e.g., sample mean in the example above) into a test statistic of which we know the distribution assuming that the null hypothesis is true
  - e.g., we transformed the sample mean so that it follows the standard Normal distribution.
+ check if the actual number you got from the test statistic is likely to happen or not (formal criteria has not been discussed yet)

### Exercise

::: {.panel-tabset}

#### Problem

You have collected data on annual salary for those who graduated from University A and B. You are interested in testing whether the difference in annual salary between the universities (call it $x$) is 10 on average. You know (for unknown reasons) know that the difference is distributed as $N(\alpha, 16)$. 

1. What is the null hypothesis?
2. Under the null hypothesis, what is the distribution of the sample mean when the number of observation is 400?
3. Normalize the test statistic so that the transformed version follows $N(0, 1)$.
4. The actual difference you observed is 10.2. What is the probability that you observe a number greater than 10.2 if the null hypothesis is true? Use `prnom()`.


:::{.callout-note}
In reality,

+ we need to find out what the distribution of the test statistic is
+ we need to formerly define when we accept or not accept the null hypothesis
:::

#### Answer

1. $\alpha = 10$
2. $\theta \sim N(\alpha, 16/400)$
3. $\sqrt{\frac{400}{16}}\cdot (\theta - \alpha)$
4. The test statistic is $5 \times (10.2 - 10) = 1$

```{webr-r}
1 - pnorm(1)
```

:::
<!--end of panel-->

:::
<!--end of panel-->

# Hypothesis Testing on the Coefficients

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Hypothesis Testing of the Coefficients

::: {.panel-tabset}

### Example

Consider the following model,

$$wage = \beta_0 + \beta_1 educ + \beta_2 exper + u$$

:::{.callout-note title="Example Hypotheses"}
+ education has no impact on wage $(\beta_1=0)$
+ experience has a positive impact on wage $(\beta_2>0)$
:::

<br>

+ If $\widehat{\beta}_1$ is non-random, but just a scalar, all you have to do is just check if $\widehat{\beta}_1=0$ or not

+ But, the estimate you get is <span style = "color: blue;"> just one realization </span> of the range of values $\widehat{\beta}_1$ could take because it is a random varaible

+ This means that even if $\beta_1=0$ in the population, it is possible to get an estimate that is very far from 0

### Distribution of $\beta_j$

::: {.panel-tabset}

#### Assumption

So far, we learned that:

+ OLS estimators are unbiased under MLR.1 ~ MLR.4
+ Variance of the OLS estimator of $\beta_j$ is $\frac{\sigma^2}{SST_x\cdot (1-R^2_j)}$ under MLR.1 ~ MLR.5

We have <span style = "color: blue;"> NOT </span> made any assumptions about the distribution of the error term!!

In order to perform hypothesis testing, we need to make assumptions about the distribution of error term (this is not strictly true, but more on this later)

#### Additional assumption

For the purpose of hypothesis testing, we will make the following assumption:

:::{.callout-note title="Normality of the error term"}
The population error $u$ is <span style = "color: red;"> independent </span> of the explanatory variables $x_1,\dots,x_k$ and is <span style = "color: red;"> normally </span> distributed with zero mean and variance $\sigma^2$:

$$u\sim N(0,\sigma^2)$$
:::


:::{.callout-note}
The normality assumption is much more than error term being distributed as Normal.

Independence of the error term implies

+ $E[u|x] = 0$
+ $Var[u|x]= \sigma^2$

So, we are necessarily assuming MLR.4 and MLR.5 hold by the independence assumption.
:::

#### Implications of the Assumptions

::: {.columns}

::: {.column width="50%"}
**distribution of the dependent variable**

The distribution of $y$ conditional on $x$ is a Normal distribution

$y|x \sim N(\beta_0+\beta_1 x_1+\dots+\beta_k x_k,\sigma^2)$

+ $E[y|x]$ is $\beta_0+\beta_1 x_1+\dots+\beta_k x_k$
+ $u|x$ is $N(0,\sigma^2)$
:::
<!--end of the 1st column-->
::: {.column width="50%"}
**distribution of the OLS estimator**

If the MLR.1 through MLR.6 are satisfied, $OLS$ estimators are also Normally distributed!

$\widehat{\beta}_j \sim N(\beta_j,Var(\widehat{\beta}_j))$

which means,

$\frac{\widehat{\beta}_j-\beta_j}{se(\widehat{\beta}_j)} \sim N(0,1)$
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


:::{.callout-note title="Question"}
Okay, so are we going to use this for testing involving $\beta_j$?
:::


:::
<!--end of panel-->

### t-test

In practice, we need to estimate $se(\beta_j)$. If we use $\widehat{se(\widehat{\beta}_j)}$ instead of $se(\widehat{\beta}_j)$, then,

$\frac{\widehat{\beta}_j-\beta_j}{\widehat{se(\widehat{\beta}_j)}} \sim t_{n-k-1}$

where $n-k-1$ is the degree of freedom of residual.

(Note: $\widehat{se(\widehat{\beta}_j)} = \widehat{\sigma}^2/\big[SST_X\cdot (1-R_j^2)\big]$)

### Recap on notations

+ $k$: the number of explanatory variables included except the intercept
+ $\sigma^2$: the <span style = "color: blue;"> true </span> variance of the error term
+ $\widehat{\sigma^2}$: the estimator (estimate) of the variance of the error term
  - $\widehat{\sigma^2} = \frac{\sum \widehat{u}_i^2}{n-k-1}$
+ $SST_X = \sum (x_i - \bar{x})^2$ 
+ $\widehat{\beta}_j$: OLS estimator (estimate) on explanatory variable $x_j$
  - $\widehat{\beta} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{SST_X}$ (for simple univariate regression)
+ $var(\widehat{\beta}_j)$: the <span style = "color: blue;"> true </span> variance of $\widehat{\beta}_j$
+ $R^2_j$: $R^2$ when you regres $x_j$ on all the other covariates (mathematical expression omitted)
+ $\widehat{var(\widehat{\beta}_j)}$: Estimator (estimate) of $var(\widehat{\beta}_j)$
  - $\widehat{var(\widehat{\beta}_j)} = \frac{\widehat{\sigma^2}}{SST_X\cdot(1-R^2_j)}$
+ $se(\widehat{\beta}_j)$: square root of $var(\widehat{\beta}_j)$
+ $\widehat{se(\widehat{\beta}_j)}$: square root of $\widehat{var(\widehat{\beta}_j)}$

:::
<!--end of panel-->  

## Details of hypothesis testing on $\beta_j$

::: {.panel-tabset}

### Null and alternative hypothesis

::: {.panel-tabset}

#### Introduction

Statistical hypothesis testing involves two hypotheses: Null and Alternative hypotheses. 

Pretend that you are an attorney who indicted a defendent who you think commited a crime. 

<br>  

**Null Hypothesis**

Hypothesis that you would like to reject (defendent is not guilty)

<br>

**Alternative Hypothesis**

Hypothesis you are in support of (defendent is guilty) 

#### One-sided and Two-sided Alternatives Hypotheses

**one-sided alternative**

$H_0:$ $\beta_j = 0$ 
$H_1:$ $\beta_j > 0$ 

You look at the positive end of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level). 

<br>

**two-sided alternative**

$H_0:$ $\beta_j = 0$ 
$H_1:$ $\beta_j \ne 0$ 

You look at the both ends of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level).

:::
<!--end of panel-->

### Significance level

```{r include = F}
u <- seq(-3.5, 3.5, length = 1000)
df <- 100 - 10
pdf <- dt(u, df = df) # default is mean=0,sd=1
data <- data.table(x = u, y = pdf)

g_t <- ggplot() +
  geom_line(data = data, aes(y = y, x = x)) +
  geom_hline(yintercept = 0) +
  xlab("") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

ggsave(g_t, file = "./t_dist.pdf", width = 4, height = 3)

alpha <- 0.05
c_rej <- qt(1 - alpha, df = df)
rej_data <- data[u >= c_rej, ]

g_t_005_one <- g_t +
  geom_ribbon(
    data = rej_data,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  annotate("text",
    x = c_rej, y = -0.02,
    label = as.character(round(c_rej, digits = 2)),
    size = 4,
    family = "Times"
  ) +
  annotate("text",
    x = 2.7, y = 0.1,
    label = paste("area  =  ", round(alpha, digits = 2), sep = ""),
    size = 4,
    family = "Times"
  )


alpha <- 0.01
c_rej <- qt(1 - alpha, df = df)
rej_data_001 <- data[u >= c_rej, ]

g_t_001_one <- g_t +
  geom_ribbon(
    data = rej_data_001,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  annotate("text",
    x = c_rej, y = -0.02,
    label = as.character(round(c_rej, digits = 2)),
    size = 4,
    family = "Times"
  ) +
  annotate("text",
    x = 2.7, y = 0.1,
    label = paste("area  =  ", round(alpha, digits = 2), sep = ""),
    size = 4,
    family = "Times"
  )
```


::: {.panel-tabset}

#### Definition

:::{.callout-note title="Definition"}
The probability of rejecting the null when the null is actually true (The probability that you wrongly claim that the null hypothesis is wrong even though it's true in reality: Type I error)
:::

The lower the significance level, you are more sure that the null is indeed wrong when you reject the null hypothesis

#### One-sided: 5% significance

::: {.columns}

::: {.column width="40%"}
+ Figure on the right presents the distribution of $\frac{\widehat{\beta}_j-\beta_j}{\widehat{se(\widehat{\beta}_j)}}$ if $\beta_j = 0$ (the null hypothesis is true).

+ The probability that you get a value larger than 1.66 is 5% (0.05 in area).
:::
<!--end of the 1st column-->
::: {.column width="60%"}
```{r echo = F}
g_t_005_one + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

:::{.callout-note title="Decision Rule"}
Reject the null hypothesis if the t-statistic is greater than 1.66 (95% quantile of the t-distribution)
:::

If you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of $\beta_j = 0$. Here, 

+ 5% is the <span style = "color: red;"> significance level</span>
+ 1.66 is the <span style = "color: red;"> critical value</span> above which you will reject the null

#### One-sided: 1% significance

::: {.columns}

::: {.column width="40%"}
+ The figure on the right presents the distribution of $\frac{\widehat{\beta}_j-\beta_j}{\widehat{se(\widehat{\beta}_j)}}$ if $\beta_j = 0$ (the null hypothesis is true).
+ The probability that you get a value larger than 2.37 is 1% (0.01 in area).
:::
<!--end of the 1st column-->
::: {.column width="60%"}
```{r echo = F}
g_t_001_one + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

:::{.callout-note title="Decision Rule"}
Reject the null hypothesis if the t-statistic is greater than 2.37 (99% quantile of the t-distribution)
:::

If you follow the decision rule, then you have a 1% chance that you are wrong in rejecting the null hypothesis of $\beta_j = 0$. Here, 

+ 1% is the <span style = "color: red;"> significance level </span>
+ 2.37 is the <span style = "color: red;"> critical value </span> above which you will reject the null

#### Two-sided: 5% significance

```{r t-005-two, include = F}
u <- seq(-3, 3, length = 1000)
df <- 500 - 10
pdf <- dt(u, df = df) # default is mean = 0,sd = 1
data <- data.table(x = u, y = pdf)

alpha <- 0.05

c_rej <- qt(1 - alpha / 2, df = df)
rej_data_high <- data[u >= c_rej, ]
rej_data_low <- data[u <= -c_rej, ]

g_t_005_two <- g_t +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  annotate("text",
    x = c_rej, y = -0.02,
    label = as.character(round(c_rej, digits = 2)),
    size = 4,
    family = "Times"
  ) +
  annotate("text",
    x = 0, y = 0.1,
    label = paste("area  = ", round(alpha / 2, digits = 3), sep = ""),
    size = 4,
    family = "Times"
  )
```

::: {.columns}

::: {.column width="40%"}
+ The figure on the right presents the distribution of $\frac{\widehat{\beta}_j-\beta_j}{\widehat{se(\widehat{\beta}_j)}}$ if $\beta_j = 0$ (the null hypothesis is true).

+ The probability that you get a value more extreme than 1.96 or -1.96 is 5% (0.05 in area cobining the two area at the edges).

(Note: irrespective of the type of tests, the distribution of t-statistics is the same.)
:::
<!--end of the 1st column-->
::: {.column width="60%"}
```{r echo = F, dependson = "t-005-two"}
g_t_005_two + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->  

:::{.callout-note title="Decision Rule"}
Reject the null hypothesis if the absolute value of the t-statistic is greater than 1.96.
:::

If you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of $\beta_j = 0$. Here,

+ 5% is the <span style = "color: red;"> significance level </span>
+ 1.96 is the <span style = "color: blue;"> critical value </span> above which you will reject the null

:::
<!--end of panel-->

### p-value

:::{.callout-note title="Definition"}
The smallest significance level at which the null hypothesis would be rejected (the probability of observing a test statistic at least as extreme as we did if the null hypothesis is true)
:::


```{r p-dist, include = F}
u <- seq(-3.5, 3.5, length = 1000)
df <- 500 - 10
pdf <- dt(u, df = df) # default is mean = 0,sd = 1
data <- data.table(x = u, y = pdf)

g_t <- ggplot() +
  geom_line(data = data, aes(y = y, x = x)) +
  geom_hline(yintercept = 0) +
  xlab("") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

alpha <- 0.031
c_rej <- qt(1 - alpha / 2, df = df)
rej_data_high <- data[u >= c_rej, ]
rej_data_low <- data[u <= -c_rej, ]

g_t_0031 <- g_t +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  annotate("text",
    x = c_rej, y = -0.02,
    label = as.character(round(c_rej, digits = 2)),
    size = 4,
    family = "Times"
  ) +
  annotate("text",
    x = 0, y = 0.1,
    label = paste("area  =  ", round(alpha, digits = 3), sep = ""),
    size = 4,
    family = "Times"
  )
```

::: {.columns}

::: {.column width="50%"}
+ Suppose the t-statistic you got is 2.16. Then, there's a 3.1% chance you reject the null when it is actually true, if you use it as the critical value.

+ So, the lower significance level the null hypothesis is rejected is 3.1%, which is the definition of p-value.
:::
<!--end of the 1st column-->
::: {.column width="50%"}
```{r echo = F, dependson = "p-dist"}
g_t_0031 + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->  
 
:::{.callout-note title="Decision rule"}
If the p-value is lower than your choice of significance level, then you reject the null.
:::

This decision rule of course results in the same test results as the one we saw that uses a t-value and critical value.

### Example

**Estimated Model**

The impact of experience on wage:

+ $log(wage) = 0.284+0.092\times educ+0.0041\times exper + 0.022 \times tenure$
+ $\widehat{se(\widehat{\beta}_{exper})} = 0.0017$ 
+ $n = 526$

<br>

**Hypothesis**

+ $H_0$: $\beta_{exper}=0$
+ $H_1$: $\beta_{exper}>0$

<br>

**Test**

t-statistic $= 0.0041/0.0017 = 2.41$

The critical value is the 99% quantile of $t_{526-3-1}$, which is $2.33$ (it can be obtained by `qt(0.95, 522)`)

Since $2.41 > 2.33$, we reject the null in favor of the alternative hypothesis at the 1% level.

:::
<!--end of panel-->


## R implementation

::: {.panel-tabset}

### Run regression

```{webr-r}
#--- get the data ---#
data("wage1", package = "wooldridge")

#--- run a regression ---#
reg_wage <- feols(wage ~ educ + exper + tenure, data = wage1)
```

### Obtain t-statistics

You can apply the `broom::tidy()` function from the `broom` package to access the regression results (`reg_wage` here) as a `tibble` (`data.frame`).

```{webr-r}
#* tidy up the regression results
reg_results_table <- broom::tidy(reg_wage)

#* display
reg_results_table
```

<br>

+ `estimate`: coefficient estimates $(\widehat{\beta}_j)$
+ `std.error`: $\widehat{se(\widehat{\beta}_j}$)
+ `statistic`: t-statistic for the null of $\beta_j = 0$
+ `p.value`: p-value (for the two sided test with the null of $\beta_j = 0$)

So, for the t-test of of $\beta_j = 0$ is already there. You do not need to do anything further.

For the null hypothesis other than $\beta_j = 0$, you need further work.

### R implementation (by hand)

Suppose you are testing the null hypothesis of $\beta_{educ} = 1$ against the alternative hypothesis of $\beta_{educ} \ne 1$ (so, this is a two-sided test). 

The t-value for this test is not available from the summary. 

::: {.panel-tabset}

```{r}
#| include: false
data("wage1", package = "wooldridge")

#--- run a regression ---#
reg_wage <- feols(wage ~ educ + exper + tenure, data = wage1) 
 
reg_results_table <- broom::tidy(reg_wage)

#--- coefficient estimate on educ ---#
beta_educ <-
  reg_results_table %>%
  dplyr::filter(term == "educ") %>%
  dplyr::pull(estimate)

#--- se of the coefficient on educ ---#
se_beta_educ <-
  reg_results_table %>%
  dplyr::filter(term == "educ") %>%
  dplyr::pull(std.error)

#--- get the t-value ---#
t_value <- (beta_educ - 1) / se_beta_educ
df_t <- degrees_freedom(reg_wage, "resid")
critical_value <- qt(0.975, df = df_t)
```

#### get t-value

::: {.columns}

::: {.column width="80%"}
```{webr-r}
#--- coefficient estimate on educ ---#
beta_educ <-
  reg_results_table %>%
  filter(term == "educ") %>%
  pull(estimate)

#--- se of the coefficient on educ ---#
se_beta_educ <-
  reg_results_table %>%
  filter(term == "educ") %>%
  pull(std.error)

#--- get the t-value ---#
(
  t_value <- (beta_educ - 1) / se_beta_educ
)
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->



#### critical value and t-test

The degree of freedom $(n-k-1)$ of the t-distribution can be obtained by applying `degrees_freedom(reg_wage, "resid")`:

```{webr-r}
(
  df_t <- degrees_freedom(reg_wage, "resid")
)
```

<br>

You can get the 97.5% quantile of the $t_{522}$ using the `qt()` function:

```{webr-r}
(
  critical_value <- qt(0.975, df = df_t)
)
```

<br>

Since the absolute value of the t-value (`r t_value`) is greater than the critical value (`r critical_value`), you reject the null.

:::
<!--end of panel-->

:::
<!--end of panel-->

# Confidence Interval (CI)

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Confidence Interval (CI)

::: {.panel-tabset}

### Definition

:::{.callout-note title="Definition"}
If you calculate the 95% confidence interval on multiple different samples, 95% of the time, the calculated CI includes the true parameter
:::

<br>

**What confidence interval is not**

The probability that a realized CI calculated from specific sample data includes the true parameter

### How to get CI (in general)

::: {.panel-tabset}

#### General Procedure

For the **assumed** distribution of statistic $x$, the $A\%$ confidence interval of $x$ is the range with

+ lower bound: 100 − A/2 percent quantile of $x$
+ upper bound: 100-(100 − A)/2 percent quantile of $x$ 

#### Example

::: {.columns}

::: {.column width="40%"}
For the 95% CI (A = 95), 

+ lower bound: 2.5 (100 − 95/2) percent quantile of $x$
+ upper bound: 97.5 (100-(100 − 95)/2) percent quantile of $x$

If $x$ follos the standard normal distribution $(x \sim N(0, 1))$, then,the 2.5% and 97.5% quantiles are -1.96 and 1.96, respectively. So, the 95% CI of $x$ is [-1.96, 1.96].

:::
<!--end of the 1st column-->
::: {.column width="60%"}
```{r norm-95, include = F}
u <- seq(-3, 3, length = 1000)
pdf <- dnorm(u) # default is mean=0,sd=1
data <- data.table(x = u, y = pdf)

g_norm <- ggplot() +
  geom_line(data = data, aes(y = y, x = x)) +
  geom_hline(yintercept = 0) +
  xlab("") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

alpha <- 0.05
c_rej <- qnorm(1 - alpha / 2)
rej_data_high <- data[u >= c_rej, ]
rej_data_low <- data[u <= -c_rej, ]

g_norm_005 <- g_norm +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  annotate("text",
    x = c_rej, y = -0.02,
    label = as.character(round(c_rej, digits = 2)),
    size = 4,
    family = "Times"
  ) +
  annotate("text",
    x = 2.5, y = 0.15,
    label = paste("area = ", round(alpha / 2, digits = 3), sep = ""),
    size = 4,
    family = "Times"
  )
```
```{r echo = F}
g_norm_005 + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->



:::
<!--end of panel-->


### How to get the CI of coefficients

Under the assumption of MLR.1 through MLR.6 (which includes the normality assumption of the error), we learned that

$\frac{\widehat{\beta}_j - \beta_j}{\widehat{se(\widehat{\beta}_j)}} \sim t_{n-k-1}$

So, following the general procedure we discussed in the previous slide, the A% confidence interval of $\frac{\widehat{\beta}_j - \beta_j}{\widehat{se(\widehat{\beta}_j)}}$ is 

+ lower bound: $(100 − A)/2$% quantile of the $t_{n-k-1}$ distribution (let's call this $Q_l$)
+ upper bound: $100 - (100 − A)/2$% quantile of the $t_{n-k-1}$ distribution (let's call this $Q_h$)  

But, we want the A% CI of $\beta_j$, not $\frac{\widehat{\beta}_j - \beta_j}{\widehat{se(\widehat{\beta}_j)}}$. Solving for $\beta_j$,

$\beta_j = t_{n-k-1}\times \widehat{se(\widehat{\beta}_j)} + \widehat{\beta}_j$

So, to get the A% CI of $\beta_j$, we scale the CI of $\frac{\widehat{\beta}_j - \beta_j}{\widehat{se(\widehat{\beta}_j)}}$ by $se(\widehat{\beta}_j)$ and then shift by $\widehat{\beta}_j$.

+ lower bound: $Q_l \times se(\widehat{\beta}_j) + \widehat{\beta}_j$ 
+ upper bound: $Q_h \times se(\widehat{\beta}_j) + \widehat{\beta}_j$

Note that $Q_l$ is negative and $Q_h$ is negative.

### Example (R implementation)

::: {.panel-tabset}
#### Regression

**Run OLS and extract necessary information**

```{webr-r}
wage_reg <- feols(wage ~ educ + exper + tenure, data = wage1)
```

<br>

Applying `broom::tidy()` to `wage_reg`,

```{webr-r}
(
wage_reg_coef <- broom::tidy(reg_wage)
)
```

```{r}
#| include: false 
wage_reg <- feols(wage ~ educ + exper + tenure, data = wage1)
wage_reg_coef <- broom::tidy(reg_wage)
```

#### Collect necessary information

We are interested in getting the 90% confidence interval of the coefficient on `educ` $(\beta_{educ})$. Under all the assumptions (MLR.1 through MLR.6), we know that in general,

$\frac{\widehat{\beta}_{educ} - \beta_{educ}}{\widehat{se(\widehat{\beta}_{educ})}} \sim t_{n-k-1}$

Specifically for this regression,

+ $\widehat{\beta}_{educ}$ = `r wage_reg_coef %>% filter(term == 'educ') %>% pull(estimate)`

+ $\widehat{se(\widehat{\beta}_{educ})}$ = `r wage_reg_coef %>% filter(term == 'educ') %>% pull(std.error)`

+ $n - k - 1 = `r df`$ (degrees of freedom)

```{r df}
#| echo: false
df <- degrees_freedom(reg_wage, "resid")
```

#### Find CI

```{r include = F}
u <- seq(-3, 3, length = 1000)
df <- degrees_freedom(wage_reg, "resid")
pdf <- dt(u, df = df) # default is mean=0,sd=1
data <- data.table(x = u, y = pdf)

g_t <- ggplot() +
  geom_line(data = data, aes(y = y, x = x)) +
  geom_hline(yintercept = 0) +
  xlab("") +
  ylab("") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  )

alpha <- 0.05
c_rej <- qt(1 - alpha / 2, df = df)
rej_data_high <- data[u >= c_rej, ]
rej_data_low <- data[u <= -c_rej, ]

g_t_005 <- g_t +
  geom_ribbon(
    data = rej_data_high,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  ) +
  geom_ribbon(
    data = rej_data_low,
    aes(ymax = y, ymin = 0, x = x),
    fill = "red", alpha = 0.4
  )
```

::: {.columns}

::: {.column width="50%"}
Now, we need to find the 5% ((100-90)/2) and 95% (100-(100-90)/2) quantile of $t_{522}$.

```{webr-r}
qt(0.05, df = 522)
qt(0.95, df = 522)
```

<br>

So, the 90% CI of $\frac{0.599 - \beta_{educ}}{0.051} \sim t_{522}$ is [`r qt(0.05, df = 522)`, `r qt(0.95, df = 522)`]

By scaling and shifting, the lower and upper bounds of the 90% CI of $\beta_{educ}$ are:

+ lower bound: 0.599 + 0.051 $\times$ `r qt(0.05, df = 522)` = `r 0.599 + 0.051 * qt(0.05, df = 522)`

+ upper bound: 0.599 + 0.051 $\times$ `r qt(0.95, df = 522)` = `r 0.599 + 0.051 * qt(0.95, df = 522)`


:::
<!--end of the 1st column-->
::: {.column width="50%"}
The distribution of $t_{522}$:

```{r echo = F}
g_t_005 + theme_bw()
```
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

#### In practice

You can just use `broom::tidy()` with `conf.int = TRUE`, `conf.level = confidence level` like below:

```{webr-r}
broom::tidy(
    wage_reg, 
    conf.int = TRUE, 
    conf.level = 0.9
  ) %>%
  dplyr::relocate(term, conf.low, conf.high)
```

:::
<!--end of panel-->

:::
<!--end of panel-->


# Linear Combination of Multiple Coefficients

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Linear Combination of Multiple Coefficients

::: {.panel-tabset}

### Example

**Model**

$$log(wage) = \beta_0+\beta_1 jc+ \beta_2 univ + \beta_3 exper + u$$

+ $jc$: 1 if you attended 2-year college, 0 otherwise
+ $univ$: 1 if you attended 4-year college, 0 otherwise

**Question**

Does the impact of education on wage is greater if you attend a 4-year college than 2-year college?

**Hypothesis**

The null and alternative hypotheses would be:

+ $H_1:$ $\beta_1 < \beta_2$
+ $H_0:$ $\beta_1 = \beta_2$

### Rewrite the hypothesis

The null and alternative hypotheses are:

+ $H_1:$ $\beta_1 < \beta_2$
+ $H_0:$ $\beta_1 = \beta_2$

Rewriting them,

+ $H_1:$ $\beta_1-\beta_2 < 0$
+ $H_0:$ $\beta_1 - \beta_2 =0$

Or,  

+ $H_1:$ $\alpha < 0$
+ $H_0:$ $\alpha =0$

where $\alpha = \beta_1 - \beta_2$

Note that $\alpha$ is a linear combination of $\beta_1$ and  $\beta_2$.

### Math

:::{.callout-note title="Important Fact"}
For any linear combination of the OLS coefficients, denoted as $\widehat{\alpha}$, the following holds:

$$\frac{\widehat{\alpha}-\alpha}{\widehat{se(\widehat{\alpha})}} \sim t_{n-k-1}$$

Where $\alpha$ is the true value (it is $\beta_1 - \beta_2$ in the example in the previous slide).
:::

<br>

So, using the example, this means that 

$$\frac{\widehat{\alpha}-\alpha}{\widehat{se(\widehat{\alpha})}} = \frac{\widehat{\beta}_1-\widehat{\beta}_2-(\beta_1 - \beta_2)}{\widehat{se(\widehat{\beta}_1-\widehat{\beta}_2)}} \sim t_{n-k-1}$$

This is great because we know how to do t-test!

### Going back to the example

Our null hypothesis is $\alpha = 0$ (or $\beta_1 - \beta_2 = 0$).

So, <span style = "color: red;"> If </span> indeed the null hypothesis is true, then

$$\frac{\widehat{\alpha}-0}{\widehat{se(\widehat{\alpha})}} = \frac{\widehat{\beta}_1-\widehat{\beta}_2-0}{\widehat{se(\widehat{\beta}_1-\widehat{\beta}_2)}} \sim t_{n-k-1}$$

So, all you need to do is to substitute $\widehat{\beta}_1$, $\widehat{\beta}_2$, $\widehat{se(\widehat{\beta}_1 - \widehat{\beta}_2)}$ into the formula and see if the value is beyond the critical value for your chosen level of statistical significance.

### get $se(\widehat{\beta}_1-\widehat{\beta}_2)$

But, 

$$se(\widehat{\beta}_1-\widehat{\beta}_2)= \sqrt{Var(\widehat{\beta}_1-\widehat{\beta}_2}) \ne \sqrt{Var(\widehat{\beta}_1)+Var(\widehat{\beta}_2)}$$

If the following was true,

$$se(\widehat{\beta}_1-\widehat{\beta}_2) = \sqrt{Var(\widehat{\beta}_1)+Var(\widehat{\beta}_2)}$$

then, we could have just extracted $Var(\widehat{\beta}_1)$ and $Var(\widehat{\beta}_2)$ individually from the regression object on R, sum them up, and take a square root of it.

:::{.callout-note title="Math aside"}
$$Var(ax+by) = a^2 Var(x) + 2abCov(x,y) + b^2 Var(y)$$
:::

So, 

$$se(\widehat{\beta}_1-\widehat{\beta}_2)= \sqrt{Var(\widehat{\beta}_1-\widehat{\beta}_2}) = \sqrt{Var(\widehat{\beta}_1)-2Cov(\widehat{\beta}_1,\widehat{\beta}_2)+Var(\widehat{\beta}_2)}$$

### Demonstration using R

::: {.panel-tabset}

#### Regression

```{webr-r}
#--- import data ---#
data("twoyear", package = "wooldridge")

reg_sc <- fixest::feols(lwage ~ jc + univ + exper, data = twoyear) # OLS
```

#### Variance covariance matrix

:::{.callout-note title="Definition"}
Variance covariance matrix is a matrix where

+ $VCOV_{i,i}$: the variance of $i$th variable's coefficient estimator
+ $VCOV_{i,j}$: the covariance between $i$th and $j$th variables' estimators
:::

<br>

You can get it by applying `vcov()` to regression results:

```{webr-r}
vcov_sc <- vcov(reg_sc) # variance covariance matrix
```

<br>

For example, $vcov_{sc}[2, 2]$ is the variance of $\widehat{\beta}_{jc}$, and $vcov_{sc}[2, 3]$ is the covariance between $\widehat{\beta}_{jc}$ and $\widehat{\beta}_{univ}$.

#### Calculate the t-statistic

::: {.columns}

::: {.column width="80%"}
Get coefficient estimates: 

```{webr-r}
#--- reg results ---#
reg_sc_td <- broom::tidy(reg_sc)

#--- get coefficients ---#
beta_jc <-
  reg_sc_td %>%
  filter(term == "jc") %>%
  pull(estimate)

beta_univ <-
  reg_sc_td %>%
  filter(term == "univ") %>%
  pull(estimate)
```

<br>

Calcualt t-statistic:

```{webr-r}
#---  ---#
numerator <- beta_jc - beta_univ
denominator <-
  sqrt(
    vcov_sc["jc", "jc"] - 2 * vcov_sc["jc", "univ"] + vcov_sc["univ", "univ"]
  )
t_stat <- numerator / denominator
t_stat
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

:::
<!--end of panel-->

:::
<!--end of panel-->

# Multiple Linear Restrictions: F-test

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Multiple Linear Restrictions: F-test


::: {.panel-tabset}

### Example

::: {.panel-tabset}

#### Model and hypothesis

**Model**

$$log(salary) =  \beta_0 + \beta_1 years + \beta_2 gamesyr + \beta_3 bavg + \beta_4 hrunsyr + \beta_5 rbisyr + u$$

+ $salary$: salary in 1993
+ $years$: years in the league
+ $gamesyr$: average games played per year
+ $bavg$: career batting average
+ $hrunsyr$: home runs per year
+ $rbisyr$: runs batted in per year

**Hypothesis**

Once years in the league and games per year have been controlled for, the statistics measuring performance ( $bavg$, $hrunsyr$, $rbisyr$) have no effect on salary collectively.

$H_0$: $\beta_3=0$, $\beta_4=0$, and $\beta_5=0$

$H_1$: $H_0$ is not true

#### Questions
  
How do we test this?

+ $H_0$ holds if all of $\beta_3$, $\beta_4$, or $\beta_5$ are zero.
+ Conduct t-test for each coefficient individually?

:::
<!--end of panel-->

### Individual t-test

::: {.panel-tabset}

#### Run regression

```{webr-r}
#| autorun: true
#--- load the mlb data ---#
data("mlb1", package = "wooldridge")

#--- run a rergession ---#
mlb_reg <- fixest::feols(log(salary) ~ years + gamesyr + bavg
  + hrunsyr + rbisyr, data = mlb1)

#--- take a look at the results ---#
broom::tidy(mlb_reg)
```

#### Question

```{webr-r}
#| autorun: true
broom::tidy(mlb_reg)
```

What do you find?

#### Answer

None of the coefficients on `bavg`, `hrunsyr`, and `rbisyr` is statistically significantly different from 0 even at the 10% level!!

So, does this mean that they collectively have no impact on the salary of MLB players?

If you were to conclude that they do not have statistically significant impact jointly, you would turn out to be wrong!!

$SSR$ (or $R^2$) turns out to be useful for testing their impacts jointly.

:::
<!--end of panel-->

### F-test

In doing an F-test of the null hypothesis, we compare sum of squared residuals $(SSR)$ of two models:

<br>

**Unrestricted Model**

$$log(salary) = \beta_0 + \beta_1 years + \beta_2 gamesyr + \beta_3 bavg + \beta_4 hrunsyr + \beta_5 rbisyr + u$$

<br>

**Restricted Model**

$$log(salary) = \beta_0 + \beta_1 years + \beta_2 gamesyr + u$$

The coefficients on $bavg$, $hrunsyr$, and $rbisyr$ are restricted to be 0 following the null hypothesis. 

::: {.panel-tabset}
#### Question

If the null hypothesis is indeed true, then what do you think is going to happen if you compare the $SSR$ of the two models? Which one has a bigger $SSR$?

#### Answer

$SSR$ from the restricted model should be large because the restricted model has a smaller explanatory power than the unrestricted model.

:::
<!--end of panel-->

### SSR

**SSR of the unrestricted model**: $SSR_u$

```{webr-r}
#--- run OLS ---#
res_u <- feols(log(salary) ~ years + gamesyr + bavg
  + hrunsyr + rbisyr, data = mlb1)

#--- SSR ---#
sum(res_u$residuals^2)
```

<br>

**SSR of the restricted model**: $SSR_r$

```{webr-r}
#--- run OLS ---#
res_r <- feols(log(salary) ~ years + gamesyr, data = mlb1)

#--- SSR ---#
sum(res_r$residuals^2)
```

<br>

::: {.panel-tabset}
#### Question 1

What does $SSR_r - SSR_u$ measure?

#### Answer 1

The contribution from the three excluded variables in explaining the dependent variable.

#### Question 2

Is the contribution large enough to say that the excluded variables are important? 

#### Answer 2

Cannot tell at this point because we do not know the distribution of the difference!

:::
<!--end of panel-->

### F-test in general

**Setup**

Consider a following general model:

$$
y = \beta_0 +\beta_1 x_1 + \dots+\beta_k x_k +u
$$

Suppose we have $q$ restrictions to test: that is, the null hypothesis states that $q$ of the variables have zero coefficients.

$$H_0: \beta_{k-q+1} =0, \beta_{k-q+2} =0, \dots, \beta_k=0$$

When we impose the restrictions under $H_0$, the restricted model is the following:

$$y = \beta_0 +\beta_1 x_1 + \dots+\beta_{k-q} x_{k-q} + u$$


**F-statistic**

<span style = "color: red;"> If </span> the null hypothesis is true, then,

$$F = \frac{(SSR_r-SSR_u)/q}{SSR_u/(n-k-1)} \sim F_{q,n-k-1}$$

+ $q$: the number of restrictions
+ $n-k-1$: degrees of freedom of residuals

::: {.panel-tabset}

#### Question

Is the above $F$-statistic always positive?

#### Answer

Yes, because $SSR_r-SSR_u$ is always positive.

#### Question 2

The greater the joint contribution of the $q$ variables, the (greater or smaller) the $F$-statistic?

#### Answer 2

Greater.

:::
<!--end of panel-->

### F-test steps

::: {.columns}
::: {.column width="60%"}
**F-distribution**

```{r echo = F}
x <- seq(0, 4, length = 1000)
y_1 <- df(x, df1 = 4, df2 = 200)
y_2 <- df(x, df1 = 7, df2 = 400)

f_plot <- data.table(x = x, "F(2,100)" = y_1, "F(4,400)" = y_2) %>%
  melt(id.var = "x")

ggplot(data = f_plot) +
  geom_line(aes(x = x, y = value, color = variable)) +
  scale_color_discrete(name = "") +
  theme(
    legend.position = "bottom"
  ) +
  theme_bw()
```
:::
<!--end of the 1st column-->
::: {.column width="40%"}
**F-test steps**

+ Define the null hypothesis
+ Estimate the unrestricted and restricted models to obtains their $SSR$
+ Calculate $F$-statistic
+ Define the significance level and corresponding critical value according to the F distribution with appropriate degrees of freedoms
+ Reject if your $F$-statistic is greater than the critical value, otherwise do not reject
:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->

### F-test by hand

::: {.panel-tabset}

#### Step 1 

Eestimate the unrestricted and restricted models

```{webr-r}
#--- unrestricted model ---#
reg_u <- feols(log(salary) ~ years + gamesyr +
  bavg + hrunsyr + rbisyr, data = mlb1)

SSR_u <- sum(reg_u$residuals^2)

#--- restricted model ---#
reg_r <- feols(log(salary) ~ years + gamesyr, data = mlb1)

SSR_r <- sum(reg_r$residuals^2)
```

#### Step 2

Calculate F-stat

```{webr-r}
df_q <- 3 # the number of restrictions
df_ur <- degrees_freedom(reg_u, "resid") # degrees of freedom for the unrestricted model
F_stat_num <- (SSR_r - SSR_u) / df_q # numerator of F-stat
F_stat_denom <- SSR_u / df_ur # denominator of F-stat
F_sta <- F_stat_num / F_stat_denom # F-stat
F_sta
```

#### Steps 3 and 4

Find the critical value

```{webr-r}
alpha <- 0.05 # 5%  significance level
c_value <- qf(1 - alpha, df1 = df_q, df2 = df_ur)
c_value
```

Is F-stat > critical value?

```{webr-r}
F_sta > c_value
```

:::{.callout-note title="Observation"}
So, the performance variables have statistically significant impacts on salary <span style = "color: red;"> jointly</span>!!

What happened?
:::

:::
<!--end of panel-->

### F-test (easier way)

::: {.panel-tabset}

#### How

You can use the `car::linearHypothesis()` function from the `car` package. 

<br>

**Syntax**

```{r eval = F}
car::linearHypothesis(regression, hypothesis)
```

+ `regression`: the name of the regression results of the unrestricted model

+ `hypothesis`" text of null hypothesis. For example,

`c("x1 = 0", "x2 = 1")` means the coefficients on $x1$ and $x2$ are $0$ and $1$, respectively

#### Demonstration

```{webr-r}
#--- unrestricted regression ---#
reg_u <- fixest::feols(y ~ x1 + x2 + x3, data = data)

#--- F-test ---#
car::linearHypothesis(reg_u, c("x1=0", "x2=0"))
```

:::
<!--end of panel-->

:::
<!--end of panel-->

## Simulation (multicollinearity, t-test, and F-test)


::: {.panel-tabset}

### Data generation

::: {.columns}

::: {.column width="80%"}
```{webr-r}
N <- 300 # num observations
mu <- runif(N) # term shared by indep vars 1 and 2
x1 <- 0.1 * runif(N) + 2 * mu # indep 1
x2 <- 0.1 * runif(N) + 2 * mu # indep 2
x3 <- runif(N) # indep 3
u <- rnorm(N) # error
y <- 1 + x1 + x2 + x3 + u # generate y
data <- data.table(y = y, x1 = x1, x2 = x2, x3 = x3) # combine into a data.table
```

<br>

`x1` and `x2` are highly correlated with each other:

```{webr-r}
cor(x1, x2) # correlation between x1 and x2
```
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


### Regression

```{webr-r}
reg_u <- fixest::feols(y ~ x1 + x2 + x3, data = data) # OLS
broom::tidy(reg_u) # results
```

<br>

Both `x1` and `x2` are statistically insignificant individually. 

### F-test

::: {.columns}

::: {.column width="80%"}
```{webr-r}
#--- unrestricted ---#
SSR_u <- sum(reg_u$residuals^2)

#--- restricted ---#
reg_r <- fixest::feols(y ~ x3, data = data)
SSR_r <- sum(reg_r$residuals^2)

#--- degrees of freedom of residuals ---#
df_resid <- degrees_freedom(reg_u, "resid")

#--- F ---#
F_stat <- ((SSR_r - SSR_u) / 2) / (SSR_u / df_resid)

#--- critical value ---#
alpha <- 0.05
(
  c_value <- qf(1 - alpha, df1 = 2, df2 = df_resid)
)
```

<br>

The F-statistic for the hypothesis testing is:

```{webr-r}
#--- F > critical value? ---#
F_stat
F_stat > c_value
```

<br>

The F-statistic is very high, meaning they collectively affect the dependent variable significantly.
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


### Important

The standard error estimates of the coefficients on `x1` and `x2` are very high because they are so highly correlated that your estimation of the model had such a difficult time to distinguish the their individual impacts.

But, collectively, they have large impacts. $F$-test was able to detect the statistical significance of their impacts <span style = "color: red;"> collectively</span>.

:::
<!--end of panel-->

## MLB example

::: {.panel-tabset}

### Correlation check

Here is the correlation coefficients between the three variables:

```{webr-r}
#| autorun: true
dplyr::select(mlb1, bavg, hrunsyr, rbisyr) %>% cor()
```

As you can see, `brunsyr` and `hrunsyr` are highly correlated with each other. 

They are not so highly correlated with `bavg`.



### Multiple coefficients again

::: {.columns}

::: {.column width="80%"}
The test of a linear combination of the parameters we looked at earlier is a special case of F-test where the number of restriction is 1. 

It can be shown that square root of $F_{1, df}$ follows the $t_{df}$ distribution.

So, we can actually use $F$-test for this type of hypothesis testing because $F_{1,t-n-k} \sim t_{t-n-k}^2$.

```{webr-r}
#--- OLS with lm() ---#
reg_sc <- feols(lwage ~ jc + univ + exper, data = twoyear) # OLS

#--- F-test ---#
F_test <- linearHypothesis(reg_sc, c("jc-univ=0"))

#--- t-statistic ---#
sqrt(F_test$Chisq[2])
```

<br>

Check with the previous slide of the same t-test and confirm that the t-statistics we got there and here are the same.
:::
<!--end of the 1st column-->
::: {.column width="20%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


:::
<!--end of panel-->



