---
title: "08: OLS Asymptotics"
format: 
  revealjs: 
    theme: [default, ../custom.scss]
    fontsize: 1.2em
    callout-icon: false
    scrollable: true
    echo: true
    embed-resources: false
    fig-dpi: 400
    footer: "[back to course website with lecture slides](https://tmieno2.github.io/Applied-Econometrics-MS-Quarto/lectures/)"
execute:
  echo: true
  out-width: 80%
webr:
  packages: ['fixest', 'dplyr', 'data.table', 'ggplot2']
  cell-options:
    editor-font-scale: 0.8
filters:
  - webr
  - shinylive
---

## OLS Asymptotics

```{r}
#| include: false
library(ggplot2)
library(dplyr)
library(data.table)
```

**Large Sample Properties of OLS**
  
+ Properties of OLS that hold only when the sample size is infinite 

+ (loosely put) How OLS estimators behave when the number of observations goes <span style = "color: blue;"> infinite (really large) </span>

<br>

**Small Sample Properties of OLS**

Under certain conditions:

+ OLS estimators are unbiased
+ OLS estimators are efficient

These hold <span style = "color: blue;"> whatever the sample size is </span>.

# Consistency

## Consistency 

::: {.panel-tabset}

### Definition

**Verbally (and very loosely)**

An estimator is <span style = "color: blue;"> consistent </span> if the probability that the estimator produces the true parameter is 1 when sample size is infinite.


### MC simulation

::: {.panel-tabset}

#### Setup

OLS estimator of the coefficient on $x$ in the following model with all $MLR.1$ through $MLR.4$ satisfied:

$$y_i = \beta_0 + \beta_1 x_i + u_i$$

with all the conditions necessary for the unbiasedness property of OLS satisfied.

#### Conceptual steps (Pseudo Code)

+ Generate data according to $y_i = \beta_0 + \beta_1 x_i + u_i$
+ Estimate the coefficients and store them
+ Repeat the above experiment 1000 times
+ Examine how the coefficient estimates are distributed

**What you should see is**

As $N$ gets larger (more observations), the distribution of $\widehat{\beta}_1$ get more tightly centered around its true value (here, $1$). Eventually, it becomes so tight that the probability you get the true value becomes 1.

#### R code

```{webr-r}
#--- Preparation ---#
B <- 1000 # the number of iterations
N_list <- c(100, 1000, 10000) # sample size
N_len <- length(N_list)
estimate_storage <- matrix(0, B, 3) # estimates storage

for (j in 1:N_len) {
  temp_N <- N_list[j]
  for (i in 1:B) {
    #--- generate data ---#
    x <- rnorm(temp_N) # indep var 1
    u <- rnorm(temp_N) * 0.2 # error
    y <- 1 + x + u # dependent variable 1
    data <- data.frame(y = y, x = x)

    #--- OLS ---#
    reg <- lm(y ~ x, data = data) # OLS

    #--- store coef estimates ---#
    estimate_storage[i, j] <- reg$coef[2]
  }
}
```

#### Results

```{webr-r}
plot_data <- melt(data.frame(estimate_storage))

#--- create a figure ---#
g_co_ex <-
  ggplot(data = plot_data) +
  geom_density(aes(x = value, fill = variable), alpha = 0.4) +
  geom_vline(xintercept = 1) +
  xlim(0.9, 1.1) +
  scale_fill_discrete(
    name = "Sample Size",
    labels = c("N=100 ", "N=1,000 ", "N=10,000")
  ) +
  theme(
    legend.position = "bottom"
  )

g_co_ex
```

**Consistency of OLS estimators**

Under $MLR.1$ through $MLR.4$, OLS estimators are consistent

:::
<!--end of panel-->

### MC simualation: Inconsistency

::: {.panel-tabset}

#### Conceptual steps (Pseudo Code)

**Pseudo Code**

+ generate data ($N$ observations) according to $y_i = \beta_0 + \beta_1 x_i + u_i$ with $E[u_i|x_i]\ne 0$
+ Estimate the coefficients and store them
+ repeat the above experiment 1000 times
+ examine how the coefficient estimates are distributed

**Question**

Would the bias disappear as N gets larger?

#### R code

```{webr-r}
#--- Preparation ---#
N_list <- c(100, 1000, 10000) # sample size
N_len <- length(N_list)
estimate_storage <- matrix(0, B, 3) # estimates storage

for (j in 1:N_len) {
  temp_N <- N_list[j]
  for (i in 1:B) {
    #--- generate data ---#
    mu <- rnorm(temp_N) # shared term between x and u #<<
    x <- rnorm(temp_N) + 0.5 * mu # <<
    u <- rnorm(temp_N) + 0.5 * mu # <<
    y <- 1 + x + u # dependent variable
    data <- data.frame(y = y, x = x)

    #--- OLS ---#
    reg <- lm(y ~ x, data = data) # OLS

    #--- store coef estimates ---#
    estimate_storage[i, j] <- reg$coef[2]
  }
}
```

#### Results

```{webr-r}
#--- wide to long format ---#
plot_data <- melt(data.frame(estimate_storage))

#--- create a figure ---#
g_inco_ex <- ggplot(data = plot_data) +
  geom_density(aes(x = value, fill = variable), alpha = 0.4) +
  geom_vline(xintercept = 1) +
  scale_fill_discrete(
    name = "Sample Size",
    labels = c("N=100 ", "N=1,000 ", "N=10,000")
  ) +
  theme(
    legend.position = "bottom"
  )

g_inco_ex
```

:::
<!--end of panel-->

:::
<!--end of panel-->

# Asymptotic Normality

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=1000px></html>

## Asymptotic Normality

::: {.panel-tabset}

### Normality assumption

When we talked about hypothesis testing, we made the following assumption:

<br>

**Normality assumption**

The population error $u$ is .blue[independent] of the explanatory variables $x_1,\dots,x_k$ and is .blue[normally] distributed with zero mean and variance $\sigma^2$: 

$u\sim Normal(0,\sigma^2)$ 

<br>

**Remember**

+ If the normality assumption is violated, t-statistic and F-statistic we constructed before are no longer distributed as t-distribution and F-distribution, respectively

+ So, whenever $MLR.6$ is violated, our t- and F-tests are invalid

<br>

**Fortunately**

You can continue to use t- and F-tests because (slightly transformed) OLS estimators are .blue[approximately] normally distributed when the sample size is .blue[large enough].

### Central Limit Theorem (CLT)

:::{.callout-note title="Central Limit Theorem"}
Suppose $\{x_1,x_2,\dots\}$ is a sequence of idetically independently distributed random variables with $E[x_i]=\mu$ and $Var[x_i]=\sigma^2<\infty$. Then, as $n$ approaches infinity, 

$$\sqrt{n}(\frac{1}{n} \sum_{i=1}^n x_i-\mu)\overset{d}{\longrightarrow} N(0,\sigma^2)$$
:::

<br>

**Verbally**
 
Sample mean less its expected value multiplied by $\sqrt{n}$ (square root of the sample size) is going to be distributed as Normal distribution as $n$ goes infinity where its expected value is 0 and variance is the variance of $x$.


### Demonstration

::: {.panel-tabset}

#### Example 


::: {.columns}

::: {.column width="50%"}
**Setup**

Conside a random variable ($x$) that follows Bernouli distribution with $p = 0.3$. That is, it takes 0 and 1 with probability of 0.7 and 0.3, respectively.

$x_i \sim Bern(p = 0.3)$

+ $\mu = E[x_i] = p = 0.3$
+ $\sigma^2 \equiv Var[x_i] = p(1-p) = 0.21$
:::
<!--end of the 1st column-->
::: {.column width="50%"}

This is what 10 random draws and the transformed version of their sum look like:

```{webr-r}
#| autorun: true
(
x <- runif(10) < 0.3
)

sqrt(10) * (mean(x) - 0.3)
```

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


<br>

**According to CLT**

$\sqrt{n}(\frac{1}{n} \sum_{i=1}^n x_i-\mu)\overset{d}{\longrightarrow} N(0,\sigma^2)$ 

<br>

**So,**

$\sqrt{n}(\frac{1}{n} \sum_{i=1}^n x_i-0.3)\overset{d}{\longrightarrow} N(0,0.21)$


#### Demonstration

In each of the 5000 iterations, this application draws the number of samples you specify ($N$) from $x_i \sim Bern(p=0.3)$ and then calculate $\sqrt{N}(\frac{1}{N} \sum_{i=1}^N x_i-0.3)$. Then the histogram of the 5000 values is presented.

<br>

```{shinylive-r}
#| standalone: true

library(shiny)
library(ggplot2)
library(bslib)

# Define UI for CLT demonstration

ui <- page_sidebar(
  title = "CLT demonstration",
  sidebar = sidebar(
    numericInput("n_samples",
        "Number of samples:",
        min = 1, max = 100000, value = 10, step = 100
      ),
    br(),
    actionButton("simulate", "Simulate"),
    open = TRUE
  ),
  plotOutput("cltPlot")
)

# Define server logic to simulate the Central Limit Theorem
server <- function(input, output) {
  observeEvent(input$simulate, {
    output$cltPlot <- renderPlot({
      N <- input$n_samples # number of observations #<<
      B <- 5000 # number of iterations
      p <- 0.3 # mean of the Bernoulli distribution
      storage <- rep(0, B)

      for (i in 1:B) {
        #--- draw from Bern[0.3] (x distributed as Bern[0.3]) ---#
        x_seq <- runif(N) <= p

        #--- sample mean ---#
        x_mean <- mean(x_seq)

        #--- normalize ---#
        lhs <- sqrt(N) * (x_mean - p)

        #--- save lhs to storage ---#
        storage[i] <- lhs
      }

      #--- create a figure to present ---#
      ggplot() +
        geom_histogram(
          data = data.frame(x = storage),
          aes(x = x),
          color = "blue",
          fill = "gray"
        ) +
        xlab("Transformed version of sample mean") +
        ylab("Count") +
        theme_bw()
    })
  })
}

# Run the application
shinyApp(ui = ui, server = server)
```

:::
<!--end of panel-->

### OLS estimator

::: {.panel-tabset}

#### Asymptotic normality

Under assumptions $MLR.1$ through $MLR.5$, with <span style = "color: red;"> any </span> distribution of the error term (the normality assumption of the error term not necessary), OLS estimator is asymptotically normally distributed. 

<br>

**Asymptotic Normality of OLS**

$\sqrt{n}(\widehat{\beta}_j-\beta_j)\overset{a}{\longrightarrow} N(0,\sigma^2/\alpha_j^2)$

where $\alpha_j^2=plim(\frac{1}{n}\sum_{i=1}^n r^2_{i,j})$, where $r^2_{i,j}$ are the residuals from regressing $x_j$ on the other independent variables.

<br>

**Consistency**

$\widehat{\sigma}^2\equiv \frac{1}{n-k-1}\sum_{i=1}^n \widehat{u}_i^2$ is a consistent estimator of of $\sigma^2$ $(Var(u))$

#### test-statistic (small v.s. large sample)

**Small sample (any sample size)**

Under $MLR.1$ through $MLR.5$ .blue[and] $MLR.6$ $(u_i\sim N(0,\sigma^2))$,

+ $(\widehat{\beta}_j-\beta_j)/se(\widehat{\beta}_j) \sim N(0,1)$
+ $(\widehat{\beta}_j-\beta_j)/\widehat{se(\widehat{\beta}_j)} \sim t_{n-k-1}$

<br>

**Large sample (when $n$ goes infinity)**

Under $MLR.1$ through $MLR.5$ .blue[without] $MLR.6$,

+ $(\widehat{\beta}_j-\beta_j)/se(\widehat{\beta}_j) \overset{a}{\longrightarrow} N(0,1)$ 
+ $(\widehat{\beta}_j-\beta_j)/\widehat{se(\widehat{\beta}_j)} \overset{a}{\longrightarrow} N(0,1)$ 
:::
<!--end of panel-->




:::
<!--end of panel-->

## Testing under large sample

::: {.panel-tabset}

### How?

**It turns out,**

You can proceed exactly the same way as you did before (practically speaking)!!

+ calculate $(\widehat{\beta}_j-\beta_j)/\widehat{se(\widehat{\beta}_j)}$

+ check if the obtained value is greater than (in magnitude) the critical value for the specified significance level under $t_{n-k-1}$

<br>

**But,**

Shouldn't we use $N(0,1)$ when you find the critical value?

### t-distribution and normal distribution

```{r echo = F, out.width = "80%"}
x <- seq(-3, 3, length = 1000)
y_norm <- dnorm(x) # pdf of N(0,1)
y_t_2 <- dt(x, df = 2) # pdf of t_{2}
y_t_10 <- dt(x, df = 10) # pdf of t_{10}
y_t_30 <- dt(x, df = 30) # pdf of t_{30}
y_t_50 <- dt(x, df = 50) # pdf of t_{50}

plot_data <- data.table(
  x = x,
  "N(0,1)" = y_norm,
  "t (df=2)" = y_t_2,
  "t (df=10)" = y_t_10,
  "t (df=30)" = y_t_30,
  "t (df=50)" = y_t_50
) %>%
  melt(id.var = "x")

ggplot(data = plot_data) +
  geom_line(aes(y = value, x = x, color = variable)) +
  scale_color_discrete(name = "", guide = guide_legend(nrow = 2)) +
  theme_bw() +
  theme(
    legend.position = "bottom"
  )
```

<br>

Since $t_{n-k-1}$ and $N(0,1)$ are almost identical when $n$ is large, there is very little error in using $t_{n-k-1}$ instead of $N(0,1)$ to find the critical value.


### non-homosketastic error case

::: {.columns}

::: {.column width="70%"}
The consistency of the default estimation of $\widehat{Var(\widehat{\beta})}$ <span style = "color: red;"> DOES </span> require the homoskedasticity assumption (MLR.5).

In other words, the problem of using the default variance estimator under the hteroskedasticity does not go away even when the sample size is large. 

So, we should use heteroskedasticity-robust or cluster-robust standard error estimators even when the sample size is large
:::
<!--end of the 1st column-->
::: {.column width="30%"}

:::
<!--end of the 2nd column-->
:::
<!--end of the columns-->


:::
<!--end of panel-->