[
  {
    "objectID": "lectures/05-testing/05-testing.html#hypothesis-testing",
    "href": "lectures/05-testing/05-testing.html#hypothesis-testing",
    "title": "05: Hypothesis Testing",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nGeneral stepsExampleBy the waySummaryExercise\n\n\nHere is the general step of any hypothesis testing:\n\nStep 1: specify the null \\((H_0)\\) and alternative \\((H_1)\\) hypotheses\nStep 2: find the distribution of the test statistic  if the null hypothesis is true \nStep 3: calculate the test statistic based on the data and regression results\nStep 4: define the significance level\nStep 5: check how unlikely that you get the actual test statistic (found at Step 3)  if indeed the null hypothesis is true \n\n\n\n\nSettingdistirbution of \\(\\theta\\)distirbution of \\(\\theta\\) if \\(H_0\\) is trueCase 1Case 2\n\n\nGoal\nSuppose you want to test if the expected value of a normally distributed random variable \\((x)\\) is 1 or not.\n\nState of Knowledge\nWe do know \\(x\\) follows a normal distribution and its variance is 4 for some reason.\n\nYour Estimator\nYour estimator is the sample mean: \\(\\theta = \\sum_{i=1}^J x_i/J\\)\n\n\n\n\n\n\nMath Aside 1\n\n\n\\(Var(ax) = a^2 Var(X)\\)\n\n\n\n\nSo, we know that \\(\\theta \\sim N(\\alpha, 4/J)\\) (of course \\(\\alpha\\) is not known).\n\n\n\n\n\nMath Aside 2\n\n\nIf \\(x \\sim N(a, b)\\), then, \\(x-a \\sim N(0, b)\\) (shift)\n\n\n\n\nSo, \\(\\frac{x-a}{\\sqrt{b}} \\sim N(0, 1)\\) (combined with Math Aside 1)\n\nThis means,\nSince \\(\\theta = \\sum_{i=1}^J x_i/J\\) and \\(x_i \\sim N(\\alpha, 4)\\),\n\n\\(Var(\\theta) = J \\times \\frac{1}{J^2}Var(x) = 4/J\\)\n\\(\\frac{\\sqrt{J}}{2} \\cdot (\\theta - \\alpha)\\sim N(0, 1)\\).\n\n\n\n\n\nWe established that \\(\\frac{\\sqrt{J}}{2} \\cdot (\\theta - \\alpha)\\sim N(0, 1)\\).\nThe null hypothesis is \\(\\alpha = 1\\).\nIf \\(\\alpha = 1\\) is indeed true, then \\(\\sqrt{J} \\times (\\theta - 1)/2 \\sim N(0, 1)\\).\nIn other words, if you multiply the sample mean by the square root of the number of observations and divide it by 2, then it follows the standard normal distribution like below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have obtained 100 samples \\((J = 100)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 2.\nThen, your test statistic is \\(\\sqrt{100} \\times (2-1)/2 = 5\\).\nHow unlikely is it to get the number you got (5) if the null hypothesis is indeed true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have obtained 400 samples \\((J = 400)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 1.02.\nThen, your test statistic is \\(\\sqrt{400} \\times (1.02-1)/2 = 0.2\\).\nHow unlikely is it to get the number you got (0.2) if the null hypothesis is indeed true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that you do not really need to use \\(\\sqrt{J} \\times (\\theta - \\alpha)/2\\) as your test statistic.\nYou could alternatively use \\(\\theta - \\alpha\\). But, in that case, you need to be looking at \\(N(0, 4/J)\\) instead of \\(N(0, 1)\\) to see how unlikely you get the number you got.\nFor example, when the number of observations is 100 \\((J = 100)\\), the distribution of \\(\\theta-\\alpha\\) looks like the figure on the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconsider the case 1\nSuppose you have obtained 100 samples \\((J = 100)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 2.\nThen, your test statistic is \\(2-1 = 1\\).\nIs it unlikely for you to get 1 if the null hypothesis is true?\nThe conclusion would be exactly the same as using \\(\\sqrt{J} \\times (\\theta - \\alpha)/2\\) because the distribution under the null is adjusted according to the test statistic you use.\n\n\n\nNote\n\n\nWe always use normalize test statistic so that we can always look up the same distribution.\n\n\n\n\n\nWhat do we need?\n\ntest-statistic of which we know the distribution (e.g., t-distribution, Normal distribution) assuming the null hypothesis\n\n\nWhat do we (often) do?\n\ntransform (most of the time) a raw random variable (e.g., sample mean in the example above) into a test statistic of which we know the distribution assuming that the null hypothesis is true\n\ne.g., we transformed the sample mean so that it follows the standard Normal distribution.\n\ncheck if the actual number you got from the test statistic is likely to happen or not (formal criteria has not been discussed yet)\n\n\n\n\nProblemAnswer\n\n\nYou have collected data on annual salary for those who graduated from University A and B. You are interested in testing whether the difference in annual salary between the universities (call it \\(x\\)) is 10 on average. You know (for unknown reasons) know that the difference is distributed as \\(N(\\alpha, 16)\\).\n\nWhat is the null hypothesis?\nUnder the null hypothesis, what is the distribution of the sample mean when the number of observation is 400?\nNormalize the test statistic so that the transformed version follows \\(N(0, 1)\\).\nThe actual difference you observed is 10.2. What is the probability that you observe a number greater than 10.2 if the null hypothesis is true? Use prnom().\n\n\n\n\nNote\n\n\nIn reality,\n\nwe need to find out what the distribution of the test statistic is\nwe need to formerly define when we accept or not accept the null hypothesis\n\n\n\n\n\n\n\n\\(\\alpha = 10\\)\n\\(\\theta \\sim N(\\alpha, 16/400)\\)\n\\(\\sqrt{\\frac{400}{16}}\\cdot (\\theta - \\alpha)\\)\nThe test statistic is \\(5 \\times (10.2 - 10) = 1\\)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#hypothesis-testing-of-the-coefficients",
    "href": "lectures/05-testing/05-testing.html#hypothesis-testing-of-the-coefficients",
    "title": "05: Hypothesis Testing",
    "section": "Hypothesis Testing of the Coefficients",
    "text": "Hypothesis Testing of the Coefficients\n\nExampleDistribution of \\(\\beta_j\\)t-testRecap on notations\n\n\nConsider the following model,\n\\[wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\\]\n\n\n\n\nExample Hypotheses\n\n\n\neducation has no impact on wage \\((\\beta_1=0)\\)\nexperience has a positive impact on wage \\((\\beta_2&gt;0)\\)\n\n\n\n\n\n\n\nIf \\(\\widehat{\\beta}_1\\) is non-random, but just a scalar, all you have to do is just check if \\(\\widehat{\\beta}_1=0\\) or not\nBut, the estimate you get is  just one realization  of the range of values \\(\\widehat{\\beta}_1\\) could take because it is a random varaible\nThis means that even if \\(\\beta_1=0\\) in the population, it is possible to get an estimate that is very far from 0\n\n\n\n\nAssumptionAdditional assumptionImplications of the Assumptions\n\n\nSo far, we learned that:\n\nOLS estimators are unbiased under MLR.1 ~ MLR.4\nVariance of the OLS estimator of \\(\\beta_j\\) is \\(\\frac{\\sigma^2}{SST_x\\cdot (1-R^2_j)}\\) under MLR.1 ~ MLR.5\n\nWe have  NOT  made any assumptions about the distribution of the error term!!\nIn order to perform hypothesis testing, we need to make assumptions about the distribution of error term (this is not strictly true, but more on this later)\n\n\nFor the purpose of hypothesis testing, we will make the following assumption:\n\n\n\n\nNormality of the error term\n\n\nThe population error \\(u\\) is  independent  of the explanatory variables \\(x_1,\\dots,x_k\\) and is  normally  distributed with zero mean and variance \\(\\sigma^2\\):\n\\[u\\sim N(0,\\sigma^2)\\]\n\n\n\n\n\n\n\nNote\n\n\nThe normality assumption is much more than error term being distributed as Normal.\nIndependence of the error term implies\n\n\\(E[u|x] = 0\\)\n\\(Var[u|x]= \\sigma^2\\)\n\nSo, we are necessarily assuming MLR.4 and MLR.5 hold by the independence assumption.\n\n\n\n\n\n\n\ndistribution of the dependent variable\nThe distribution of \\(y\\) conditional on \\(x\\) is a Normal distribution\n\\(y|x \\sim N(\\beta_0+\\beta_1 x_1+\\dots+\\beta_k x_k,\\sigma^2)\\)\n\n\\(E[y|x]\\) is \\(\\beta_0+\\beta_1 x_1+\\dots+\\beta_k x_k\\)\n\\(u|x\\) is \\(N(0,\\sigma^2)\\)\n\n\ndistribution of the OLS estimator\nIf the MLR.1 through MLR.6 are satisfied, \\(OLS\\) estimators are also Normally distributed!\n\\(\\widehat{\\beta}_j \\sim N(\\beta_j,Var(\\widehat{\\beta}_j))\\)\nwhich means,\n\\(\\frac{\\widehat{\\beta}_j-\\beta_j}{se(\\widehat{\\beta}_j)} \\sim N(0,1)\\)\n\n\n\n\n\n\n\nQuestion\n\n\nOkay, so are we going to use this for testing involving \\(\\beta_j\\)?\n\n\n\n\n\n\n\n\n\n\nIn practice, we need to estimate \\(se(\\beta_j)\\). If we use \\(\\widehat{se(\\widehat{\\beta}_j)}\\) instead of \\(se(\\widehat{\\beta}_j)\\), then,\n\\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}} \\sim t_{n-k-1}\\)\nwhere \\(n-k-1\\) is the degree of freedom of residual.\n(Note: \\(\\widehat{se(\\widehat{\\beta}_j)} = \\widehat{\\sigma}^2/\\big[SST_X\\cdot (1-R_j^2)\\big]\\))\n\n\n\n\\(k\\): the number of explanatory variables included except the intercept\n\\(\\sigma^2\\): the  true  variance of the error term\n\\(\\widehat{\\sigma^2}\\): the estimator (estimate) of the variance of the error term\n\n\\(\\widehat{\\sigma^2} = \\frac{\\sum \\widehat{u}_i^2}{n-k-1}\\)\n\n\\(SST_X = \\sum (x_i - \\bar{x})^2\\)\n\\(\\widehat{\\beta}_j\\): OLS estimator (estimate) on explanatory variable \\(x_j\\)\n\n\\(\\widehat{\\beta} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{SST_X}\\) (for simple univariate regression)\n\n\\(var(\\widehat{\\beta}_j)\\): the  true  variance of \\(\\widehat{\\beta}_j\\)\n\\(R^2_j\\): \\(R^2\\) when you regres \\(x_j\\) on all the other covariates (mathematical expression omitted)\n\\(\\widehat{var(\\widehat{\\beta}_j)}\\): Estimator (estimate) of \\(var(\\widehat{\\beta}_j)\\)\n\n\\(\\widehat{var(\\widehat{\\beta}_j)} = \\frac{\\widehat{\\sigma^2}}{SST_X\\cdot(1-R^2_j)}\\)\n\n\\(se(\\widehat{\\beta}_j)\\): square root of \\(var(\\widehat{\\beta}_j)\\)\n\\(\\widehat{se(\\widehat{\\beta}_j)}\\): square root of \\(\\widehat{var(\\widehat{\\beta}_j)}\\)"
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#details-of-hypothesis-testing-on-beta_j",
    "href": "lectures/05-testing/05-testing.html#details-of-hypothesis-testing-on-beta_j",
    "title": "05: Hypothesis Testing",
    "section": "Details of hypothesis testing on \\(\\beta_j\\)",
    "text": "Details of hypothesis testing on \\(\\beta_j\\)\n\nNull and alternative hypothesisSignificance levelp-valueExample\n\n\n\nIntroductionOne-sided and Two-sided Alternatives Hypotheses\n\n\nStatistical hypothesis testing involves two hypotheses: Null and Alternative hypotheses.\nPretend that you are an attorney who indicted a defendent who you think commited a crime.\n\nNull Hypothesis\nHypothesis that you would like to reject (defendent is not guilty)\n\nAlternative Hypothesis\nHypothesis you are in support of (defendent is guilty)\n\n\none-sided alternative\n\\(H_0:\\) \\(\\beta_j = 0\\) \\(H_1:\\) \\(\\beta_j &gt; 0\\)\nYou look at the positive end of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level).\n\ntwo-sided alternative\n\\(H_0:\\) \\(\\beta_j = 0\\) \\(H_1:\\) \\(\\beta_j \\ne 0\\)\nYou look at the both ends of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level).\n\n\n\n\n\n\n\nDefinitionOne-sided: 5% significanceOne-sided: 1% significanceTwo-sided: 5% significance\n\n\n\n\n\n\nDefinition\n\n\nThe probability of rejecting the null when the null is actually true (The probability that you wrongly claim that the null hypothesis is wrong even though it’s true in reality: Type I error)\n\n\n\n\nThe lower the significance level, you are more sure that the null is indeed wrong when you reject the null hypothesis\n\n\n\n\n\nFigure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value larger than 1.66 is 5% (0.05 in area).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the t-statistic is greater than 1.66 (95% quantile of the t-distribution)\n\n\n\n\nIf you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n5% is the  significance level\n1.66 is the  critical value above which you will reject the null\n\n\n\n\n\n\nThe figure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value larger than 2.37 is 1% (0.01 in area).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the t-statistic is greater than 2.37 (99% quantile of the t-distribution)\n\n\n\n\nIf you follow the decision rule, then you have a 1% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n1% is the  significance level \n2.37 is the  critical value  above which you will reject the null\n\n\n\n\n\n\nThe figure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value more extreme than 1.96 or -1.96 is 5% (0.05 in area cobining the two area at the edges).\n\n(Note: irrespective of the type of tests, the distribution of t-statistics is the same.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the absolute value of the t-statistic is greater than 1.96.\n\n\n\n\nIf you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n5% is the  significance level \n1.96 is the  critical value  above which you will reject the null\n\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe smallest significance level at which the null hypothesis would be rejected (the probability of observing a test statistic at least as extreme as we did if the null hypothesis is true)\n\n\n\n\n\n\n\nSuppose the t-statistic you got is 2.16. Then, there’s a 3.1% chance you reject the null when it is actually true, if you use it as the critical value.\nSo, the lower significance level the null hypothesis is rejected is 3.1%, which is the definition of p-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision rule\n\n\nIf the p-value is lower than your choice of significance level, then you reject the null.\n\n\n\n\nThis decision rule of course results in the same test results as the one we saw that uses a t-value and critical value.\n\n\nEstimated Model\nThe impact of experience on wage:\n\n\\(log(wage) = 0.284+0.092\\times educ+0.0041\\times exper + 0.022 \\times tenure\\)\n\\(\\widehat{se(\\widehat{\\beta}_{exper})} = 0.0017\\)\n\\(n = 526\\)\n\n\nHypothesis\n\n\\(H_0\\): \\(\\beta_{exper}=0\\)\n\\(H_1\\): \\(\\beta_{exper}&gt;0\\)\n\n\nTest\nt-statistic \\(= 0.0041/0.0017 = 2.41\\)\nThe critical value is the 99% quantile of \\(t_{526-3-1}\\), which is \\(2.33\\) (it can be obtained by qt(0.95, 522))\nSince \\(2.41 &gt; 2.33\\), we reject the null in favor of the alternative hypothesis at the 1% level."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#r-implementation",
    "href": "lectures/05-testing/05-testing.html#r-implementation",
    "title": "05: Hypothesis Testing",
    "section": "R implementation",
    "text": "R implementation\n\nRun regressionObtain t-statisticsR implementation (by hand)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nYou can apply the broom::tidy() function from the broom package to access the regression results (reg_wage here) as a tibble (data.frame).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nestimate: coefficient estimates \\((\\widehat{\\beta}_j)\\)\nstd.error: \\(\\widehat{se(\\widehat{\\beta}_j}\\))\nstatistic: t-statistic for the null of \\(\\beta_j = 0\\)\np.value: p-value (for the two sided test with the null of \\(\\beta_j = 0\\))\n\nSo, for the t-test of of \\(\\beta_j = 0\\) is already there. You do not need to do anything further.\nFor the null hypothesis other than \\(\\beta_j = 0\\), you need further work.\n\n\nSuppose you are testing the null hypothesis of \\(\\beta_{educ} = 1\\) against the alternative hypothesis of \\(\\beta_{educ} \\ne 1\\) (so, this is a two-sided test).\nThe t-value for this test is not available from the summary.\n\nget t-valuecritical value and t-test\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nThe degree of freedom \\((n-k-1)\\) of the t-distribution can be obtained by applying degrees_freedom(reg_wage, \"resid\"):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can get the 97.5% quantile of the \\(t_{522}\\) using the qt() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSince the absolute value of the t-value (-7.8199528) is greater than the critical value (1.9645189), you reject the null."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#confidence-interval-ci-1",
    "href": "lectures/05-testing/05-testing.html#confidence-interval-ci-1",
    "title": "05: Hypothesis Testing",
    "section": "Confidence Interval (CI)",
    "text": "Confidence Interval (CI)\n\nDefinitionHow to get CI (in general)How to get the CI of coefficientsExample (R implementation)\n\n\n\n\n\n\nDefinition\n\n\nIf you calculate the 95% confidence interval on multiple different samples, 95% of the time, the calculated CI includes the true parameter\n\n\n\n\n\nWhat confidence interval is not\nThe probability that a realized CI calculated from specific sample data includes the true parameter\n\n\n\nGeneral ProcedureExample\n\n\nFor the assumed distribution of statistic \\(x\\), the \\(A\\%\\) confidence interval of \\(x\\) is the range with\n\nlower bound: 100 − A/2 percent quantile of \\(x\\)\nupper bound: 100-(100 − A)/2 percent quantile of \\(x\\)\n\n\n\n\n\nFor the 95% CI (A = 95),\n\nlower bound: 2.5 (100 − 95/2) percent quantile of \\(x\\)\nupper bound: 97.5 (100-(100 − 95)/2) percent quantile of \\(x\\)\n\nIf \\(x\\) follos the standard normal distribution \\((x \\sim N(0, 1))\\), then,the 2.5% and 97.5% quantiles are -1.96 and 1.96, respectively. So, the 95% CI of \\(x\\) is [-1.96, 1.96].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder the assumption of MLR.1 through MLR.6 (which includes the normality assumption of the error), we learned that\n\\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}} \\sim t_{n-k-1}\\)\nSo, following the general procedure we discussed in the previous slide, the A% confidence interval of \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) is\n\nlower bound: \\((100 − A)/2\\)% quantile of the \\(t_{n-k-1}\\) distribution (let’s call this \\(Q_l\\))\nupper bound: \\(100 - (100 − A)/2\\)% quantile of the \\(t_{n-k-1}\\) distribution (let’s call this \\(Q_h\\))\n\nBut, we want the A% CI of \\(\\beta_j\\), not \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\). Solving for \\(\\beta_j\\),\n\\(\\beta_j = t_{n-k-1}\\times \\widehat{se(\\widehat{\\beta}_j)} + \\widehat{\\beta}_j\\)\nSo, to get the A% CI of \\(\\beta_j\\), we scale the CI of \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) by \\(se(\\widehat{\\beta}_j)\\) and then shift by \\(\\widehat{\\beta}_j\\).\n\nlower bound: \\(Q_l \\times se(\\widehat{\\beta}_j) + \\widehat{\\beta}_j\\)\nupper bound: \\(Q_h \\times se(\\widehat{\\beta}_j) + \\widehat{\\beta}_j\\)\n\nNote that \\(Q_l\\) is negative and \\(Q_h\\) is negative.\n\n\n\nRegressionCollect necessary informationFind CIIn practice\n\n\nRun OLS and extract necessary information\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nApplying broom::tidy() to wage_reg,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe are interested in getting the 90% confidence interval of the coefficient on educ \\((\\beta_{educ})\\). Under all the assumptions (MLR.1 through MLR.6), we know that in general,\n\\(\\frac{\\widehat{\\beta}_{educ} - \\beta_{educ}}{\\widehat{se(\\widehat{\\beta}_{educ})}} \\sim t_{n-k-1}\\)\nSpecifically for this regression,\n\n\\(\\widehat{\\beta}_{educ}\\) = 0.5989651\n\\(\\widehat{se(\\widehat{\\beta}_{educ})}\\) = 0.0512835\n\\(n - k - 1 = 490\\) (degrees of freedom)\n\n\n\n\n\nNow, we need to find the 5% ((100-90)/2) and 95% (100-(100-90)/2) quantile of \\(t_{522}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSo, the 90% CI of \\(\\frac{0.599 - \\beta_{educ}}{0.051} \\sim t_{522}\\) is [-1.6477779, 1.6477779]\nBy scaling and shifting, the lower and upper bounds of the 90% CI of \\(\\beta_{educ}\\) are:\n\nlower bound: 0.599 + 0.051 \\(\\times\\) -1.6477779 = 0.5149633\nupper bound: 0.599 + 0.051 \\(\\times\\) 1.6477779 = 0.6830367\n\n\nThe distribution of \\(t_{522}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can just use broom::tidy() with conf.int = TRUE, conf.level = confidence level like below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#linear-combination-of-multiple-coefficients-1",
    "href": "lectures/05-testing/05-testing.html#linear-combination-of-multiple-coefficients-1",
    "title": "05: Hypothesis Testing",
    "section": "Linear Combination of Multiple Coefficients",
    "text": "Linear Combination of Multiple Coefficients\n\nExampleRewrite the hypothesisMathGoing back to the exampleget \\(se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)\\)Demonstration using R\n\n\nModel\n\\[log(wage) = \\beta_0+\\beta_1 jc+ \\beta_2 univ + \\beta_3 exper + u\\]\n\n\\(jc\\): 1 if you attended 2-year college, 0 otherwise\n\\(univ\\): 1 if you attended 4-year college, 0 otherwise\n\nQuestion\nDoes the impact of education on wage is greater if you attend a 4-year college than 2-year college?\nHypothesis\nThe null and alternative hypotheses would be:\n\n\\(H_1:\\) \\(\\beta_1 &lt; \\beta_2\\)\n\\(H_0:\\) \\(\\beta_1 = \\beta_2\\)\n\n\n\nThe null and alternative hypotheses are:\n\n\\(H_1:\\) \\(\\beta_1 &lt; \\beta_2\\)\n\\(H_0:\\) \\(\\beta_1 = \\beta_2\\)\n\nRewriting them,\n\n\\(H_1:\\) \\(\\beta_1-\\beta_2 &lt; 0\\)\n\\(H_0:\\) \\(\\beta_1 - \\beta_2 =0\\)\n\nOr,\n\n\\(H_1:\\) \\(\\alpha &lt; 0\\)\n\\(H_0:\\) \\(\\alpha =0\\)\n\nwhere \\(\\alpha = \\beta_1 - \\beta_2\\)\nNote that \\(\\alpha\\) is a linear combination of \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\n\n\n\n\nImportant Fact\n\n\nFor any linear combination of the OLS coefficients, denoted as \\(\\widehat{\\alpha}\\), the following holds:\n\\[\\frac{\\widehat{\\alpha}-\\alpha}{\\widehat{se(\\widehat{\\alpha})}} \\sim t_{n-k-1}\\]\nWhere \\(\\alpha\\) is the true value (it is \\(\\beta_1 - \\beta_2\\) in the example in the previous slide).\n\n\n\n\n\nSo, using the example, this means that\n\\[\\frac{\\widehat{\\alpha}-\\alpha}{\\widehat{se(\\widehat{\\alpha})}} = \\frac{\\widehat{\\beta}_1-\\widehat{\\beta}_2-(\\beta_1 - \\beta_2)}{\\widehat{se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)}} \\sim t_{n-k-1}\\]\nThis is great because we know how to do t-test!\n\n\nOur null hypothesis is \\(\\alpha = 0\\) (or \\(\\beta_1 - \\beta_2 = 0\\)).\nSo,  If  indeed the null hypothesis is true, then\n\\[\\frac{\\widehat{\\alpha}-0}{\\widehat{se(\\widehat{\\alpha})}} = \\frac{\\widehat{\\beta}_1-\\widehat{\\beta}_2-0}{\\widehat{se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)}} \\sim t_{n-k-1}\\]\nSo, all you need to do is to substitute \\(\\widehat{\\beta}_1\\), \\(\\widehat{\\beta}_2\\), \\(\\widehat{se(\\widehat{\\beta}_1 - \\widehat{\\beta}_2)}\\) into the formula and see if the value is beyond the critical value for your chosen level of statistical significance.\n\n\nBut,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)= \\sqrt{Var(\\widehat{\\beta}_1-\\widehat{\\beta}_2}) \\ne \\sqrt{Var(\\widehat{\\beta}_1)+Var(\\widehat{\\beta}_2)}\\]\nIf the following was true,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2) = \\sqrt{Var(\\widehat{\\beta}_1)+Var(\\widehat{\\beta}_2)}\\]\nthen, we could have just extracted \\(Var(\\widehat{\\beta}_1)\\) and \\(Var(\\widehat{\\beta}_2)\\) individually from the regression object on R, sum them up, and take a square root of it.\n\n\n\n\nMath aside\n\n\n\\[Var(ax+by) = a^2 Var(x) + 2abCov(x,y) + b^2 Var(y)\\]\n\n\n\n\nSo,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)= \\sqrt{Var(\\widehat{\\beta}_1-\\widehat{\\beta}_2}) = \\sqrt{Var(\\widehat{\\beta}_1)-2Cov(\\widehat{\\beta}_1,\\widehat{\\beta}_2)+Var(\\widehat{\\beta}_2)}\\]\n\n\n\nRegressionVariance covariance matrixCalculate the t-statistic\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDefinition\n\n\nVariance covariance matrix is a matrix where\n\n\\(VCOV_{i,i}\\): the variance of \\(i\\)th variable’s coefficient estimator\n\\(VCOV_{i,j}\\): the covariance between \\(i\\)th and \\(j\\)th variables’ estimators\n\n\n\n\n\n\nYou can get it by applying vcov() to regression results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor example, \\(vcov_{sc}[2, 2]\\) is the variance of \\(\\widehat{\\beta}_{jc}\\), and \\(vcov_{sc}[2, 3]\\) is the covariance between \\(\\widehat{\\beta}_{jc}\\) and \\(\\widehat{\\beta}_{univ}\\).\n\n\n\n\nGet coefficient estimates:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalcualt t-statistic:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#multiple-linear-restrictions-f-test-1",
    "href": "lectures/05-testing/05-testing.html#multiple-linear-restrictions-f-test-1",
    "title": "05: Hypothesis Testing",
    "section": "Multiple Linear Restrictions: F-test",
    "text": "Multiple Linear Restrictions: F-test\n\nExampleIndividual t-testF-testSSRF-test in generalF-test stepsF-test by handF-test (easier way)\n\n\n\nModel and hypothesisQuestions\n\n\nModel\n\\[log(salary) =  \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u\\]\n\n\\(salary\\): salary in 1993\n\\(years\\): years in the league\n\\(gamesyr\\): average games played per year\n\\(bavg\\): career batting average\n\\(hrunsyr\\): home runs per year\n\\(rbisyr\\): runs batted in per year\n\nHypothesis\nOnce years in the league and games per year have been controlled for, the statistics measuring performance ( \\(bavg\\), \\(hrunsyr\\), \\(rbisyr\\)) have no effect on salary collectively.\n\\(H_0\\): \\(\\beta_3=0\\), \\(\\beta_4=0\\), and \\(\\beta_5=0\\)\n\\(H_1\\): \\(H_0\\) is not true\n\n\nHow do we test this?\n\n\\(H_0\\) holds if all of \\(\\beta_3\\), \\(\\beta_4\\), or \\(\\beta_5\\) are zero.\nConduct t-test for each coefficient individually?\n\n\n\n\n\n\n\n\nRun regressionQuestionAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhat do you find?\n\n\nNone of the coefficients on bavg, hrunsyr, and rbisyr is statistically significantly different from 0 even at the 10% level!!\nSo, does this mean that they collectively have no impact on the salary of MLB players?\nIf you were to conclude that they do not have statistically significant impact jointly, you would turn out to be wrong!!\n\\(SSR\\) (or \\(R^2\\)) turns out to be useful for testing their impacts jointly.\n\n\n\n\n\n\nIn doing an F-test of the null hypothesis, we compare sum of squared residuals \\((SSR)\\) of two models:\n\nUnrestricted Model\n\\[log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u\\]\n\nRestricted Model\n\\[log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + u\\]\nThe coefficients on \\(bavg\\), \\(hrunsyr\\), and \\(rbisyr\\) are restricted to be 0 following the null hypothesis.\n\nQuestionAnswer\n\n\nIf the null hypothesis is indeed true, then what do you think is going to happen if you compare the \\(SSR\\) of the two models? Which one has a bigger \\(SSR\\)?\n\n\n\\(SSR\\) from the restricted model should be large because the restricted model has a smaller explanatory power than the unrestricted model.\n\n\n\n\n\n\nSSR of the unrestricted model: \\(SSR_u\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSSR of the restricted model: \\(SSR_r\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQuestion 1Answer 1Question 2Answer 2\n\n\nWhat does \\(SSR_r - SSR_u\\) measure?\n\n\nThe contribution from the three excluded variables in explaining the dependent variable.\n\n\nIs the contribution large enough to say that the excluded variables are important?\n\n\nCannot tell at this point because we do not know the distribution of the difference!\n\n\n\n\n\n\nSetup\nConsider a following general model:\n\\[\ny = \\beta_0 +\\beta_1 x_1 + \\dots+\\beta_k x_k +u\n\\]\nSuppose we have \\(q\\) restrictions to test: that is, the null hypothesis states that \\(q\\) of the variables have zero coefficients.\n\\[H_0: \\beta_{k-q+1} =0, \\beta_{k-q+2} =0, \\dots, \\beta_k=0\\]\nWhen we impose the restrictions under \\(H_0\\), the restricted model is the following:\n\\[y = \\beta_0 +\\beta_1 x_1 + \\dots+\\beta_{k-q} x_{k-q} + u\\]\nF-statistic\n If  the null hypothesis is true, then,\n\\[F = \\frac{(SSR_r-SSR_u)/q}{SSR_u/(n-k-1)} \\sim F_{q,n-k-1}\\]\n\n\\(q\\): the number of restrictions\n\\(n-k-1\\): degrees of freedom of residuals\n\n\nQuestionAnswerQuestion 2Answer 2\n\n\nIs the above \\(F\\)-statistic always positive?\n\n\nYes, because \\(SSR_r-SSR_u\\) is always positive.\n\n\nThe greater the joint contribution of the \\(q\\) variables, the (greater or smaller) the \\(F\\)-statistic?\n\n\nGreater.\n\n\n\n\n\n\n\n\nF-distribution\n\n\n\n\n\n\n\n\n\n\nF-test steps\n\nDefine the null hypothesis\nEstimate the unrestricted and restricted models to obtains their \\(SSR\\)\nCalculate \\(F\\)-statistic\nDefine the significance level and corresponding critical value according to the F distribution with appropriate degrees of freedoms\nReject if your \\(F\\)-statistic is greater than the critical value, otherwise do not reject\n\n\n\n\n\n\n\nStep 1Step 2Steps 3 and 4\n\n\nEestimate the unrestricted and restricted models\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCalculate F-stat\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFind the critical value\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIs F-stat &gt; critical value?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nObservation\n\n\nSo, the performance variables have statistically significant impacts on salary  jointly!!\nWhat happened?\n\n\n\n\n\n\n\n\n\n\n\nHowDemonstration\n\n\nYou can use the car::linearHypothesis() function from the car package.\n\nSyntax\n\ncar::linearHypothesis(regression, hypothesis)\n\n\nregression: the name of the regression results of the unrestricted model\nhypothesis” text of null hypothesis. For example,\n\nc(\"x1 = 0\", \"x2 = 1\") means the coefficients on \\(x1\\) and \\(x2\\) are \\(0\\) and \\(1\\), respectively\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#simulation-multicollinearity-t-test-and-f-test",
    "href": "lectures/05-testing/05-testing.html#simulation-multicollinearity-t-test-and-f-test",
    "title": "05: Hypothesis Testing",
    "section": "Simulation (multicollinearity, t-test, and F-test)",
    "text": "Simulation (multicollinearity, t-test, and F-test)\n\nData generationRegressionF-testImportant\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nx1 and x2 are highly correlated with each other:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBoth x1 and x2 are statistically insignificant individually.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe F-statistic for the hypothesis testing is:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe F-statistic is very high, meaning they collectively affect the dependent variable significantly.\n\n\n\n\n\n\n\nThe standard error estimates of the coefficients on x1 and x2 are very high because they are so highly correlated that your estimation of the model had such a difficult time to distinguish the their individual impacts.\nBut, collectively, they have large impacts. \\(F\\)-test was able to detect the statistical significance of their impacts  collectively."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#mlb-example",
    "href": "lectures/05-testing/05-testing.html#mlb-example",
    "title": "05: Hypothesis Testing",
    "section": "MLB example",
    "text": "MLB example\n\nCorrelation checkMultiple coefficients again\n\n\nHere is the correlation coefficients between the three variables:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you can see, brunsyr and hrunsyr are highly correlated with each other.\nThey are not so highly correlated with bavg.\n\n\n\n\nThe test of a linear combination of the parameters we looked at earlier is a special case of F-test where the number of restriction is 1.\nIt can be shown that square root of \\(F_{1, df}\\) follows the \\(t_{df}\\) distribution.\nSo, we can actually use \\(F\\)-test for this type of hypothesis testing because \\(F_{1,t-n-k} \\sim t_{t-n-k}^2\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCheck with the previous slide of the same t-test and confirm that the t-statistics we got there and here are the same.\n\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#univariate-vs-multivariate-regression-models",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#univariate-vs-multivariate-regression-models",
    "title": "02: Multivariate Regression",
    "section": "Univariate vs Multivariate Regression Models",
    "text": "Univariate vs Multivariate Regression Models\n\nBi-variate vs. Uni-variateExampleExampleModel (general)\n\n\nUnivariate\nThe most important assumption \\(E[u|x] = 0\\) (zero conditional mean) is almost always violated (unless you data comes from randomized experiments) because all the other variables are sitting in the error term, which can be correlated with \\(x\\).\n\nMultivariate\nMore independent variables mean less factors left in the error term, which makes the endogeneity problem  less severe\n\n\nUni-variate vs. bi-variate\n\\[\\begin{align}\n  \\mbox{Uni-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + u_1 (=u_2+\\beta_2 exper)\\\\\n  \\mbox{Bi-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u_2\n\\end{align}\\]\n\nWhat’s different?\n\nuni-variate: \\(\\widehat{\\beta}_1\\) is biased unless experience is uncorrelated with education because experience was in error term\nbi-variate: able to measure the effect of education on wage,  holding experience fixed  because experience is modeled explicitly ( We say \\(exper\\) is controlled for. )\n\n\n\nThe impact of per student spending (expend) on standardized test score (avgscore) at the high school level\n\\[\\begin{align}\navgscore= & \\beta_0+\\beta_1 expend + u_1 (=u_2+\\beta_2 avginc) \\notag \\\\\navgscore= & \\beta_0+\\beta_1 expend +\\beta_2 avginc + u_2 \\notag\n\\end{align}\\]\n\n\nMore generally,\n\\[\\begin{align}\n  y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + u\n\\end{align}\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\\(\\beta_2\\): measure the change in \\(y\\) with respect to \\(x_2\\), holding other factors fixed"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "title": "02: Multivariate Regression",
    "section": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator",
    "text": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator\n\nUni-variate v.s. Bi-variateMean independence condition: example\n\n\nUni-variate\n\\(y = \\beta_0 + \\beta_1x + u\\),\n\\(E[u|x]=0\\)\n\nBi-variate\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\),\n\nMathematically: \\(E[u|x_1,x_2]=0\\)\nVerbally: for any values of \\(x_1\\) and \\(x_2\\), the expected value of the unobservables is zero\n\n\n\nIn the following wage model,\n\\[\\begin{align*}\nwage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\n\\end{align*}\\]\nMean independence condition is\n\\[\\begin{align}\n  E[u|educ,exper]=0\n\\end{align}\\]\nVerbally:\nThis condition would be satisfied if innate ability of students is on average unrelated to education level and experience."
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-model-with-k-independent-variables",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-model-with-k-independent-variables",
    "title": "02: Multivariate Regression",
    "section": "The model with \\(k\\) independent variables",
    "text": "The model with \\(k\\) independent variables\n\nGeneral modelImplementation (R)Present results(Math: derive OLS estimator)\n\n\nModel\n\\[\\begin{align}\n  y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\n\\end{align}\\]\n\nMean independence assumption?\n\\(\\beta_{OLS}\\) (OLS estimators of \\(\\beta\\)s) is unbiased if,\n\\[\\begin{align}\n    E[u|x_1,x_2,\\dots,x_k]=0\n\\end{align}\\]\nVerbally: this condition would be satisfied if the error term is uncorrelated wtih any of the independent variables, \\(x_1,x_2,\\dots,x_k\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhen you are asked to present regression results in assignments or your final paper, use the msummary() function from the modelsummary package.\n\n\n\n\nlibrary(modelsummary)\n\n#* run regression\nreg_results &lt;- feols(speed ~ dist, data = cars)\n\n#* report regression table\nmsummary(\n  reg_results,\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n)\n\n\n\n\n \n\n  \n    \n    \n    tinytable_n6m414tm0hp4c5r9437x\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  8.284***\n                \n                \n                             \n                  (0.874) \n                \n                \n                  dist       \n                  0.166***\n                \n                \n                             \n                  (0.017) \n                \n                \n                  Num.Obs.   \n                  50      \n                \n                \n                  R2         \n                  0.651   \n                \n                \n                  RMSE       \n                  3.09    \n                \n                \n                  Std.Errors \n                  IID     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\nOLS\nFind the combination of \\(\\beta\\)s that minimizes the sum of squared residuals\n\nSo,\nDenoting the collection of \\(\\widehat{\\beta}\\)s as \\(\\widehat{\\theta} (=\\{\\widehat{\\beta}_0,\\widehat{\\beta}_1,\\dots,\\widehat{\\beta}_k\\})\\),\n\\[\\begin{align}\n    Min_{\\theta} \\sum_{i=1}^n \\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\widehat{\\beta}_k x_{k,i}) \\Big]^2\n\\end{align}\\]\nFind the FOCs by partially differentiating the objective function (sum of squared residuals) wrt each of \\(\\widehat{\\theta} (=\\{\\widehat{\\beta}_0,\\widehat{\\beta}_1,\\dots,\\widehat{\\beta}_k\\})\\),\n\\[\\begin{align}\n  \\sum_{i=1}^n(y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]\nOr more succinctly,\n\\[\\begin{align}\n  \\sum_{i=1}^n \\widehat{u}_i = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#unbiasedness-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#unbiasedness-of-ols-estimators",
    "title": "02: Multivariate Regression",
    "section": "Unbiasedness of OLS Estimators",
    "text": "Unbiasedness of OLS Estimators\n\nConditions (four)Perfect CollinearityEndogeneity (Definition)\n\n\n\n\n\nImportant\n\n\nOLS estimators of multivariate models are unbiased if the following conditions are satisfied.\n\n\n\n\n\n\nCondition 1\nYour model is correct (Assumption \\(MLR.1\\))\n\nCondition 2\nRandom sampling (Assumption \\(MLR.2\\))\n\n\n\n\n\nCondition 3\nNo perfect collinearity (Assumption \\(MLR.3\\))\n\nCondition 4\nZero Conditional Mean (Assumption \\(MLR.4\\)) \\(E[u|x_1,x_2,\\dots,x_k]=0 \\;\\;\\mbox{(Assumption MLR.4)}\\)\n\n\n\n\n\nNo Perfect Collinearity (\\(MLR.3\\))\nAny variable cannot be a linear function of the other variables\n\nExample (silly)\n\\[\\begin{align}\n  wage = \\beta_0 + \\beta_1 educ + \\beta_2 (3\\times educ) + u\n\\end{align}\\]\n( More on this later when we talk about dummy variables)\n\n\n\n\n\n\nEndogeneity: Definition\n\n\n\\[\nE[u|x_1,x_2,\\dots,x_k] = f(x_1,x_2,\\dots,x_k) \\ne 0\n\\]\n\n\n\n\n\nWhat could cause endogeneity problem?\n\nfunctional form misspecification\n\n\\[\\begin{align}\n  wage = & \\beta_0 + \\beta_1 log(x_1) + \\beta_2 x_2 + u_1 \\;\\;\\mbox{(true)}\\\\\n  wage = & \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u_2 (=log(x_1)-x_1) \\;\\; \\mbox{(yours)}\n\\end{align}\\]\n\nomission of variables that are correlated with any of \\(x_1,x_2,\\dots,x_k\\) ( more on this soon )\n other sources of enfogeneity later"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#variance-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#variance-of-ols-estimators",
    "title": "02: Multivariate Regression",
    "section": "Variance of OLS estimators",
    "text": "Variance of OLS estimators\n\nHomoskedasticity and Variance of \\(\\widehat{\\beta}_{OLS}\\)Estimating \\(\\sigma^2\\)Estimator of \\(Var{\\widehat{\\beta}_j}\\)\n\n\nCondition 5\nError term is homoeskedastic (Assumption \\(MLR.5\\))\n\\[\\begin{align}\nVar(u|x_1,\\dots,x_k)=\\sigma^2\n\\end{align}\\]\n\nUnder conditions \\(MLR.1\\) through \\(MLR.5\\), conditional on the sample values of the independent variables,\n\n\n\n\nVariance of \\(\\widehat{\\beta}_{OLS}\\)\n\n\n\\[\\begin{align}\n    Var(\\widehat{\\beta}_j)= \\frac{\\sigma^2}{SST_j(1-R^2_j)},\n\\end{align}\\]\n\n\n\n\nwhere\n\n\\(SST_j= \\sum_{i=1}^n (x_{ji}-\\bar{x_j})^2\\)\n\\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables including an intercept. ( We will revisit this equation)\n\n\n\nJust like uni-variate regression, you need to estimate \\(\\sigma^2\\) if you want to estimate the variance (and standard deviation) of the OLS estimators.\nuni-variate regression\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-2}\n\\end{align}\\]\nmulti-variate regression\nA model with \\(k\\) independent variables with intercept.\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-(k+1)}\n\\end{align}\\]\nYou solved \\(k+1\\) simultaneous equations to get \\(\\widehat{\\beta}_j\\) \\((j=0,\\dots,k)\\). So, once you know the value of \\(n-k-1\\) of the residuals, you know the rest.\n\n\nUsing the estimator of \\(\\sigma^2\\) in place of \\(\\sigma^2\\), we have the  estimator  of the variance of the OLS estimator.\n\n\n\n\nEstimator of the variance of the OLS estimator\n\n\n\\[\\begin{align}\n\\widehat{Var{\\widehat{\\beta}_j}} = \\frac{\\widehat{\\sigma}^2}{SST_j(1-R^2_j)} = \\left(\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-k-1}\\right) \\cdot \\frac{1}{SST_j(1-R^2_j)}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#frischwaughlovell-theorem",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#frischwaughlovell-theorem",
    "title": "02: Multivariate Regression",
    "section": "Frisch–Waugh–Lovell Theorem",
    "text": "Frisch–Waugh–Lovell Theorem\nConsider the following simple model,\n\\[\\begin{align}\n  y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + u_i\n\\end{align}\\]\nSuppose you are interested in estimating only \\(\\beta_1\\).\nLet’s consider the following two methods,\n\nMethod 1: Regular OLS\nRegress \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\) with an intercept to estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) at the same time (just like you normally do)\n\nMethod 2: 3-step\n\nregress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\nregress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\nregress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\) \\((\\widehat{u}_y=\\alpha_1 \\widehat{u}_{x_1}+v_3)\\)\n\nFrisch-Waugh–Lovell theorem\nMethods 1 and 2 produces the same coefficient estimate on \\(x_1\\)\n\\[\\widehat{\\beta}_1 = \\widehat{\\alpha_1}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#partialing-out-interpretation-from-method-2",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#partialing-out-interpretation-from-method-2",
    "title": "02: Multivariate Regression",
    "section": "Partialing out Interpretation from Method 2",
    "text": "Partialing out Interpretation from Method 2\nStep 1\nRegress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\n\n\\(\\widehat{u}_y\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(y\\)\n\nStep 2\nRegress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\n\n\\(\\widehat{u}_{x_1}\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(x_1\\)\n\nStep 3\nRegress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\), which produces an estimte of \\(\\beta_1\\) that is identical to that you can get from regressin \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\)"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#interpretation",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#interpretation",
    "title": "02: Multivariate Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nRegressing \\(y\\) on all explanatory variables \\((x_1\\), \\(x_2\\), and \\(x_3)\\) in a multivariate regression is as if you are looking at the impact of a single explanatory variable with the effects of all the other effects partiled out\nIn other words, including variables beyond your variable of interest lets you  control for (remove the effect of)  other variables, avoiding confusing the impact of the variable of interest with the impact of other variables.\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "href": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "title": "00: Introduction to Econometrics",
    "section": "What econometrics is about",
    "text": "What econometrics is about\n\nWhat?Steps in Econometric Analysis\n\n\nWhat are we doing?\nEstimate quantitative relationships between variables.\n\nExamples\n\nthe impact of fertilizer on crop yield\nthe impact of political campaign expenditure on voting outcomes\nthe impact of education on wage\n\n\n\n\nformulation of the question of interest (what are you trying to find out?)\ndevelop an economic model of the phenomenon you are interested in understanding (identify variables that matter)\nturn the economic model into an econometric model\ncollect data\n estimate the model using econometrics \n test hypotheses"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "href": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "title": "00: Introduction to Econometrics",
    "section": "Go through the steps",
    "text": "Go through the steps\n\nStep 2: Economic ModelStep 3: Econometric modelStep 4: Collect dataSteps 5 and 6\n\n\nExample: Job training and worker productivity\n\\[wage = f(educ,exper,training)\\]\n\n\\(wage\\): hourly wage\n\\(educ\\): years of formal education\n\\(exper\\): years of workforce experience\n\\(training\\): weeks spent in job training\n\nNote\nDepending on questions you would like to answer, the economic model can (and should) be much more involved\n\n\nWe have built a conceptual model:\n\\[wage = f(educ,exper,training)\\]\nNow, the form of the function \\(f(\\cdot)\\) must be specified (almost always) before we can undertake an econometric analysis\n\\[\nwage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\n\\]\n\\(\\beta_0,\\beta_1,\\beta_2,\\beta_3\\)\n\nare the  parameters  of the econometric model.\ndescribe the directions and strengths of the relationship between \\(wage\\) and the factors used to determine \\(wage\\) in the model\n\n\\(u\\)\n\nis called error term\nincludes  ALL  the other factors that can affect wage other than the included variables (like innate ability)\n\n\n\nWe can collect data using various ways. Some of them include survey, websites, experiments. Let’s look at different data types:\n\nCross-sectional DataTime-series DataPanel (Longitudinal) Data\n\n\n\nSample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time\nThe data on all units do not correspond to precisely the same time period\n\nsome families surveyed during different weeks within a year\n\n\nWhat a cross-sectional data looks like on R\n\n\n      wage  educ exper female married\n     &lt;num&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n  1:  3.10    11     2      1       0\n  2:  3.24    12    22      1       1\n  3:  3.00    11     2      0       0\n  4:  6.00     8    44      0       1\n  5:  5.30    12     7      0       1\n ---                                 \n522: 15.00    16    14      1       1\n523:  2.27    10     2      1       0\n524:  4.67    15    13      0       1\n525: 11.56    16     5      0       1\n526:  3.50    14     5      1       0\n\n\n\n\nObservations on a variable or several variables over time + corn price + oil price\n\nNote\n\nThe econometric frameworks necessary to analyze time series data are quite different from those for cross-sectional data\nWe do  NOT  learn time-series econometric methods\n\n\n\nTime series data for each cross-sectional member in the data set ( same  cross-sectional units are tracked over a given period of time)\nExample\n\nwage data for individuals collected every five years over the past 30 years\nyearly GDP data for 60 countries over the past 10 years\n\nWhat a panel data looks like on R\n\n\n     county  year    crmrte   prbarr  prbpris\n      &lt;int&gt; &lt;int&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n  1:      1    81 0.0398849 0.289696 0.472222\n  2:      1    82 0.0383449 0.338111 0.506993\n  3:      1    83 0.0303048 0.330449 0.479705\n  4:      1    84 0.0347259 0.362525 0.520104\n  5:      1    85 0.0365730 0.325395 0.497059\n ---                                         \n626:    197    83 0.0155747 0.226667 0.428571\n627:    197    84 0.0136619 0.204188 0.372727\n628:    197    85 0.0130857 0.180556 0.333333\n629:    197    86 0.0128740 0.112676 0.244444\n630:    197    87 0.0141928 0.207595 0.360825\n\n\n\n\n\n\n\n\nThis is what you learn for the next few months!!\n\nestimate the model using econometrics\ntest hypothesis"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "title": "00: Introduction to Econometrics",
    "section": "Causality and Association",
    "text": "Causality and Association\n\nDistinction between causality and associationGlasses?\n\n\nAssociation\nAn association of two variables arise because  either of or both  variables affect the other variable\n\\[\\begin{align}\n  A \\longleftrightarrow B \\\\\n  A \\longrightarrow B \\\\\n  A \\longleftarrow B\n\\end{align}\\]\nAssociation does  NOT  concern which affects which. Under all the three cases above, A and B are  associated. Or, we say there is an association between A and B. This is what  correlation coefficient  measures.\n\nCausality\nWhen A has a causal impact on B,\n\\[\nA \\longrightarrow B\n\\]\nHere, changes in \\(A\\) cause changes in \\(B\\), not the other way around\n\n\n\nVideoClaimsBut,\n\n\nLet’s watch this interesting CM.\n\n\nPeople who wear glasses are\n\nmuch smarter than those who don’t\nmore likely to pursue higher education\n200% more likely to graduate college\n\nFor you to be convinced to buy glasses, these claims needs to be causal, not association:\n\nDoes wearing glasses make you much smarter?\nDoes wearing glasses make it more likely for you to pursue higher education?\nDoes wearing glasses make it 200% more likely for you to graduate college?\n\n\n\nHowever, this seems to be a more likely explanation of the association:\n\nOne spends more time studying academic subjects\n\nsmarter (or knowledgeable) \\(\\Rightarrow\\) pursue higher education and graduate college\nworsened eyesight \\(\\Rightarrow\\) wear glasses\n\n\n\n\n\nImportant\n\n\n\nWe care about isolating causal effects, but not association\nIdentifying association is super easy\nIdentifying causal effects is extremely hard (this is what we tackle)"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "title": "00: Introduction to Econometrics",
    "section": "Endogeneity: Your Nemesis",
    "text": "Endogeneity: Your Nemesis\n\nEndogeneityExampleWhat happened?EndogeneityAnother example\n\n\nIt is super easy to find an association of multiple variables, but it is incredibly hard to find a causal effect (at least in Economics)!!\nThat is due to the problem called  endogeneity , which is going to be defined formally later.\n\n\nYou are interested in the causal impact of fire fighters on the number of death tolls in fire events\n\n\nfire eventdeath toll# of firefighters deployed11020203351043555050\n\n\nQuestions\n\nHow are they  associated ?\nCan you say anything about the causal effect of fire fighters deployment on the number of death tolls?\n\n\n\nYou ignored an important variable!!\n\n\nfire eventdeath toll# of firefighters deployedscale of fire110202020353510204351055050100\n\n\n\n\n\n\n\n\nDefinition\n\n\nVariables of interest are correlated with some  unobservables  (variables that cannot be observed or are missing) that have non-zero impacts on the variable that you want to explain\n\n\n\n\n\nThe unobserved variables are also called  confounder/confounding factor .\n\nThe example\nIn the the firefighter example,\n\n variable of interest : the number of firefighters\n unobservables/confounder : the scale of fire events (and other factors)\n variable to explain : death toll\n\nThe model\n\\[\\begin{align}\n  \\mbox{death toll} & = \\alpha + \\beta\\; \\mbox{# of fire fighters} + \\mu\\\\\n  ,\\mbox{where } \\mu & = (\\gamma\\; \\mbox{scale} + v) \\mbox{ is the error term (collection of unobservables)}\n\\end{align}\\]\nEndogeneity Problem\n# of fire fighters is correlated with scale, which we ignored\n\n\n\n\n\\[wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\\]\nWhat are unobservables in \\(u\\) that are likely to be correlated with \\(educ\\)?\nAn important unobservable\n\ninnate ability \\(\\Rightarrow\\) wage\ninnate ability \\(\\Rightarrow\\) education"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "href": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "title": "00: Introduction to Econometrics",
    "section": "How to deal with endogeneity",
    "text": "How to deal with endogeneity\n\nThe questionHow to deal with endogeneity?\n\n\nProblem\nMost of the time, you will be faced with endogeneity problems caused by at least one of the followings,\n\nomitted variables (the scale of fire events, innate ability)\nself-selection\nsimultaneity\nmeasurement error\n\nCentral Question\nHow can we avoid or solve endogeneity problems?\n\n\n\nYou have two opportunities to deal with endogeneity problems\n\nat the design (design to collect data) stage\nat the regression stage (what you will learn in this course)\n\nEconometrics has evolved mostly to address endogeneity problems at the  regression stage  because randomized experiments are infeasible most of the time\nHow about econometrics and other fields of statistics: Statistics, Psychometrics, and Biometrics?\n\n\n\nFieldDesignEstimation MethodEconometricsnot feasible (often)intricateMany other fieldsfeasiblerelatively simple"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "href": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "title": "00: Introduction to Econometrics",
    "section": "Randomized-experiments",
    "text": "Randomized-experiments\nIn randomized experiments,\n\nyou have a liberty to determine the level of the variable of interest\nby randomizing the value of the variable of interest, you can effectively break the link (association) with whatever is included in the error term\n\n\nExample (Non-Randomized)RandomizedRandomized Experiments on Education?\n\n\n\nDataFarmer’s decisionBias\n\n\nYield and nitrogen rate data obtained from a field that is managed by a farmer\n\n\n\n\n\n\n\n\n\n\n\nFarmer\n\ndecide nitrogen rate based on soil/field characteristics (some of them we researchers do not get to observe)\n\nResearcher\n\nsoil characteristics is not observable, so it is in the error term\n\n\\[yield = \\beta_0 + \\beta_1 N + (\\gamma SC + \\mu)\\]\n\nN (nitrogen rate) and SC (soil characteristics) are correlated\n\n\n\nSuppose the farmer applied more nitrogen to the area where its soil characteristics lead to higher corn yield\nQuestion If the researcher estimate the model (which ignores soil characteristics), do you over- or under-estimate the impact of nitrogen rate on corn yield?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSoil quality (in error term) is no longer correlated with N!!\n\n\n\n\n\nRandomized Experiment?\nResearchers determine randomly how much education subjects (people) can get?\nEndogeneity Problem in Economics\n\nEconomics is about understanding human behavior\nAlmost always, you need to deal with endogeneity problem because people are smart: we make decisions based on available information (not just randomly) so that our decisions lead to good outcomes (whether our decisions turn out to be good or not is irrelevant)\n\nhow much education one get is determined based on their judgment of their own ability (not by rolling a dice)\nhow many fire fighters to be deployed was determined based on the scale of fire (not by rolling a dice)\nhow much nitrogen to apply based on soil characteristics (not by rolling a dice)\n\nIf people are not smart and just roll a dice for their decision making, we would have much easier time identifying causal effects"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Introduction",
    "text": "Monte Carlo Simulation: Introduction\n\nWhat is ti?QuestionKey part of MC simulations\n\n\nIt is a way to test econometric theories via simulation.\n\nHow is it used in econometrics?\n\nconfirm ecoometric theory numerically\n\nOLS estimators are unbiased if \\(E[u|x]=0\\) along with other conditions (theory)\nI know the above theory is right, but let’s check if it is true numerically\n\nYou kind of sense that something in your data may cause problems, but there is no proven econometric theory about what’s gonna happen (I used MC simulation for this purpose a lot)\nassist students in understanding econometric theories by providing actual numbers instead of a series of Greek letters\n\n\n\nSuppose you are interested in checking what happens to OLS estimators if \\(E[u|x]=0\\) (the error term and \\(x\\) are not correlated) is violated.\n\n\n\n\nQuestion\n\n\nCan you use the real data to do this?\n\n\n\n\n\n\n\n\nAnswer\n\nNo because you will never observe either error term or true value of \\(\\beta\\)s.\n\n\n\n You  generate data (you have control over how data are generated)\n\nYou know the true parameter unlike the real data generating process\nYou can change only the part that you want to change about data generating process and econometric methods with everything else fixed"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "title": "03: Monte Carlo Simulation",
    "section": "Generating data",
    "text": "Generating data\n\nRNGPseudo?Normal Distribution\n\n\nPseudo random number generators (Pseudo RNG)\nAlgorithms for generating a sequence of numbers whose properties  approximate  the properties of sequences of random numbers\n\nExamples\nDraw from a uniform distribution:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nNumbers drawn using pseudo random number generators are not truly random\n\nWhat numbers you will get are pre-determined\nWhat numbers you will get can be determined by setting a  seed \n\nDemonstration\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuestion\nWhat benefits does setting a seed have?\n\n\n\n\n\\(x \\sim N(0, 1)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\\(x \\sim N(2, 2)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions-for-often-used-distributions",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions-for-often-used-distributions",
    "title": "03: Monte Carlo Simulation",
    "section": "R functions for often-used distributions",
    "text": "R functions for often-used distributions\n\nDistribution typesdnorm()pnorm()qnorm()\n\n\n\n\n\nNormal\nUniform\nBeta\nChi-square\nF\nLogistic\nLog-normal\nmany others\n\n\nFor each distribution, you have four different kinds of functions:\n\n dnorm: density function\n pnorm: distribution function\n qnorm: quantile function\n rnorm: random draw\n\n\n\n\n\n\ndnorm(x) gives you the height of the density function at \\(x\\).\ndnorm(-1) and dnorm(2)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\npnorm(x) gives you the probability that a single random draw is  less  than \\(x\\).\n\npnorm(-1)pnorm(2)Exercise\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the probability that a single random draw from a Normal distribution with mean = 1 and sd = 2 is less than 1?\n\nWork here\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAnswer\n\npnorm(1, mean = 1, sd = 2) \n\n\n\n\n\n\n\n\nWhat is it?qnorm(0.95)Exercise\n\n\nqnorm(x), where \\(0 &lt; x &lt; 1\\), gives you a number \\(\\pi\\), where the probability of observing a number from a single random draw is less than \\(\\pi\\) with probability of \\(x\\).\nWe call the output of qnorm(x), \\(x%\\) quantile of the standard Normal distribution (because the default is mean = 0 and sd = 1 for rnorm()).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the 88% quantile of Normal distribution with mean = 0 and sd = 9?\n\nWork hereAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nCode\nqnorm(0.88, mean = 0, sd = 9)"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Steps",
    "text": "Monte Carlo Simulation: Steps\n\nspecify the data generating process\ngenerate data based on the data generating process\nget an estimate based on the generated data (e.g. OLS, mean)\nrepeat the above steps many many times\ncompare your estimates with the true parameter\n\nQuestion\nWhy do the steps \\(1-3\\) many many times?"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 1",
    "text": "Monte Carlo Simulation: Example 1\n\nProblemSteps 1-3Sample Mean: Step 4Loop: for loopStep 4Step 5\n\n\n\n\n\n\nQuestion\n\n\nIs sample mean really an unbiased estimator of the expected value?\n\n\n\n\n\nThat is, is \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nrepeat the above steps many times\nWe use a  loop  to do the same (similar) thing over and over again\n\n\n\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVerbally\nFor each of \\(i\\) in \\(1:B\\) \\((1, 2, \\dots, 1000)\\), do print(i).\n\ni takes the value of 1, and then print(1)\ni takes the value of 2, and then print(2)\n…\ni takes the value of 999, and then print(999)\ni takes the value of 1000, and then print(1000)\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nCompare your estimates with the true parameter\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 2",
    "text": "Monte Carlo Simulation: Example 2\n\nQuestionR codeCompare\n\n\n\n\n\n\nQuestion\n\n\nWhat happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\)?\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 3 (optional)",
    "text": "Monte Carlo Simulation: Example 3 (optional)\n\nQuestionR codeCheckDistribution\n\n\nModel\n\\[\\begin{aligned}\n    y = \\beta_0 + \\beta_1 x + u \\\\\n\\end{aligned}\\]\n\n\\(x\\sim N(0,1)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u|x]=0\\)\n\n\nVariance of the OLS estimator\nTrue Variance of \\(\\hat{\\beta_1}\\): \\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} = \\frac{\\sigma^2}{SST_X}\\)\nIts estimator: \\(\\widehat{V(\\hat{\\beta_1})} =\\frac{\\hat{\\sigma}^2}{SST_X} = \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{n-2} \\times \\frac{1}{SST_X}\\)\n\nQuestion\nDoes the estimator really work? (Is it unbiased?)\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nTrue Variance\n\n\\(SST_X = 112.07\\)\n\\(\\sigma^2 = 4\\)\n\n\\[V(\\hat{\\beta}) = 4/112.07 = 0.0357\\]\n\nCheck\nYour Estimates of Variance of \\(\\hat{\\beta_1}\\)?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "title": "03: Monte Carlo Simulation",
    "section": "Exercise (optional)",
    "text": "Exercise (optional)\n\nProblemSolutionResults visualization\n\n\nUsing MC simulations, find out how the variation in \\(x\\) affects the OLS estimators\n\nModel setup\n\\[\\begin{align}\n  y = \\beta_0 + \\beta_1 x_1 + u \\\\\n  y = \\beta_0 + \\beta_1 x_2 + u\n\\end{align}\\]\n\n\\(x_1\\sim N(0,1)\\) and \\(x_2\\sim N(0,9)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u_1|x]=0\\) and \\(E[u_2|x]=0\\)\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Small sample property of OLS estimators",
    "text": "Small sample property of OLS estimators\n\nSmall sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\widehat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS estimator",
    "text": "Unbiasedness of OLS estimator\n\nUnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nDirection\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\widehat{\\beta}_1]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\widehat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\nLinear in Parameters\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\nRandom sampling\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\nVariation in covariates\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\nRoughly speaking\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\widehat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\widehat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\widehat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\widehat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\widehat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\widehat{\\beta}_1|x] = & E[\\widehat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS in practice",
    "text": "Unbiasedness of OLS in practice\n\nGood empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Variance of OLS estimator",
    "text": "Variance of OLS estimator\n\nIntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\widehat{\\beta}_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\widehat{\\beta}_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\widehat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\widehat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\widehat{\\beta}_1) = \\sqrt{Var(\\widehat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Efficiency of OLS Estimators",
    "text": "Efficiency of OLS Estimators\n\nNature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the variance of error",
    "text": "Estimating the variance of error\n\nWhy?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\widehat{u}_i^2\\)\n\n\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\nNote\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\ndates_data &lt;- \n  data.table(\n    date = seq( as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by=\"+1 day\")\n  ) %&gt;%\n  .[, day := weekdays(date)]\n\nw_month_data &lt;- dates_data[month(date) == 8,]\n\nimp_dates &lt;- \n  rep(NA, nrow(w_month_data))\n\nlecture_ind &lt;- \n  w_month_data[, day %in% c(\"Monday\", \"Wednesday\") & day(date) &gt;= 19]\n\nlab_ind &lt;- \n  w_month_data[, day %in% c(\"Friday\") & day(date) &gt;= 19 ]\n\n# Add the events to the desired days\nimp_dates[lecture_ind] &lt;- \"Lecture\"\nimp_dates[lab_ind] &lt;- \"Lab\"\n\n# Create a calendar with a legend\n\ntemp &lt;- \n  calendR::calendR(\n    year = 2024, \n    month = 8, \n    special.days = imp_dates,\n    special.col = c(\n      \"lightcyan2\", \"tan\"),\n    weeknames = c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ),\n    mbg.col = \"15\",\n    months.col = \"blue\",\n    legend.pos = \"bottom\"\n  )"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "href": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "title": "00-0: Logistics",
    "section": "Instructors",
    "text": "Instructors\n\n Instructor : Taro Mieno (Office: 209, E-mail: tmieno2@unl.edu)\n Teaching Assistant :\n\nTBD"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "href": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "title": "00-0: Logistics",
    "section": "Goals of the course",
    "text": "Goals of the course\n\nLearn modern introductory econometric theory\nApply econometric theories to real economic problems\nLearn how to use statistical software (R) so you can conduct research independently (without technical help from your advisor)\n\nmanage data\nvisualize data\nrun regressions\ninterpret results"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "href": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "title": "00-0: Logistics",
    "section": "Text Books",
    "text": "Text Books\nRecommended: Wooldridge, Jeffrey M. 2006. “Introductory Econometrics: A Modern Approach (5th edition).” Mason, OH: Thomson/South-Western."
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "href": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "title": "00-0: Logistics",
    "section": "Course Schedule",
    "text": "Course Schedule\n\nLectures (MW): 3:00-4:30pm\nLab sessions (F): 1:00-2:30pm"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "href": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "title": "00-0: Logistics",
    "section": "Grading",
    "text": "Grading\n\nProblem sets (3 assignments): 30%\nSmall-size midterms (2): 40%\nPaper: 40%"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "href": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "title": "00-0: Logistics",
    "section": "Assignments",
    "text": "Assignments\nProblem sets + Most questions are from the required text book + Some questions come from what we cover in lab sessions\n** Quarto to do and submit your problem sets** + You are required to present your R codes + You learn how to compile your assignment with your R code written in a document using  Quarto , which will be covered in the second lab session\nCaution + 2nd year students have answers to all the questions I will assign (I will use exactly the same problems because they are really good to learn econometrics) + You are free to copy and paste (or rephrase) the answers for your assignment. I won’t bother to try to tell if you have copied and pasted answers. + However, you are simply doing dis-service to yourself by depriving yourself of learning opportunities + Moreover, your lack of understanding of the material will be clearly manifested on your performance at midterms and final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "href": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "title": "00-0: Logistics",
    "section": "Midterms",
    "text": "Midterms\nIn-class open-book midterms\n\nMidterm 1: Oct, 9 (M)\nMidterm 2: Nov, 20 (M)"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIn this assignment, + you write a paper with a particular emphasis on econometric analysis using a real world data set + you are encouraged to use the data set you are using for your masters thesis (talk with your advisor) + you need to ensure that you use a  panel  dataset + No presentation of your final paper\nTime line\n\n Oct, 16 : identify a research topic and the data set you will be using, and get an approval from the instructor\n Oct, 23 : paper proposal\n Dec, 15 : final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "href": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "title": "00-0: Logistics",
    "section": "Paper Proposal",
    "text": "Paper Proposal\nIntroduction + clear identification of what you are trying to find out (research question) + why the research question is worthwhile answering\nSimple Model + dependent variable (the variable to be explained) + explanatory variable (variables to be explain)\nData Source + where you get data"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIntroduction\n\nclear identification of what you are trying to find out (research question) [1 point]\nwhy the research question is worthwhile answering [1 point]\n\nData description\n\nthe nature of the data with summary statistics table [1 point]\nvisualize a few key variables in a meaningful way [3 points]\n\nEconometric Methods\nThe  process  of how you end up with the final econometric models and methods. [40 points ( or more )]\n\njustification of your choice of independent variables\npotential endogeneity problems\nwhat did you do to address the endogeneity problems?\njustification of econometric model(s) and method(s)\nidentify appropriate standard error estimation methods\n\nResults, Discussions, and Conclusions + interpret and describe the results [2 points] + implications of the results [1 point] + conclusions [1 point]"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Assignment 1\n\nGo here and download all the files in the folder\nWork on assignment-1-student.qmd\nSubmit both the qmd and resulting html files here"
  },
  {
    "objectID": "LabLectures.html",
    "href": "LabLectures.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "Visit here for R lab lecture notes. Chapter 1 through Chapter 4 will be covered in this course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (MS)",
    "section": "",
    "text": "This website hosts course materials for Applied Econometrics (AECN 896-04) at UNL."
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n00-0: Logistics\n\n\n\n\n00: Introduction to Econometrics\n\n\n\n\n01-1: Univariate Regression: Introduction\n\n\n\n\n01-2: Univariate Regression: OLS Mechanics and Implementation\n\n\n\n\n01-3: Univariate Regression: OLS Small Sample Property\n\n\n\n\n02-1: Multivariate Regression\n\n\n\n\n03-1: Monte Carlo Simulation\n\n\n\n\n04-1: Omitted Variable Bias and Multicollinearity\n\n\n\n\n05-Hypothesis Testing\n\n\n\n\n06: Standard Error Estimation\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall\n\n\n\nMona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall"
  },
  {
    "objectID": "syllabus.html#ta",
    "href": "syllabus.html#ta",
    "title": "Personnel",
    "section": "",
    "text": "Mona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Personnel",
    "section": "Lectures and Labs:",
    "text": "Lectures and Labs:\n\nLectures: MW 3:00 - 4:30 PM\nLabs: F 1:00 - 2:30 PM"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Personnel",
    "section": "Office Hours:",
    "text": "Office Hours:\nWednesday, 10:00 to 11:30 pm or by appointment"
  },
  {
    "objectID": "syllabus.html#lecture",
    "href": "syllabus.html#lecture",
    "title": "Personnel",
    "section": "Lecture",
    "text": "Lecture\n\nIntroduction to econometrics\nSimple univariate regression\nMonte Carlo Simulation\nMultivariate regression\nMulti-collinearity and omitted variable\nHypothesis Testing\nHetereoskedasticity and robust standard error estimation\nClustered error and bootstrap\nFunctional form and scaling\nDummy variables\nPanel data methods\nCausal Inference\nCausal Inference\nLimited dependent variable"
  },
  {
    "objectID": "syllabus.html#computer-lab-r",
    "href": "syllabus.html#computer-lab-r",
    "title": "Personnel",
    "section": "Computer Lab (R)",
    "text": "Computer Lab (R)\n\nIntroduction to R\nRmarkdown\nData wrangling\nData visualization\nResearch Flow and R"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "The data set and model",
    "text": "The data set and model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSet upR code to get dataData visualization\n\n\nData\nObservations of house price and lot size for 546 houses.\n\nModel\n\\[price_i = \\beta_0 + \\beta_1 lotsize_i+u_i\\]\n\n\\(price_i\\): house price ($) of house \\(i\\)\n\\(lotsize_i\\): lot size of house \\(i\\)\n\\(u_i\\): error term (everything else) of house \\(i\\)\n\n\nObjective\nEstimate the impact of lot size on house price\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Estimation with OLS",
    "text": "Estimation with OLS\n\nRough ideaExamplesResidualsOLSVisualizationDerivationEstimators vs Estimates\n\n\n\nWe want to draw a line like this, the slope of which is an estimate of \\(\\beta_1\\)\nA way: Ordinary Least Squares (OLS)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEx. 1: \\(\\widehat{\\beta}_0=20000\\), \\(\\widehat{\\beta}_1=7\\)Ex. 2: \\(\\widehat{\\beta}_0=70000\\), \\(\\widehat{\\beta}_1=3.8\\)So,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion\n\n\n\nAmong all the possible values of \\(\\beta_0\\) and \\(\\beta_1\\), which one is the best?\nWhat criteria do we use (what does the best even mean?)\n\n\n\n\n\n\n\n\n\n\n\nFor particular values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) you pick, the modeled value of \\(y\\) for individual \\(i\\) is \\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\).\nThen, the residual for individual \\(i\\) is:\n\\[\n\\widehat{u}_i =  y_i - (\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i)\n\\]\nThat is, residual is the observed value of the dependent variable less the value of modeled value. For different values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\), you have a different value of residual.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIdea of OLS (Ordinary Least Squares)\nLet’s find the value of \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the sum of the squared residuals!\n\nMathematically\nSolve the following minimization problem:\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n \\widehat{u}_i^2, \\mbox{where} \\;\\; \\widehat{u}_i=y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestions\n\nWhy do we square the residuals, and then sum them up together? What’s gonna happen if you just sum up residuals?\nHow about taking the absolute value of residuals, and then sum them up?\n\n\n\nMinimization problem to solve\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nSteps\n\npartial differentiation of the objective function with respect to \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\nsolve for \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\n\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nFOC\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{align}\n\\frac{\\partial }{\\partial \\widehat{\\beta}_0}=& 2 \\sumn [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]=0 \\\\\\\\\n\\frac{\\partial }{\\partial \\widehat{\\beta}_1}=& 2 \\sumn x_i\\cdot [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]= \\sumn x_i\\cdot \\widehat{u}_i = 0\n\\end{align}\n\\]\nOLS estimators: analytical formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2},\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}, \\\\\\\\\n  \\mbox{where} & \\;\\; \\bar{y} = \\sumn y_i/n \\;\\; \\mbox{and} \\;\\;\\bar{x} = \\sumn x_i/n\n\\end{aligned}\n\\]\n\n\nEstimators\nSpecific  rules (formula)  to use once you get the data\n\nEstimates\nNumbers you get once you plug values (your data) into the formula"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "OLS demonstration in R",
    "text": "OLS demonstration in R\n\nR code: hard wayR code: a better waypost-estimation\n\n\nOLS Estimator Formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\]\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe can use the feols() function from the fixest package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLots of information is stored in the regression results (here, uni_reg), which is of class list.\nApply ls() to see its elements:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimated coefficients:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPredicted values at the observation points:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nResiduals:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can have a nice quick summary of the regression results with summary() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Once the model is estimated",
    "text": "Once the model is estimated\n\nEstimated modelPredicted values (R)New predictions (R)Exercise: The impact of lotsize\n\n\nModel to be estimated\n\\[\nprice = \\beta_0 + \\beta_1 lotsize + u\n\\]\n\nEstimated Model\nThis is the estimated version of the expected value of \\(y\\) conditional on \\(x\\).\n\\[\nprice =  3.4136\\times 10^{4} + 6.599 \\times lotsize\n\\]\nThis is called  sample regression function (SRF) , and it is an estimation of \\(E[price|lotsize]\\), the  population regression function (PRF).\n\n\n\nImportant\n\n\n\nOLS regression predicts the  expected  value of the dependent variable  conditional on the explanatory variables.\n\\(\\widehat{\\beta}_1\\) is an estimate of how a change in \\(x\\) affects the  expected  value of \\(y\\).\n\n\n\n\n\n\nYou can access the predicted values at the observed points by looking at the fitted.value element of the regression results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTo calculate the predicted value at arbitrary values of \\(x\\),\n\ncreate a new data.frame with values of \\(x\\) of your choice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\napply predict() to the data.frame using the regression results.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nProblemSolution\n\n\nYour current lot size is 3000. You are thinking of expanding your lot by 1000 (with everything else fixed), which would cost you 5,000 USD. Should you do it? Use R to figure it out.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "\\(R^2\\): Goodness of fit",
    "text": "\\(R^2\\): Goodness of fit\n\nWhat is it?Decompose \\(y\\)Visualization\\(R^2\\) componentsDefinition of \\(R^2\\)Caveat\n\n\n\\(R^2\\) is a measure of how good your model is in predicting the dependent variable (explaining variations in the dependent variable)  compared  to just using the average of the dependent variable as the predictor.\n\n\nYou can decompose observed value of \\(y\\) into two parts: fitted value and residual\n\\[\ny_i=\\widehat{y}_i +\\widehat{u}_i, \\;\\;\\mbox{where}\\;\\; \\widehat{y}_i = \\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i\n\\]\nnow, subtracting \\(\\bar{y}\\) (sample average of \\(y\\)),\n\\[\ny_i-\\bar{y}=\\widehat{y}_i-\\bar{y}+\\widehat{u}_i\n\\]\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\n\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ntotal sum of squares (SST)\n\\[\nSST\\equiv \\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\]\nexplained sum of squares (SSE) \\[\nSSE\\equiv \\sum_{i=1}^{n}(\\widehat{y}_i-\\bar{y})^2\n\\]\nresidual sum of squares (SSR) \\[\nSSR\\equiv \\sum_{i=1}^{n}\\widehat{u}_i^2\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\\(R^2 = 1 - SSR/SST\\)\n\n\n\n\n\nWhere did it come from?\n\\[\\begin{align}\n& SST = SSE + SSR  \\\\\n\\Rightarrow & SSE = SST - SSR \\\\\n\\Rightarrow & SSE/SST = 1 - SSR/SST = R^2\\\\\n\\end{align}\\]\nThe value of \\(R^2\\) always lies between \\(0\\) and \\(1\\) as long as an intercept is included in the econometric model.\n\nWhat does it measure?\n\\(R^2\\) is a measure of how much improvement  in predictin the depdent variable  you’ve made by including independent variable(s) \\((y=\\beta_0+\\beta_1 x+u)\\) compared to when simply using the mean of dependent variable as the predictor \\((y=\\beta_0+u)\\).\n\n\nImportant\n\n\\(R^2\\) tells  nothing  about how well you have estimated the causal ceteris paribus impact of \\(x\\) on \\(y\\) \\((\\beta_1)\\).\nAs an economist, we typically do not care about how well we can prefict yield, rather we care about how well we have predicted \\(\\beta\\).\n\nProblem\n\nWhile we observe the dependent variable (otherwise you cannot run regression), we cannot observe \\(\\beta_1\\).\nSo, we get to check how good estimated models are in predicting the dependent variable (which we do not care), but we can  never  test whether they have estimated \\(\\beta_1\\) well.\nThis means that we need to carefully examines whether the  assumptions  necessary for good estimation of \\(\\beta_1\\) is satisfied (next topic)."
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "What variables to include or not",
    "text": "What variables to include or not\nYou often\n\nface the decision of whether you should be including a particular variable or not:  how do you make a right decision? \nmiss a variable that you know is important because it is not simply available:  what are the consequences? \n\nTwo important concepts you need to be aware of:\n\nMulticollinearity\nOmitted Variable Bias"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Multicollinearity and Omitted Variable Bias",
    "text": "Multicollinearity and Omitted Variable Bias\n\nDefinitionObjectiveCases we look atKey consequences of interest\n\n\n\n\n\n\nDefinition: Multicollinearity\n\n\nA phenomenon where two or more variables are highly correlated (negatively or positively) with each other ( consequences? )\n\n\n\n\n\n\n\n\n\nDefinition: Omitted Variable Bias\n\n\nBias caused by not including (omitting)  important  variables in the model\n\n\n\n\n\n\nConsider the following model,\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\n\\]\nYour interest is in estimating the impact of \\(x_1\\) on \\(y\\).\n\n\n\n\nObjective\n\n\nUsing this simple model, we investigate what happens to the coefficient estimate on \\(x_1\\) if you include/omit \\(x_2\\).\n\n\n\n\n\n\nThe model: \\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\]\nCase 1:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 2:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is highly correlated with \\(x_1\\)?\nCase 3:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 4:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is highly correlated with \\(x_1\\)?\n\n\n\nIs \\(\\widehat{\\beta}_1\\) unbiased, that is \\(E[\\widehat{\\beta}_1]=\\beta_1\\)?\n\\(Var(\\widehat{\\beta}_1)\\)? (how accurate the estimation of \\(\\widehat{\\beta}_1\\) is)"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 1",
    "text": "Case 1\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\).)\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either of \\(x_2\\) and \\(u\\). So, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 on average because \\(cor(x_1, x_2)=0\\)\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nThey are the same because all the components are the same.\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\) and is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) will be the same as when you do not include \\(x_2\\) as a covariate (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)) and is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 2",
    "text": "Case 2\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because\n\n\\(x_1\\) is correlated with \\(x_2\\), but \\(\\beta_2 = 0\\).\n\\(x_1\\) is not correlated with \\(u\\)\n\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 &gt; 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\), but is highly correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is larger compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is highly correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 3",
    "text": "Case 3\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either \\(x_2\\) or \\(u\\).\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is (on average) zero because \\(x_1\\) and \\(x_2\\) are not correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is zero (on average).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include a variable that has some explanatory power beyond \\(x_1\\), but is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is smaller compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an variable that has some explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 4",
    "text": "Case 4\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nNo, because \\(x_1\\) is correlated with \\(x_2\\) and \\(\\beta_2 \\ne 0\\).\nSo, there will be bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 \\ne 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j) = \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nIt depends.\n\n\n\nIn the MC simulations we saw,\n\n\\(x_1\\) and \\(x_2\\) are highly correlated, so \\(R_j^2\\) is very high for \\(EE_2\\)\n\n\nx1 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\nx2 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\n\n\n\nThe impact of \\(x_2\\) (\\(\\beta_2 = 1\\)) and the variance of \\(x_2\\) is small (approximately 1).\n\n\ny &lt;- 1 + x1 + 1 * x2 + u\n\n\nThese conditions led to lower \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) compared to \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\).\n\n\nNow, let’s reverse the current conditions. We now have:\n\n\\(x_1\\) and \\(x_2\\) are NOT highly correlated, so \\(R_j^2\\) is small for \\(EE_2\\)\nThe impact of \\(x_2\\) (\\(\\beta_2 = 5\\)) and the variance of \\(x_2\\) is large (approximately 5).\n\n\nx1 &lt;- 0.9 * rnorm(N) + 0.1 * mu # independent variable\nx2 &lt;- 2.23 * rnorm(N) + 0.1 * mu # independent variable\ncor(x1, x2)\n\n\n\ny &lt;- 1 + x1 + 5 * x2 + u\n\n\nLet’s rerun MC simulations with this updated data generating process.\n\n\n\n\n\n\n\nThere exists bias-variance trade-off when independent variables are both important (their coefficients are non-zero) and they are correlated\nEconomists tend to opt for unbiasedness"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Omitted Variable Bias (Theory)",
    "text": "Omitted Variable Bias (Theory)\n\nSetupMagnitude and direction of biasExamplesHow does this help?\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(EE_1\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\nLet \\(\\tilde{\\beta_1}\\) denote the estimator of \\(\\beta_1\\) from this model\n\n\\(EE_2\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\nLet \\(\\widehat{\\beta}_1\\) and \\(\\widehat{\\beta}_2\\) denote the estimator of \\(\\beta_1\\) and \\(\\beta_2\\)\n\nRelationship between \\(x_1\\) and \\(x_2\\)\n\\(x_{1,i} = \\sigma_0 + \\sigma_1 x_{2,i} + \\mu_{i}\\)\n\n\n\n\nImportant\n\n\nThen, \\(E[\\tilde{\\beta_1}] = \\beta_1 + \\beta_2 \\cdot \\sigma_1\\), where \\(\\beta_2 \\cdot \\sigma_1\\) is the bias.\n\n\n\nThat is, if you omit \\(x_2\\) and regress \\(y\\) only on \\(x_1\\), then the bias is going to be the multiple of the impact of \\(x_2\\) on \\(y\\) (\\(\\beta_2\\)) and the impact of \\(x_2\\) on \\(x_1\\) (\\(\\sigma_1\\)).\n\n\nDirection of bias\n\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &gt; 0\\)\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &gt; 0\\)\n\n\nMagnitude of bias\n\nThe greater the correlation between \\(x_1\\) and \\(x_2\\), the greater the bias\nThe greater \\(\\beta_1\\) is, the greater the bias\n\n\n\n\nExample 1Example 2Example 3\n\n\n\\[\n\\begin{aligned}\n\\mbox{corn yield} = \\alpha + \\beta \\cdot N + (\\gamma \\cdot \\mbox{soil erodability}  + \\mu)\n\\end{aligned}\n\\]\n\nFamers tend to apply more nitrogen to the field that is more erodible to compensate for loss of nutrient due to erosion\nSoil erodability affects corn yield negatively \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe city planner placed incinerators in the outskirt of a city to avoid their potentially negative health effects\nDistance to city center has a negative impact on house price \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\\(\\mbox{groundwater use}\\): groundwater use by a farmer for irrigated production\n\\(\\mbox{center pivot}\\): 1 if center pivot is used, 0 if flood irrigation (less effective) is used\n\nFarmers who have relatively low precipitation during the growing season tend to adopt center pivot more\ncenter pivot applied water more efficiently than flood irrigation \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\n\n\n\nWhen the direction of the bias is the  opposite  of the expected coefficient on the variable of interest, you can claim that  even after  suffering from the bias, you are still seeing the impact of the variable interest. So, it is a strong evidence that you would have had an even stronger estimated impact.\n\nExample 1Example 2\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-5\\) ( you only observe this )\n\nYou believe the direction of bias is positive (you need provide reasoning behind your belief), and yet, the estimated coefficient is still negative. So, you can be quite confident that the sign of the impact of precipitation is negative. You can say your estimate is a conservative estimate of the impact of precipitation on groundwater use.\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(-5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-15\\) ( you only observe this )\n\nYou believe the direction of bias is negative, and the estimated coefficient is negative. So, unlike the case above, you cannot be confident that \\(\\widehat{\\beta}\\) would have been negative if it were not for the bias (by observing dist to city center and include it as a covariate). It is very much possible that the degree of bias is so large that the estimated coefficient turns negative even though the true sign of \\(\\beta\\) is positive. In this case, there is nothing you can do.\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#homoskedasticity-and-heteroskedasticity",
    "href": "lectures/06-se-estiation/06-se-estimation.html#homoskedasticity-and-heteroskedasticity",
    "title": "06: Standard Error Estimation",
    "section": "Homoskedasticity and Heteroskedasticity",
    "text": "Homoskedasticity and Heteroskedasticity\n\nReviewVisualizationCentral QuestionsCoefficient estimatorsVariance of the coefficient estimators\n\n\n\n\n\n\nHomoskedasticity\n\n\n\\(Var(u|x) = \\sigma^2\\)\n\n\n\n\n\n\n\n\n\nHeteroskedasticity\n\n\n\\(Var(u|x) = f(x)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the consequences of assuming the error is homoskedastic when it is heteroskedastic in reality?\n\nEstimation of coefficients \\((\\widehat{\\beta}_j)\\)?\nEstimation of the variance of \\(\\widehat{\\beta}_j\\)?\n\n\n\n\nQuestionAnswer\n\n\nAre OLS estimators unbiased when error is heteroskedastic?\n\n\nYes. We do not need to use the homoskedasticity assumption to prove that the OLS estimator is unbiased.\n\n\n\n\n\n\n\nhomeskedastic errorheteroskedastic errorConsequences\n\n\nWe learned that when the homoskedasticity assumption holds, then,\n\\(Var(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{SST_x(1-R^2_j)}\\)\nWe used the following as the estimator of \\(Var(\\widehat{\\beta}_j)\\)\n\\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) where \\(\\widehat{\\sigma}^2 = \\frac{\\sum_{i=1}^{N} \\widehat{u}_i^2}{N-k-1}\\)\n\n\n\n\nImportant\n\n\n By default , R and other statistical software uses this formula to get estimates of the variance of \\(\\widehat{\\beta}_j\\).\n\n\n\n\n\nBut, under heteroskedasticity,\n\\(Var(\\widehat{\\beta}_j) \\ne \\frac{\\sigma^2}{SST_x(1-R^2_j)}\\)\n\n\nQuestionAnswer\n\n\nIs \\(E[\\widehat{Var(\\widehat{\\beta}_j)}_{default}] \\equiv E\\Big[\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\Big]=Var(\\widehat{\\beta}_j)\\) under heteroskedasticity?\n\n\nNo.\n\n\n\n\n\n\nSo, what are the consequences of using \\(\\widehat{Var(\\widehat{\\beta}_j)}=\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) under heteroskedasticity?\n\\(\\;\\;\\;\\;\\downarrow\\)\n Your hypothesis testing is going to be biased!! \n\n\nQuestionAnswer\n\n\nWhat does it mean to have hypothesis testing biased?\n\n\nRoughly speaking, it means that you over-reject/under-reject the hypothesis than you intend to."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#consequence-of-heteroskedasticity-on-testing",
    "href": "lectures/06-se-estiation/06-se-estimation.html#consequence-of-heteroskedasticity-on-testing",
    "title": "06: Standard Error Estimation",
    "section": "Consequence of heteroskedasticity on testing",
    "text": "Consequence of heteroskedasticity on testing\n\nMotivation and setupMC simulation\n\n\nLet’s run MC simulations to see the consequence of ignoring heteroskedasticity.\n\nModel\n\\(y = 1 + \\beta x + u\\), where \\(\\beta = 0\\)\n\nTest of interest\n\n\\(H_0:\\) \\(\\beta=0\\)\n\\(H_1:\\) \\(\\beta \\ne 0\\)\n\n\n\nQuestionAnswer\n\n\nIf you test the null hypothesis at the \\(5\\%\\) significance level, what should be the probability that you reject the null hypothesis when it is actually true?\n\\(Pr(\\mbox{reject} \\;\\; H_0|H_0 \\;\\; \\mbox{is true})=?\\)\n\n\n\\(5\\%\\)\n\n\n\n\n\n\n\nconceptual stepsR implementationResults\n\n\n\ngenerate a dataset so that \\(\\beta_1\\) (the coefficient on \\(x\\)) is zero\n\n\\[y=\\beta_0+\\beta_1 x + v\\]\n\nestimate the model and find \\(\\widehat{\\beta}_1\\) and \\(\\widehat{se(\\widehat{\\beta}_1)}\\)\ncalculate \\(t\\)-statistic \\((\\widehat{\\beta}_x-0)/\\widehat{se(\\widehat{\\beta}_x)}\\) and decide whether you reject the null or not\nrepeat the above 1000 times\ncheck how often you reject the null (should be close to 50 times)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nConsequence of ignoring heteroskedasticity\n\n\nWe rejected the null hypothesis 10.8% of the time, instead of \\(5\\%\\).\n\nSo, in this case, you are more likely to claim that \\(x\\) has a statistically significant impact than you are supposed to.\nThe use of the formula \\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) seemed to (over/under)-estimate the true variance of the OLS estimators?\nIn general, the direction of bias is ambiguous."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#how-should-we-address-this-problem",
    "href": "lectures/06-se-estiation/06-se-estimation.html#how-should-we-address-this-problem",
    "title": "06: Standard Error Estimation",
    "section": "How should we address this problem?",
    "text": "How should we address this problem?\n\nWhat to do?Robust estimator of seIn practice\n\n\nNow, we understand the consequence of heteroskedasticity:\n\\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) is a biased estimator of \\(Var(\\widehat{\\beta})\\), which makes any kind of testings based on it invalid.\nCan we credibly estimate the variance of the OLS estimators?\n\n\n\n\n\nWhite-Huber-Eicker heteroskedasticity-robust standard error estimator\n\n\n\nvalid in the presence of heteroskedasticity of .red[unknown form]\nheteroskedasticity-robust standard error estimator in short\n\n\n\n\n\n\n\nHeteroskedasticity-robust standard error estimator\n\\(\\widehat{Var(\\widehat{\\beta}_j)} = \\frac{\\sum_{i=1}^n \\widehat{r}^2_{i,j} \\widehat{u}^2_i}{SSR^2_j}\\)\n\n\\(\\widehat{u}_i\\): residual from regressing \\(y\\) on all the independent variables\n\\(\\widehat{r}_{i,j}\\): residual from regressing \\(x_j\\) on all other independent variables for \\(i\\)th observation\n\\(SSR^2_j\\): the sum of squared residuals from regressing \\(x_j\\) on all other independent variables\n\n\n\n\nNote\n\n\nWe spend  NO  time to try to understand what’s going on with the estimator.\n\n\n\nWhat you need is\n\nunderstand the consequence of heteroskedasticity\nknow there is an estimator that is appropriate under heteroskedasticity, meaning that it will give you the correct estimate of the variance of the OLS estimator\nknow how to use the heteroskedasticity-robust standard error estimator in practice using \\(R\\) (or some other software)\n\n\n\nHere is the well-accepted procedure in econometric analysis:\n\nEstimate the model using OLS (you do nothing special here)\nAssume the error term is heteroskedastic and estimate the variance of the OLS estimators\n\nThere are tests to whether error is heteroskedastic or not: .red[Breusch-Pagan] test and .red[White] test\nIn practice, almost nobody bothers to conduct these tests\nWe do not learn how to run these tests\n\nReplace the estimates from \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) with those from \\(\\widehat{Var(\\widehat{\\beta})}_{robust}\\) for testing\nBut, we do not replace coefficient estimates (remember, coefficient estimation is still unbiased under heteroskedasticity)"
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#robust-standard-error-estimation-in-r",
    "href": "lectures/06-se-estiation/06-se-estimation.html#robust-standard-error-estimation-in-r",
    "title": "06: Standard Error Estimation",
    "section": "Robust standard error estimation in R",
    "text": "Robust standard error estimation in R\n\nImplementation in RReporting the regression resultsAlternatively (Recommended)Validation\n\n\n\nPreparationObtaining Heteroskedasticity-robust SE estimatesCompare with the Default\n\n\nLet’s run a regression using MLB1.dta.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe use\n\nthe stats::vcov() function to estimate heteroskedasticity-robust standard errors\nthe fixest::se() function from the fixest package to estimate heteroskedasticity-robust standard errors (you can always get SE from VCOV)\nthe summary() function to do tests of \\(\\beta_j = 0\\)\n\nGeneral Syntax\nHere is the general syntax to obtain various types of VCOV (and se) esimaties:\n\n#* vcov\nvcov(regression result, vcov = \"type of vcov\")\n\n#* only the standard errors\nfixest::se(regression result, vcov = \"type of vcov\")\n\n\nheteroskedasticity-robust standard error estimation\nSpecifically for White-Huber heteroskedasticity-robust VCOV and se estimates,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDefault\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHeteroskedasticity-robust\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nIn presenting the regression results in a nicely formatted table, we used modelsummary::msummary().\nWe can easily swap the defulat se with the heteroskedasticity-robust se using the statistic_override option in msummary().\n\n\n\n\nvcov_het &lt;- vcov(reg_mlb, vcov = \"hetero\")\nvcov_homo &lt;- vcov(reg_mlb)\n\nmodelsummary::msummary(\n  list(reg_mlb, reg_mlb),\n  statistic_override = list(vcov_het, vcov_homo),\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n) \n\n\n\n\n \n\n  \n    \n    \n    tinytable_n6pj76sr9t3q3voh6uyv\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  11.042***\n                  11.042***\n                \n                \n                             \n                  (0.343)  \n                  (0.343)  \n                \n                \n                  years      \n                  0.166*** \n                  0.166*** \n                \n                \n                             \n                  (0.013)  \n                  (0.013)  \n                \n                \n                  bavg       \n                  0.005*** \n                  0.005*** \n                \n                \n                             \n                  (0.001)  \n                  (0.001)  \n                \n                \n                  Num.Obs.   \n                  353      \n                  353      \n                \n                \n                  R2         \n                  0.367    \n                  0.367    \n                \n                \n                  RMSE       \n                  0.94     \n                  0.94     \n                \n                \n                  Std.Errors \n                  IID      \n                  IID      \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\nAlternatively, you could add the vcov option like below inside fixest::feols(). Then, you do not need statistic_override option to override the default VCOV estimates.\n\n\n\n\nreg_mlb_with_rvcov &lt;- \n  fixest::feols(\n    log(salary) ~ years + bavg,\n    vcov = \"hetero\", \n    data = mlb1\n  ) \n\nmodelsummary::msummary(\n  list(reg_mlb_with_rvcov),\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n) \n\n\n\n\n \n\n  \n    \n    \n    tinytable_pz3jc4vn9is36vy1k18e\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  11.042***                \n                \n                \n                             \n                  (0.704)                  \n                \n                \n                  years      \n                  0.166***                 \n                \n                \n                             \n                  (0.018)                  \n                \n                \n                  bavg       \n                  0.005+                   \n                \n                \n                             \n                  (0.003)                  \n                \n                \n                  Num.Obs.   \n                  353                      \n                \n                \n                  R2         \n                  0.367                    \n                \n                \n                  RMSE       \n                  0.94                     \n                \n                \n                  Std.Errors \n                  Heteroskedasticity-robust\n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\n\nMC simulationMC simulation results\n\n\nDoes the heteroskedasticity-robust se estimator really work? Let’s see using MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOkay, not perfect. But, certainly better."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#clustered-error-1",
    "href": "lectures/06-se-estiation/06-se-estimation.html#clustered-error-1",
    "title": "06: Standard Error Estimation",
    "section": "Clustered Error",
    "text": "Clustered Error\n\nWhat is it?Consequences of clustered errorMC simulationsWhat to do?In practice\n\n\n\nOften times, observations can be grouped into clusters\nErrors within the cluster can be correlated\n\n\nExample 1Example 2\n\n\nCollege GPA: cluster by college\n\\(GPA_{col} = \\beta_0 + \\beta_1 income + \\beta_2 GPA_{hs} + u\\)\n\nYour observations consist of students’ GPA scores across many colleges\nBecause of some unobserved (omitted) school characteristics, error terms for the individuals in the same college might be correlated.\n\ngrading policy\n\n\n\n\nEduction Impacts on Income: cluster by individual\n\nYour observations consist of 500 individuals with each individual tracked over 10 years\nBecause of some unobserved (omitted) individual characteristics, error terms for time-series observations within an individual might be correlated.\n\ninnate ability\n\n\n\n\n\n\n\n\n\nQuestion 1Question 2Question 3\n\n\nAre the OLS estimators of the coefficients biased in the presence of clustered error?\n\n\n\nAnswer\n\nNo, the correlation between \\(x\\) and \\(u\\) would hurt you, but not correlation among \\(u\\).\n\n\n\n\nAre \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) unbiased estimators of \\(Var(\\widehat{\\beta})\\)?\n\n\n\nAnswer\n\nNo, \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) is unbiased only under homoskedasticity assumption, which assumes no correlation between errors.\n\n\n\nWhich has more information?\n\ntwo errors that are independent\ntwo errors that are correlated\n\nConsequences\n\nIf you were to use \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) to estimate \\(Var(\\widehat{\\beta})\\) in the presence of clustered error, you would (under/over)-estimate the true \\(Var(\\widehat{\\beta})\\).\nThis would lead to rejecting null hypothesis (more/less) often than you are supposed to.\n\n\n\n\n\n\n\n\nConceptual stepsData Genrating Process (R)Visualization of clustered errorMC simualtion (R)Results\n\n\nHere are the conceptual steps of the MC simulations to see the consequence of clustered error.\n\ngenerate data according to the generating process in which the error terms \\((u)\\) within the cluster (two clusters in this example) is correlated and \\(\\beta_1\\) is set to 0 in the model below:\n\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 x + u\n\\end{aligned}\n\\]\n\nestimate the model and find \\(\\widehat{\\beta}_x\\) and \\(\\widehat{se(\\widehat{\\beta}_x)}\\)\ncalculate \\(t\\)-statistic \\((\\widehat{\\beta}_x/\\widehat{se(\\widehat{\\beta}_x)})\\) for the (correct) null hypothesis of \\(\\beta_1 = 0\\)\nrepeat steps 1-3 for 1000 times\nsee how many times out of 1000 times you reject the null hypothesis: \\(H_0:\\) \\(\\beta_x=0\\)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nset.seed(58934)\nB &lt;- 1000\nt_stat_store &lt;- rep(0,B)\nN &lt;- 2000 # total number of observations\nG &lt;- 50 # number of groups\nNg &lt;- N/G # number of observations per group\n\nfor (i in 1:B){\n  #--- error correlated within group ---#\n  u &lt;- \n  mvrnorm(\n    G, mu = rep(0, Ng), \n    Sigma = matrix(10, nrow = Ng, ncol = Ng) + diag(Ng)\n  ) %&gt;% t() %&gt;% c()\n\n  #--- x correlated within group ---#\n  x &lt;- \n    mvrnorm(\n      G, mu = rep(0, Ng), \n      Sigma = matrix(1, nrow = Ng, ncol = Ng) + diag(Ng) * .2\n    ) %&gt;% t() %&gt;% c()\n\n  #--- other variables ---#\n  y &lt;- 1 + 0 * x + u\n\n  #--- data.frame ---#\n  data &lt;- data.frame(y = y, x = x, group = rep(1:G, each = Ng))\n\n  #--- OLS ---#\n  reg &lt;- feols(y ~ x, data = data)\n\n  #--- get vcov ---#\n  se_default &lt;- se(reg)[\"x\"]\n\n  #--- calculate t-stat ---#\n  t_stat &lt;- reg$coefficient['x']/se_default\n  t_stat_store[i] &lt;- t_stat\n} \n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nImportant\n\n\n\nclustered error can severely bias your test results\nit tends to make the impact of explanatory variables more significant than they truly are because the default estimator of the variance of the OLS estimator tends to greatly  under-estimate the true variance of the OLS estimator.\n\n\n\n\n\n\n\n\n\n\n\nCluster-robust SE estimationR implementationR DemonstrationCompareAlternativelya (Recommended)\n\n\nThere exist estimators of \\(Var(\\widehat{\\beta})\\) that take into account the possibility that errors are clustered.\n\nWe call such estimators  cluster-robust variance covariance estimator  denoted as \\((\\widehat{Var(\\widehat{\\beta})}_{cl})\\)\nWe call standart error estimates from such estimators cluster-robust standard error estimates\n\n I neither derive nor show the mathematical expressions of these estimators. \n\n\n\n\n\nThis is what you need to do\n\n\n\nunderstand the consequence of clustered errors\nknow there are estimators that are appropriate under clustered error\nknow that the estimators we will learn take care of heteroskedasticity at the same time (so, they really are cluster- and heteroskedasticity-robust standard error estimators)\nknow how to use the estimators in \\(R\\) (or some other software)\n\n\n\n\n\n\n\nCluster-robust standard error\nSimilar with the vcov option for White-Huber heteroskedasticity-robust se, we can use the cluster option to get clsuter-robust se.\n\nBefore an R demonstration\nLet’s take a look at the MLB data again.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nnl: the group variable we cluster around (1 if in the National league, 0 if in the American league).\n\n\n\nStep 1\nRun a regression\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nStep 2\nApply vcov() or se() with the cluster = option.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDefault\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCluster-robust standard error\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nOr, you could add the cluster option inside fixest::feols().\n\nSyntax\n\nfixes::feols(y ~ x, cluster = ~ variable to cluster by, data = data)\n\n\nExample\nThis code cluster by nl.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nJust like the heteroskedasticity-present case before,\n\nEstimate the model using OLS (you do nothing special here)\nAssume the error term is clustered and/or heteroskedastic, and estimate the variance of the OLS estimators \\((Var(\\widehat{\\beta}))\\) using cluster-robust standard error estimators\nReplace the estimates from \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) with those from \\(\\widehat{Var(\\widehat{\\beta})}_{cl}\\) for testing\nBut, we do not replace coefficient estimates."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#but-does-it-really-work",
    "href": "lectures/06-se-estiation/06-se-estimation.html#but-does-it-really-work",
    "title": "06: Standard Error Estimation",
    "section": "But does it really work?",
    "text": "But does it really work?\nLet’s run MC simulations to see if the use of the cluster-robust standard error estimation method works\n\nMC simulation (R)MC simulation resultsMore groupsMC simulation results\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWell, we are still rejecting too often than we should, but it is much better than the default VCOV estimator that rejected 74% of the time.\n\n\n\n\nImportant\n\n\n\nCluster-robust standard error estimation gets better as the number of groups gets larger\nThe number of groups of 2 is too small (the MLB case)\nAs a rule of thumb, # of groups larger than 50 is sufficiently large, but we just saw we still over-rejected the null of \\(\\beta = 0\\) three times more than we should.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBetter. But, we are still over-rejecting. Don’t forget it is certianly better than using the default!\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Population, Sample, and Econometrics",
    "text": "Population, Sample, and Econometrics\n\nPopulationSample\n\n\n\n\n\n\nDefinition\n\n\nA set of \\(ALL\\) individuals, items, phenomenon, that you are interested in learning about\n\n\n\n\n\nExample\n\nSuppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.\nSuppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.\n\nImportant\nPopulation differs depending on the scope of your interest\n\nIf you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world\nIf you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.\n\n\n\n\n\n\n\nDefinition\n\n\nSample is a subset of population that you observe\n\n\n\n\n\nCase 1Case 2\n\n\n\nPopulation: you are interested in the impact of education on wage\nSample (example): data on education, income, and many other things for 300 individuals from each State\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?\n\n\n\n\n\n\n\nPopulation: you are interested in the impact of water price on irrigation by farmers in Nebraska\nSample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Simple univariate model",
    "text": "Simple univariate model\n\nWhat is it?What does \\(\\beta_1\\) measure?What does \\(\\beta_0\\) measure?Visualized\n\n\nConsider a phenomenon in the population that is correctly represented by the following model ( This is the model you want to learn about using sample ):\n\\[\\begin{equation}\ny=\\beta_0+\\beta_1 x + u\n\\end{equation}\\]\n\n\\(y\\): to be explained by \\(x\\) ( dependent variable)\n\\(x\\): explain \\(y\\) ( independent variable ,  covariate ,  explanatory variable )\n\\(u\\): parts of \\(y\\) that cannot be explained by \\(x\\) ( error term )\n\\(\\beta_0\\) and \\(\\beta_1\\): real numbers that gives the model a quantitative meaning ( parameters )\n\n\n\n\n\nImportant\n\n\nYou will never know the true model. You can try estimating it using sample! That is what statistics is about.\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nIf you change \\(x\\) by \\(1\\) unit while holding \\(u\\) (everything else) constant,\n\\[\\begin{align}\n  y_{before} & = \\beta_0+\\beta_1 x + u \\\\\n  y_{after} & = \\beta_0+\\beta_1 (x + 1) + u\n\\end{align}\\]\nThe difference in \\(y_{before}\\) and \\(y_{after}\\),\n\\[\\begin{align}\n  \\Delta y = \\beta_1\n\\end{align}\\]\nThat is, \\(y\\) changes by \\(\\beta_1\\).\n\n\n\n\n\nSo,\n\n\n\n\\(\\beta_1\\) is the change in \\(y\\) when \\(x\\) increases by 1\nWe call \\(\\beta_1\\) the  ceteris paribus  (with everything else fixed) causal impact of \\(x\\) on \\(y\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nWhen \\(x = 0\\) and \\(u=0\\),\n\\[\\begin{align}\ny=\\beta_0\n\\end{align}\\]\nSo, \\(\\beta_0\\) represents the intercept.\n\n\n\n\n\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): coefficient (slope)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Why do we want  ceteris paribus  causal impact?",
    "text": "Why do we want  ceteris paribus  causal impact?\n\nExampleWhy ceteris paribus impact?What do you observe?\n\n\nQuality of College\nYou\n\nhave been admitted to University A (better, more expensive) and B (worse, less expensive)\nare trying to decide which school to attend\nare interested in knowing a boost in your future income to make a decision\n\nYou have found the following data\n\n\nUniversityaverage incomesample sizeA130.13500B90.13500\n\n\nQuestion\nShould you assume that the observed difference of 40 is the expected boost you would get if you are to attend University A instead of B?\n\n\nLet’s say your ability score is \\(6\\) out of \\(10\\) (the higher, the better),\n\\[\\mbox{(1)}\\;\\; E[inc|A,ability=9] -E[inc|B,ability=6]\\] \\[\\mbox{(2)}\\;\\; E[inc|A,ability=6] -E[inc|B,ability=6]\\]\nWhich one would like you to know?\n\n\n\n\n\nImportant\n\n\n\nYou want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A\nYou do not want the impact of school quality to be confounded with something else\n\n\n\n\n\n\n\n\n\n\n\nAside: Conditional Expectation\n\n\n\\(E[Y|X]\\) represents expected value of \\(Y\\) conditional on \\(X\\) (For a given value of \\(X\\), the expected value of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nred line: \\(E[income|A, ability]\\)\nblue line: \\(E[income|B, ability]\\)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Example: corn yield and fertilizer",
    "text": "Example: corn yield and fertilizer\n\nModelEstimate \\(\\beta_1\\)Crucial conditionCondition satisfied?Math asides\n\n\nCorn yield and fertilizer\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\nQuestion\nWhat is in the error term?\n\n\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\n\nyou do not know \\(\\beta_0\\) and \\(\\beta_1\\), and would like to estimate them\nyou observe a series of \\(\\{yield_i,fertilizer_i\\}\\) combinations \\((i=1,\\dots,n)\\)\nyou would like to estiamte \\(\\beta_1\\), the impact of fertilizer on yield, ceteris paribus (with everything else fixed)\n\nQuestion\nHow could we possibly find the ceteris paribus impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?\n\n\nIt turns out we can identify the ceteris paribus causal impact of \\(x\\) on \\(y\\) as long as the following condition is satisfied:\n\n\n\n\nZero conditional mean\n\n\n\\(E(u|x) = 0\\)\n\n\n\n\nThis is satisfied when \\(E[u|x]=E[u]\\) and \\(E[u] = 0\\). Practically (and roughtly) speaking, this condition is satisfied if\n\n\n\nImportant\n\n\n\n the error term (\\(u\\)) is not correlated with \\(x\\) \n\nan intercept (\\(\\beta_0\\)) is included in the model (which we almost always do by default)\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer + u\n\\end{align}\\]\n\nData\nYou have collected farm-level yield-fertilizer data from 200 farmers in year 2023.\n\nQuestions\n\nWhat’s in \\(u\\)? (note that factors that do not affect yield are not part of \\(u\\))\nIs it correlated with fertilizer?\n\n\n\n\nMean independenceCorrelation and mean independence\\(E(u)=0\\)\n\n\n\n\n\n\nDefinition: Mean Independence\n\n\n\\(E[u|x]=E[u]\\)\n\n\n\n\n\nverbally: the average value of the error term (collection of all the unobservables) is the same at any value of \\(x\\), and that the common average is equal to the average of \\(u\\) over the entire population\n(almost) interchangeably: the error term is not correlated with \\(x\\)\n\n\n\nMean independence of \\(u\\) and \\(x\\) implies no correlation. But, no correlation does not imply mean independence.\n\\[\\begin{aligned}\n    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\\\\\\n    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\\\\\\n    = & E[ux] \\\\\\\\\n    = & E_x[E_u[u|x]] \\;\\; \\mbox{(iterated law of expectation)}\n\\end{aligned}\\]\nIf zero conditional mean condition \\((E(u|x)=0)\\) is satisfied,\n\\[\\begin{aligned}\n    Cov(u,x)= & E_x[0] = 0\n\\end{aligned}\\]\n\n\nExpected value of the error term is 0 \\((E(u)=0)\\).\nThis is always satisfied as long as an intercept is included in the model:\n\\[y = \\beta_0 + \\beta_1 x + u_1,\\;\\; \\mbox{where}\\;\\; E(u_1)=\\alpha\\]\nRewriting the model,\n\\[\\begin{aligned}\ny & = \\beta_0 + \\alpha + \\beta_1 x + u_1 - \\alpha \\\\\\\\\n  & = \\gamma_0 + \\beta_1 x + u_2\n\\end{aligned}\\]\nwhere, \\(\\gamma_0=\\beta_0+\\alpha\\) and \\(u_2=u_1-\\alpha\\).\nNow, \\(E[u_2]=0\\)."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Going back to the college-income example",
    "text": "Going back to the college-income example\nThe model\n\\[\nIncome = \\beta_0+\\beta_1 College\\;\\; A + u\n\\]\nwhere \\(College\\;\\; A\\) is 1 if attending college A, 0 if attending college B, and \\(u\\) is the error term that includes ability. \\(u\\) includes ability.\n\nZero conditional mean satisfied?\n\\[\nE[u(ability)|college A] = 0?\n\\]\nThat is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?\n\n\n\n\n\n\n\n\n\nThis is what it would like if college choice and ability are not correlated:"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Exercise",
    "text": "Exercise\n\nconsider a phenomenon you are interested in understanding\n\ndependent variable (variable to be explained)\nexplanatory variable (variable to explain)\n\nconstruct a simple linear model\nidentify what is in the error term\ncheck if they are correlated withe explanatory variable or not\n\n\n\n\nback to course website with lecture slides"
  }
]