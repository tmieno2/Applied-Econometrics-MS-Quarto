[
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Population, Sample, and Econometrics",
    "text": "Population, Sample, and Econometrics\n\nPopulationSample\n\n\n\n\n\n\nDefinition\n\n\nA set of \\(ALL\\) individuals, items, phenomenon, that you are interested in learning about\n\n\n\n\n\nExample\n\nSuppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.\nSuppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.\n\nImportant\nPopulation differs depending on the scope of your interest\n\nIf you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world\nIf you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.\n\n\n\n\n\n\n\nDefinition\n\n\nSample is a subset of population that you observe\n\n\n\n\n\nCase 1Case 2\n\n\n\nPopulation: you are interested in the impact of education on wage\nSample (example): data on education, income, and many other things for 300 individuals from each State\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?\n\n\n\n\n\n\n\nPopulation: you are interested in the impact of water price on irrigation by farmers in Nebraska\nSample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Simple univariate model",
    "text": "Simple univariate model\n\nWhat is it?What does \\(\\beta_1\\) measure?What does \\(\\beta_0\\) measure?Visualized\n\n\nConsider a phenomenon in the population that is correctly represented by the following model ( This is the model you want to learn about using sample ):\n\\[\\begin{equation}\ny=\\beta_0+\\beta_1 x + u\n\\end{equation}\\]\n\n\\(y\\): to be explained by \\(x\\) ( dependent variable)\n\\(x\\): explain \\(y\\) ( independent variable ,  covariate ,  explanatory variable )\n\\(u\\): parts of \\(y\\) that cannot be explained by \\(x\\) ( error term )\n\\(\\beta_0\\) and \\(\\beta_1\\): real numbers that gives the model a quantitative meaning ( parameters )\n\n\n\n\n\nImportant\n\n\nYou will never know the true model. You can try estimating it using sample! That is what statistics is about.\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nIf you change \\(x\\) by \\(1\\) unit while holding \\(u\\) (everything else) constant,\n\\[\\begin{align}\n  y_{before} & = \\beta_0+\\beta_1 x + u \\\\\n  y_{after} & = \\beta_0+\\beta_1 (x + 1) + u\n\\end{align}\\]\nThe difference in \\(y_{before}\\) and \\(y_{after}\\),\n\\[\\begin{align}\n  \\Delta y = \\beta_1\n\\end{align}\\]\nThat is, \\(y\\) changes by \\(\\beta_1\\).\n\n\n\n\n\nSo,\n\n\n\n\\(\\beta_1\\) is the change in \\(y\\) when \\(x\\) increases by 1\nWe call \\(\\beta_1\\) the  ceteris paribus  (with everything else fixed) causal impact of \\(x\\) on \\(y\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nWhen \\(x = 0\\) and \\(u=0\\),\n\\[\\begin{align}\ny=\\beta_0\n\\end{align}\\]\nSo, \\(\\beta_0\\) represents the intercept.\n\n\n\n\n\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): coefficient (slope)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Why do we want  ceteris paribus  causal impact?",
    "text": "Why do we want  ceteris paribus  causal impact?\n\nExampleWhy ceteris paribus impact?What do you observe?\n\n\nQuality of College\nYou\n\nhave been admitted to University A (better, more expensive) and B (worse, less expensive)\nare trying to decide which school to attend\nare interested in knowing a boost in your future income to make a decision\n\nYou have found the following data\n\n\nUniversityaverage incomesample sizeA130.13500B90.13500\n\n\nQuestion\nShould you assume that the observed difference of 40 is the expected boost you would get if you are to attend University A instead of B?\n\n\nLet’s say your ability score is \\(6\\) out of \\(10\\) (the higher, the better),\n\\[\\mbox{(1)}\\;\\; E[inc|A,ability=9] -E[inc|B,ability=6]\\] \\[\\mbox{(2)}\\;\\; E[inc|A,ability=6] -E[inc|B,ability=6]\\]\nWhich one would like you to know?\n\n\n\n\n\nImportant\n\n\n\nYou want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A\nYou do not want the impact of school quality to be confounded with something else\n\n\n\n\n\n\n\n\n\n\n\nAside: Conditional Expectation\n\n\n\\(E[Y|X]\\) represents expected value of \\(Y\\) conditional on \\(X\\) (For a given value of \\(X\\), the expected value of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nred line: \\(E[income|A, ability]\\)\nblue line: \\(E[income|B, ability]\\)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Example: corn yield and fertilizer",
    "text": "Example: corn yield and fertilizer\n\nModelEstimate \\(\\beta_1\\)Crucial conditionCondition satisfied?Math asides\n\n\nCorn yield and fertilizer\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\nQuestion\nWhat is in the error term?\n\n\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\n\nyou do not know \\(\\beta_0\\) and \\(\\beta_1\\), and would like to estimate them\nyou observe a series of \\(\\{yield_i,fertilizer_i\\}\\) combinations \\((i=1,\\dots,n)\\)\nyou would like to estiamte \\(\\beta_1\\), the impact of fertilizer on yield, ceteris paribus (with everything else fixed)\n\nQuestion\nHow could we possibly find the ceteris paribus impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?\n\n\nIt turns out we can identify the ceteris paribus causal impact of \\(x\\) on \\(y\\) as long as the following condition is satisfied:\n\n\n\n\nZero conditional mean\n\n\n\\(E(u|x) = 0\\)\n\n\n\n\nThis is satisfied when \\(E[u|x]=E[u]\\) and \\(E[u] = 0\\). Practically (and roughtly) speaking, this condition is satisfied if\n\n\n\nImportant\n\n\n\n the error term (\\(u\\)) is not correlated with \\(x\\) \n\nan intercept (\\(\\beta_0\\)) is included in the model (which we almost always do by default)\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer + u\n\\end{align}\\]\n\nData\nYou have collected farm-level yield-fertilizer data from 200 farmers in year 2023.\n\nQuestions\n\nWhat’s in \\(u\\)? (note that factors that do not affect yield are not part of \\(u\\))\nIs it correlated with fertilizer?\n\n\n\n\nMean independenceCorrelation and mean independence\\(E(u)=0\\)\n\n\n\n\n\n\nDefinition: Mean Independence\n\n\n\\(E[u|x]=E[u]\\)\n\n\n\n\n\nverbally: the average value of the error term (collection of all the unobservables) is the same at any value of \\(x\\), and that the common average is equal to the average of \\(u\\) over the entire population\n(almost) interchangeably: the error term is not correlated with \\(x\\)\n\n\n\nMean independence of \\(u\\) and \\(x\\) implies no correlation. But, no correlation does not imply mean independence.\n\\[\\begin{aligned}\n    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\\\\\\n    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\\\\\\n    = & E[ux] \\\\\\\\\n    = & E_x[E_u[u|x]] \\;\\; \\mbox{(iterated law of expectation)}\n\\end{aligned}\\]\nIf zero conditional mean condition \\((E(u|x)=0)\\) is satisfied,\n\\[\\begin{aligned}\n    Cov(u,x)= & E_x[0] = 0\n\\end{aligned}\\]\n\n\nExpected value of the error term is 0 \\((E(u)=0)\\).\nThis is always satisfied as long as an intercept is included in the model:\n\\[y = \\beta_0 + \\beta_1 x + u_1,\\;\\; \\mbox{where}\\;\\; E(u_1)=\\alpha\\]\nRewriting the model,\n\\[\\begin{aligned}\ny & = \\beta_0 + \\alpha + \\beta_1 x + u_1 - \\alpha \\\\\\\\\n  & = \\gamma_0 + \\beta_1 x + u_2\n\\end{aligned}\\]\nwhere, \\(\\gamma_0=\\beta_0+\\alpha\\) and \\(u_2=u_1-\\alpha\\).\nNow, \\(E[u_2]=0\\)."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Going back to the college-income example",
    "text": "Going back to the college-income example\nThe model\n\\[\nIncome = \\beta_0+\\beta_1 College\\;\\; A + u\n\\]\nwhere \\(College\\;\\; A\\) is 1 if attending college A, 0 if attending college B, and \\(u\\) is the error term that includes ability. \\(u\\) includes ability.\n\nZero conditional mean satisfied?\n\\[\nE[u(ability)|college A] = 0?\n\\]\nThat is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?\n\n\n\n\n\n\n\n\n\nThis is what it would like if college choice and ability are not correlated:"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Exercise",
    "text": "Exercise\n\nconsider a phenomenon you are interested in understanding\n\ndependent variable (variable to be explained)\nexplanatory variable (variable to explain)\n\nconstruct a simple linear model\nidentify what is in the error term\ncheck if they are correlated withe explanatory variable or not\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "href": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "title": "00: Introduction to Econometrics",
    "section": "What econometrics is about",
    "text": "What econometrics is about\n\nWhat?Steps in Econometric Analysis\n\n\nWhat are we doing?\nEstimate quantitative relationships between variables.\n\nExamples\n\nthe impact of fertilizer on crop yield\nthe impact of political campaign expenditure on voting outcomes\nthe impact of education on wage\n\n\n\n\nformulation of the question of interest (what are you trying to find out?)\ndevelop an economic model of the phenomenon you are interested in understanding (identify variables that matter)\nturn the economic model into an econometric model\ncollect data\n estimate the model using econometrics \n test hypotheses"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "href": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "title": "00: Introduction to Econometrics",
    "section": "Go through the steps",
    "text": "Go through the steps\n\nStep 2: Economic ModelStep 3: Econometric modelStep 4: Collect dataSteps 5 and 6\n\n\nExample: Job training and worker productivity\n\\[wage = f(educ,exper,training)\\]\n\n\\(wage\\): hourly wage\n\\(educ\\): years of formal education\n\\(exper\\): years of workforce experience\n\\(training\\): weeks spent in job training\n\nNote\nDepending on questions you would like to answer, the economic model can (and should) be much more involved\n\n\nWe have built a conceptual model:\n\\[wage = f(educ,exper,training)\\]\nNow, the form of the function \\(f(\\cdot)\\) must be specified (almost always) before we can undertake an econometric analysis\n\\[\nwage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\n\\]\n\\(\\beta_0,\\beta_1,\\beta_2,\\beta_3\\)\n\nare the  parameters  of the econometric model.\ndescribe the directions and strengths of the relationship between \\(wage\\) and the factors used to determine \\(wage\\) in the model\n\n\\(u\\)\n\nis called error term\nincludes  ALL  the other factors that can affect wage other than the included variables (like innate ability)\n\n\n\nWe can collect data using various ways. Some of them include survey, websites, experiments. Let’s look at different data types:\n\nCross-sectional DataTime-series DataPanel (Longitudinal) Data\n\n\n\nSample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time\nThe data on all units do not correspond to precisely the same time period\n\nsome families surveyed during different weeks within a year\n\n\nWhat a cross-sectional data looks like on R\n\n\n      wage  educ exper female married\n     &lt;num&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n  1:  3.10    11     2      1       0\n  2:  3.24    12    22      1       1\n  3:  3.00    11     2      0       0\n  4:  6.00     8    44      0       1\n  5:  5.30    12     7      0       1\n ---                                 \n522: 15.00    16    14      1       1\n523:  2.27    10     2      1       0\n524:  4.67    15    13      0       1\n525: 11.56    16     5      0       1\n526:  3.50    14     5      1       0\n\n\n\n\nObservations on a variable or several variables over time + corn price + oil price\n\nNote\n\nThe econometric frameworks necessary to analyze time series data are quite different from those for cross-sectional data\nWe do  NOT  learn time-series econometric methods\n\n\n\nTime series data for each cross-sectional member in the data set ( same  cross-sectional units are tracked over a given period of time)\nExample\n\nwage data for individuals collected every five years over the past 30 years\nyearly GDP data for 60 countries over the past 10 years\n\nWhat a panel data looks like on R\n\n\n     county  year    crmrte   prbarr  prbpris\n      &lt;int&gt; &lt;int&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n  1:      1    81 0.0398849 0.289696 0.472222\n  2:      1    82 0.0383449 0.338111 0.506993\n  3:      1    83 0.0303048 0.330449 0.479705\n  4:      1    84 0.0347259 0.362525 0.520104\n  5:      1    85 0.0365730 0.325395 0.497059\n ---                                         \n626:    197    83 0.0155747 0.226667 0.428571\n627:    197    84 0.0136619 0.204188 0.372727\n628:    197    85 0.0130857 0.180556 0.333333\n629:    197    86 0.0128740 0.112676 0.244444\n630:    197    87 0.0141928 0.207595 0.360825\n\n\n\n\n\n\n\n\nThis is what you learn for the next few months!!\n\nestimate the model using econometrics\ntest hypothesis"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "title": "00: Introduction to Econometrics",
    "section": "Causality and Association",
    "text": "Causality and Association\n\nDistinction between causality and associationGlasses?\n\n\nAssociation\nAn association of two variables arise because  either of or both  variables affect the other variable\n\\[\\begin{align}\n  A \\longleftrightarrow B \\\\\n  A \\longrightarrow B \\\\\n  A \\longleftarrow B\n\\end{align}\\]\nAssociation does  NOT  concern which affects which. Under all the three cases above, A and B are  associated. Or, we say there is an association between A and B. This is what  correlation coefficient  measures.\n\nCausality\nWhen A has a causal impact on B,\n\\[\nA \\longrightarrow B\n\\]\nHere, changes in \\(A\\) cause changes in \\(B\\), not the other way around\n\n\n\nVideoClaimsBut,\n\n\nLet’s watch this interesting CM.\n\n\nPeople who wear glasses are\n\nmuch smarter than those who don’t\nmore likely to pursue higher education\n200% more likely to graduate college\n\nFor you to be convinced to buy glasses, these claims needs to be causal, not association:\n\nDoes wearing glasses make you much smarter?\nDoes wearing glasses make it more likely for you to pursue higher education?\nDoes wearing glasses make it 200% more likely for you to graduate college?\n\n\n\nHowever, this seems to be a more likely explanation of the association:\n\nOne spends more time studying academic subjects\n\nsmarter (or knowledgeable) \\(\\Rightarrow\\) pursue higher education and graduate college\nworsened eyesight \\(\\Rightarrow\\) wear glasses\n\n\n\n\n\nImportant\n\n\n\nWe care about isolating causal effects, but not association\nIdentifying association is super easy\nIdentifying causal effects is extremely hard (this is what we tackle)"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "title": "00: Introduction to Econometrics",
    "section": "Endogeneity: Your Nemesis",
    "text": "Endogeneity: Your Nemesis\n\nEndogeneityExampleWhat happened?EndogeneityAnother example\n\n\nIt is super easy to find an association of multiple variables, but it is incredibly hard to find a causal effect (at least in Economics)!!\nThat is due to the problem called  endogeneity , which is going to be defined formally later.\n\n\nYou are interested in the causal impact of fire fighters on the number of death tolls in fire events\n\n\nfire eventdeath toll# of firefighters deployed11020203351043555050\n\n\nQuestions\n\nHow are they  associated ?\nCan you say anything about the causal effect of fire fighters deployment on the number of death tolls?\n\n\n\nYou ignored an important variable!!\n\n\nfire eventdeath toll# of firefighters deployedscale of fire110202020353510204351055050100\n\n\n\n\n\n\n\n\nDefinition\n\n\nVariables of interest are correlated with some  unobservables  (variables that cannot be observed or are missing) that have non-zero impacts on the variable that you want to explain\n\n\n\n\n\nThe unobserved variables are also called  confounder/confounding factor .\n\nThe example\nIn the the firefighter example,\n\n variable of interest : the number of firefighters\n unobservables/confounder : the scale of fire events (and other factors)\n variable to explain : death toll\n\nThe model\n\\[\\begin{align}\n  \\mbox{death toll} & = \\alpha + \\beta\\; \\mbox{# of fire fighters} + \\mu\\\\\n  ,\\mbox{where } \\mu & = (\\gamma\\; \\mbox{scale} + v) \\mbox{ is the error term (collection of unobservables)}\n\\end{align}\\]\nEndogeneity Problem\n# of fire fighters is correlated with scale, which we ignored\n\n\n\n\n\\[wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\\]\nWhat are unobservables in \\(u\\) that are likely to be correlated with \\(educ\\)?\nAn important unobservable\n\ninnate ability \\(\\Rightarrow\\) wage\ninnate ability \\(\\Rightarrow\\) education"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "href": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "title": "00: Introduction to Econometrics",
    "section": "How to deal with endogeneity",
    "text": "How to deal with endogeneity\n\nThe questionHow to deal with endogeneity?\n\n\nProblem\nMost of the time, you will be faced with endogeneity problems caused by at least one of the followings,\n\nomitted variables (the scale of fire events, innate ability)\nself-selection\nsimultaneity\nmeasurement error\n\nCentral Question\nHow can we avoid or solve endogeneity problems?\n\n\n\nYou have two opportunities to deal with endogeneity problems\n\nat the design (design to collect data) stage\nat the regression stage (what you will learn in this course)\n\nEconometrics has evolved mostly to address endogeneity problems at the  regression stage  because randomized experiments are infeasible most of the time\nHow about econometrics and other fields of statistics: Statistics, Psychometrics, and Biometrics?\n\n\n\nFieldDesignEstimation MethodEconometricsnot feasible (often)intricateMany other fieldsfeasiblerelatively simple"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "href": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "title": "00: Introduction to Econometrics",
    "section": "Randomized-experiments",
    "text": "Randomized-experiments\nIn randomized experiments,\n\nyou have a liberty to determine the level of the variable of interest\nby randomizing the value of the variable of interest, you can effectively break the link (association) with whatever is included in the error term\n\n\nExample (Non-Randomized)RandomizedRandomized Experiments on Education?\n\n\n\nDataFarmer’s decisionBias\n\n\nYield and nitrogen rate data obtained from a field that is managed by a farmer\n\n\n\n\n\n\n\n\n\n\n\nFarmer\n\ndecide nitrogen rate based on soil/field characteristics (some of them we researchers do not get to observe)\n\nResearcher\n\nsoil characteristics is not observable, so it is in the error term\n\n\\[yield = \\beta_0 + \\beta_1 N + (\\gamma SC + \\mu)\\]\n\nN (nitrogen rate) and SC (soil characteristics) are correlated\n\n\n\nSuppose the farmer applied more nitrogen to the area where its soil characteristics lead to higher corn yield\nQuestion If the researcher estimate the model (which ignores soil characteristics), do you over- or under-estimate the impact of nitrogen rate on corn yield?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSoil quality (in error term) is no longer correlated with N!!\n\n\n\n\n\nRandomized Experiment?\nResearchers determine randomly how much education subjects (people) can get?\nEndogeneity Problem in Economics\n\nEconomics is about understanding human behavior\nAlmost always, you need to deal with endogeneity problem because people are smart: we make decisions based on available information (not just randomly) so that our decisions lead to good outcomes (whether our decisions turn out to be good or not is irrelevant)\n\nhow much education one get is determined based on their judgment of their own ability (not by rolling a dice)\nhow many fire fighters to be deployed was determined based on the scale of fire (not by rolling a dice)\nhow much nitrogen to apply based on soil characteristics (not by rolling a dice)\n\nIf people are not smart and just roll a dice for their decision making, we would have much easier time identifying causal effects"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Introduction",
    "text": "Monte Carlo Simulation: Introduction\n\nWhat is ti?QuestionKey part of MC simulations\n\n\nIt is a way to test econometric theories via simulation.\n\nHow is it used in econometrics?\n\nconfirm ecoometric theory numerically\n\nOLS estimators are unbiased if \\(E[u|x]=0\\) along with other conditions (theory)\nI know the above theory is right, but let’s check if it is true numerically\n\nYou kind of sense that something in your data may cause problems, but there is no proven econometric theory about what’s gonna happen (I used MC simulation for this purpose a lot)\nassist students in understanding econometric theories by providing actual numbers instead of a series of Greek letters\n\n\n\nSuppose you are interested in checking what happens to OLS estimators if \\(E[u|x]=0\\) (the error term and \\(x\\) are not correlated) is violated.\nCan you use the real data to do this?\n\n\n You  generate data (you have control over how data are generated)\n\nYou know the true parameter unlike the real data generating process\nYou can change only the part that you want to change about data generating process and econometric methods with everything else fixed"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Generating data",
    "text": "Generating data\n\nRNGPseudo?Normal Distribution\n\n\nPseudo random number generators (Pseudo RNG)\nAlgorithms for generating a sequence of numbers whose properties  approximate  the properties of sequences of random numbers\n\nExamples\nDraw from a uniform distribution:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nNumbers drawn using pseudo random number generators are not truly random\n\nWhat numbers you will get are pre-determined\nWhat numbers you will get can be determined by setting a  seed \n\nDemonstration\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuestion\nWhat benefits does setting a seed have?\n\n\n\n\n\\(x \\sim N(0, 1)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\\(x \\sim N(2, 2)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions",
    "title": "03-1: Monte Carlo Simulation",
    "section": "R functions",
    "text": "R functions\n\ndnorm()pnorm()qnorm()\n\n\ndnorm(x) gives you the height of the density function at \\(x\\).\ndnorm(-1) and dnorm(2)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\npnorm(x) gives you the probability that a single random draw is  less  than \\(x\\).\n\npnorm(1)pnorm(2)Exercise\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the probability that a single random draw from a Normal distribution with mean = 1 and sd = 2 is less than 1?\nWork here\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAnswer\n\n\n\n\n\n\n\nWhat is it?qnorm(0.95)Exercise\n\n\nqnorm(x), where \\(0 &lt; x &lt; 1\\), gives you a number \\(\\pi\\), where the probability of observing a number from a single random draw is less than \\(\\pi\\) with probability of \\(x\\).\nWe call the output of qnorm(x), \\(x%\\) quantile of the standard Normal distribution (because the default is mean = 0 and sd = 1 for rnorm()).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the 88% quantile of Normal distribution with mean = 0 and sd = 9?"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Steps",
    "text": "Monte Carlo Simulation: Steps\n\nspecify the data generating process\ngenerate data based on the data generating process\nget an estimate based on the generated data (e.g. OLS, mean)\nrepeat the above steps many many times\ncompare your estimates with the true parameter\n\nQuestion\nWhy do the steps \\(1-3\\) many many times?"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 1",
    "text": "Monte Carlo Simulation: Example 1\n\nProblemSteps 1-3Sample Mean: Step 4Loop: for loopStep 4Step 5\n\n\n\n\n\n\nQuestion\n\n\nIs sample mean really an unbiased estimator of the expected value?\n\n\n\n\n\nThat is, is \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nrepeat the above steps many times\nWe use a  loop  to do the same (similar) thing over and over again\n\n\n\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVerbally\nFor each of \\(i\\) in \\(1:B\\) \\((1, 2, \\dots, 1000)\\), do print(i).\n\ni takes the value of 1, and then print(1)\ni takes the value of 2, and then print(2)\n…\ni takes the value of 999, and then print(999)\ni takes the value of 1000, and then print(1000)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCompare your estimates with the true parameter\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 2",
    "text": "Monte Carlo Simulation: Example 2\n\nQuestionR codeCompare\n\n\n\n\n\n\nQuestion\n\n\nWhat happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\)?\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 3 (optional)",
    "text": "Monte Carlo Simulation: Example 3 (optional)\n\nQuestionR codeCheckDistribution\n\n\nModel\n\\[\\begin{aligned}\n    y = \\beta_0 + \\beta_1 x + u \\\\\n\\end{aligned}\\]\n\n\\(x\\sim N(0,1)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u|x]=0\\)\n\n.content-box-green[Variance of the OLS estimator]\nTrue Variance of \\(\\hat{\\beta_1}\\): \\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} = \\frac{\\sigma^2}{SST_X}\\)\nIts estimator: \\(\\widehat{V(\\hat{\\beta_1})} =\\frac{\\hat{\\sigma}^2}{SST_X} = \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{n-2} \\times \\frac{1}{SST_X}\\)\n.content-box-green[Question]\nDoes the estimator really work? (Is it unbiased?)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTrue Variance\n\n\\(SST_X = 112.07\\)\n\\(\\sigma^2 = 4\\)\n\n\\[V(\\hat{\\beta}) = 4/112.07 = 0.0357\\]\nCheck\nYour Estimates of Variance of \\(\\hat{\\beta_1}\\)?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "title": "03-1: Monte Carlo Simulation",
    "section": "Exercise (optional)",
    "text": "Exercise (optional)\n\nProblemSolutionResults visualization\n\n\nUsing MC simulations, find out how the variation in \\(x\\) affects the OLS estimators\n\nModel setup\n\\[\\begin{align}\n  y = \\beta_0 + \\beta_1 x_1 + u \\\\\n  y = \\beta_0 + \\beta_1 x_2 + u\n\\end{align}\\]\n\n\\(x_1\\sim N(0,1)\\) and \\(x_2\\sim N(0,9)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u_1|x]=0\\) and \\(E[u_2|x]=0\\)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Small sample property of OLS estimators",
    "text": "Small sample property of OLS estimators\n\nSmall sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\widehat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS estimator",
    "text": "Unbiasedness of OLS estimator\n\nUnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nDirection\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\widehat{\\beta}_1]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\widehat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\nLinear in Parameters\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\nRandom sampling\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\nVariation in covariates\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\nRoughly speaking\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\widehat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\widehat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\widehat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\widehat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\widehat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\widehat{\\beta}_1|x] = & E[\\widehat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS in practice",
    "text": "Unbiasedness of OLS in practice\n\nGood empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Variance of OLS estimator",
    "text": "Variance of OLS estimator\n\nIntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\widehat{\\beta}_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\widehat{\\beta}_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\widehat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\widehat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\widehat{\\beta}_1) = \\sqrt{Var(\\widehat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Efficiency of OLS Estimators",
    "text": "Efficiency of OLS Estimators\n\nNature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the variance of error",
    "text": "Estimating the variance of error\n\nWhy?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\widehat{u}_i^2\\)\n\n\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\nNote\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.2.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\ndates_data &lt;- \n  data.table(\n    date = seq( as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by=\"+1 day\")\n  ) %&gt;%\n  .[, day := weekdays(date)]\n\nw_month_data &lt;- dates_data[month(date) == 8,]\n\nimp_dates &lt;- \n  rep(NA, nrow(w_month_data))\n\nlecture_ind &lt;- \n  w_month_data[, day %in% c(\"Monday\", \"Wednesday\") & day(date) &gt;= 19]\n\nlab_ind &lt;- \n  w_month_data[, day %in% c(\"Friday\") & day(date) &gt;= 19 ]\n\n# Add the events to the desired days\nimp_dates[lecture_ind] &lt;- \"Lecture\"\nimp_dates[lab_ind] &lt;- \"Lab\"\n\n# Create a calendar with a legend\n\ntemp &lt;- \n  calendR::calendR(\n    year = 2024, \n    month = 8, \n    special.days = imp_dates,\n    special.col = c(\n      \"lightcyan2\", \"tan\"),\n    weeknames = c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ),\n    mbg.col = \"15\",\n    months.col = \"blue\",\n    legend.pos = \"bottom\"\n  )"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "href": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "title": "00-0: Logistics",
    "section": "Instructors",
    "text": "Instructors\n\n Instructor : Taro Mieno (Office: 209, E-mail: tmieno2@unl.edu)\n Teaching Assistant :\n\nTBD"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "href": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "title": "00-0: Logistics",
    "section": "Goals of the course",
    "text": "Goals of the course\n\nLearn modern introductory econometric theory\nApply econometric theories to real economic problems\nLearn how to use statistical software (R) so you can conduct research independently (without technical help from your advisor)\n\nmanage data\nvisualize data\nrun regressions\ninterpret results"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "href": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "title": "00-0: Logistics",
    "section": "Text Books",
    "text": "Text Books\nRecommended: Wooldridge, Jeffrey M. 2006. “Introductory Econometrics: A Modern Approach (5th edition).” Mason, OH: Thomson/South-Western."
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "href": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "title": "00-0: Logistics",
    "section": "Course Schedule",
    "text": "Course Schedule\n\nLectures (MW): 3:00-4:30pm\nLab sessions (F): 1:00-2:30pm"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "href": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "title": "00-0: Logistics",
    "section": "Grading",
    "text": "Grading\n\nProblem sets (3 assignments): 30%\nSmall-size midterms (2): 40%\nPaper: 40%"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "href": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "title": "00-0: Logistics",
    "section": "Assignments",
    "text": "Assignments\nProblem sets + Most questions are from the required text book + Some questions come from what we cover in lab sessions\n** Quarto to do and submit your problem sets** + You are required to present your R codes + You learn how to compile your assignment with your R code written in a document using  Quarto , which will be covered in the second lab session\nCaution + 2nd year students have answers to all the questions I will assign (I will use exactly the same problems because they are really good to learn econometrics) + You are free to copy and paste (or rephrase) the answers for your assignment. I won’t bother to try to tell if you have copied and pasted answers. + However, you are simply doing dis-service to yourself by depriving yourself of learning opportunities + Moreover, your lack of understanding of the material will be clearly manifested on your performance at midterms and final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "href": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "title": "00-0: Logistics",
    "section": "Midterms",
    "text": "Midterms\nIn-class open-book midterms\n\nMidterm 1: Oct, 9 (M)\nMidterm 2: Nov, 20 (M)"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIn this assignment, + you write a paper with a particular emphasis on econometric analysis using a real world data set + you are encouraged to use the data set you are using for your masters thesis (talk with your advisor) + you need to ensure that you use a  panel  dataset + No presentation of your final paper\nTime line\n\n Oct, 16 : identify a research topic and the data set you will be using, and get an approval from the instructor\n Oct, 23 : paper proposal\n Dec, 15 : final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "href": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "title": "00-0: Logistics",
    "section": "Paper Proposal",
    "text": "Paper Proposal\nIntroduction + clear identification of what you are trying to find out (research question) + why the research question is worthwhile answering\nSimple Model + dependent variable (the variable to be explained) + explanatory variable (variables to be explain)\nData Source + where you get data"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIntroduction\n\nclear identification of what you are trying to find out (research question) [1 point]\nwhy the research question is worthwhile answering [1 point]\n\nData description\n\nthe nature of the data with summary statistics table [1 point]\nvisualize a few key variables in a meaningful way [3 points]\n\nEconometric Methods\nThe  process  of how you end up with the final econometric models and methods. [40 points ( or more )]\n\njustification of your choice of independent variables\npotential endogeneity problems\nwhat did you do to address the endogeneity problems?\njustification of econometric model(s) and method(s)\nidentify appropriate standard error estimation methods\n\nResults, Discussions, and Conclusions + interpret and describe the results [2 points] + implications of the results [1 point] + conclusions [1 point]"
  },
  {
    "objectID": "LabLectures.html",
    "href": "LabLectures.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "Visit here for R lab lecture notes."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (MS)",
    "section": "",
    "text": "This website hosts course materials for Applied Econometrics (AECN 896-04) at UNL."
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n00-0: Logistics\n\n\n\n\n00: Introduction to Econometrics\n\n\n\n\n01-1: Univariate Regression: Introduction\n\n\n\n\n01-2: Univariate Regression: OLS Mechanics and Implementation\n\n\n\n\n01-3: Univariate Regression: OLS Small Sample Property\n\n\n\n\n02-1: Multivariate Regression\n\n\n\n\n03-1: Monte Carlo Simulation\n\n\n\n\n04-1: Omitted Variable Bias and Multicollinearity\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall\n\n\n\nMona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall"
  },
  {
    "objectID": "syllabus.html#ta",
    "href": "syllabus.html#ta",
    "title": "Personnel",
    "section": "",
    "text": "Mona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Personnel",
    "section": "Lectures and Labs:",
    "text": "Lectures and Labs:\n\nLectures: MW 3:00 - 4:30 PM\nLabs: F 1:00 - 2:30 PM"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Personnel",
    "section": "Office Hours:",
    "text": "Office Hours:\nWednesday, 10:00 to 11:30 pm or by appointment"
  },
  {
    "objectID": "syllabus.html#lecture",
    "href": "syllabus.html#lecture",
    "title": "Personnel",
    "section": "Lecture",
    "text": "Lecture\n\nIntroduction to econometrics\nSimple univariate regression\nMonte Carlo Simulation\nMultivariate regression\nMulti-collinearity and omitted variable\nHypothesis Testing\nHetereoskedasticity and robust standard error estimation\nClustered error and bootstrap\nFunctional form and scaling\nDummy variables\nPanel data methods\nCausal Inference\nCausal Inference\nLimited dependent variable"
  },
  {
    "objectID": "syllabus.html#computer-lab-r",
    "href": "syllabus.html#computer-lab-r",
    "title": "Personnel",
    "section": "Computer Lab (R)",
    "text": "Computer Lab (R)\n\nIntroduction to R\nRmarkdown\nData wrangling\nData visualization\nResearch Flow and R"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "The data set and model",
    "text": "The data set and model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSet upR code to get dataData visualization\n\n\nData\nObservations of house price and lot size for 546 houses.\n\nModel\n\\[price_i = \\beta_0 + \\beta_1 lotsize_i+u_i\\]\n\n\\(price_i\\): house price ($) of house \\(i\\)\n\\(lotsize_i\\): lot size of house \\(i\\)\n\\(u_i\\): error term (everything else) of house \\(i\\)\n\n\nObjective\nEstimate the impact of lot size on house price\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Estimation with OLS",
    "text": "Estimation with OLS\n\nRough ideaExamplesResidualsOLSVisualizationDerivationEstimators vs Estimates\n\n\n\nWe want to draw a line like this, the slope of which is an estimate of \\(\\beta_1\\)\nA way: Ordinary Least Squares (OLS)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEx. 1: \\(\\widehat{\\beta}_0=20000\\), \\(\\widehat{\\beta}_1=7\\)Ex. 2: \\(\\widehat{\\beta}_0=70000\\), \\(\\widehat{\\beta}_1=3.8\\)So,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion\n\n\n\nAmong all the possible values of \\(\\beta_0\\) and \\(\\beta_1\\), which one is the best?\nWhat criteria do we use (what does the best even mean?)\n\n\n\n\n\n\n\n\n\n\n\nFor particular values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) you pick, the modeled value of \\(y\\) for individual \\(i\\) is \\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\).\nThen, the residual for individual \\(i\\) is:\n\\[\n\\widehat{u}_i =  y_i - (\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i)\n\\]\nThat is, residual is the observed value of the dependent variable less the value of modeled value. For different values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\), you have a different value of residual.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIdea of OLS (Ordinary Least Squares)\nLet’s find the value of \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the sum of the squared residuals!\n\nMathematically\nSolve the following minimization problem:\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n \\widehat{u}_i^2, \\mbox{where} \\;\\; \\widehat{u}_i=y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestions\n\nWhy do we square the residuals, and then sum them up together? What’s gonna happen if you just sum up residuals?\nHow about taking the absolute value of residuals, and then sum them up?\n\n\n\nMinimization problem to solve\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nSteps\n\npartial differentiation of the objective function with respect to \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\nsolve for \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\n\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nFOC\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{align}\n\\frac{\\partial }{\\partial \\widehat{\\beta}_0}=& 2 \\sumn [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]=0 \\\\\\\\\n\\frac{\\partial }{\\partial \\widehat{\\beta}_1}=& 2 \\sumn x_i\\cdot [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]= \\sumn x_i\\cdot \\widehat{u}_i = 0\n\\end{align}\n\\]\nOLS estimators: analytical formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2},\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}, \\\\\\\\\n  \\mbox{where} & \\;\\; \\bar{y} = \\sumn y_i/n \\;\\; \\mbox{and} \\;\\;\\bar{x} = \\sumn x_i/n\n\\end{aligned}\n\\]\n\n\nEstimators\nSpecific  rules (formula)  to use once you get the data\n\nEstimates\nNumbers you get once you plug values (your data) into the formula"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "OLS demonstration in R",
    "text": "OLS demonstration in R\n\nR code: hard wayR code: a better waypost-estimation\n\n\nOLS Estimator Formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\]\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe can use the feols() function from the fixest package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLots of information is stored in the regression results (here, uni_reg), which is of class list.\nApply ls() to see its elements:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimated coefficients:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPredicted values at the observation points:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nResiduals:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can have a nice quick summary of the regression results with summary() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Once the model is estimated",
    "text": "Once the model is estimated\n\nEstimated modelPredicted values (R)New predictions (R)Exercise: The impact of lotsize\n\n\nModel to be estimated\n\\[\nprice = \\beta_0 + \\beta_1 lotsize + u\n\\]\n\nEstimated Model\nThis is the estimated version of the expected value of \\(y\\) conditional on \\(x\\).\n\\[\nprice =  3.4136\\times 10^{4} + 6.599 \\times lotsize\n\\]\nThis is called  sample regression function (SRF) , and it is an estimation of \\(E[price|lotsize]\\), the  population regression function (PRF).\n\n\n\nImportant\n\n\n\nOLS regression predicts the  expected  value of the dependent variable  conditional on the explanatory variables.\n\\(\\widehat{\\beta}_1\\) is an estimate of how a change in \\(x\\) affects the  expected  value of \\(y\\).\n\n\n\n\n\n\nYou can access the predicted values at the observed points by looking at the fitted.value element of the regression results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTo calculate the predicted value at arbitrary values of \\(x\\),\n\ncreate a new data.frame with values of \\(x\\) of your choice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\napply predict() to the data.frame using the regression results.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nProblemSolution\n\n\nYour current lot size is 3000. You are thinking of expanding your lot by 1000 (with everything else fixed), which would cost you 5,000 USD. Should you do it? Use R to figure it out.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "\\(R^2\\): Goodness of fit",
    "text": "\\(R^2\\): Goodness of fit\n\nWhat is it?Decompose \\(y\\)Visualization\\(R^2\\) componentsDefinition of \\(R^2\\)Caveat\n\n\n\\(R^2\\) is a measure of how good your model is in predicting the dependent variable (explaining variations in the dependent variable)  compared  to just using the average of the dependent variable as the predictor.\n\n\nYou can decompose observed value of \\(y\\) into two parts: fitted value and residual\n\\[\ny_i=\\widehat{y}_i +\\widehat{u}_i, \\;\\;\\mbox{where}\\;\\; \\widehat{y}_i = \\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i\n\\]\nnow, subtracting \\(\\bar{y}\\) (sample average of \\(y\\)),\n\\[\ny_i-\\bar{y}=\\widehat{y}_i-\\bar{y}+\\widehat{u}_i\n\\]\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\n\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ntotal sum of squares (SST)\n\\[\nSST\\equiv \\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\]\nexplained sum of squares (SSE) \\[\nSSE\\equiv \\sum_{i=1}^{n}(\\widehat{y}_i-\\bar{y})^2\n\\]\nresidual sum of squares (SSR) \\[\nSSR\\equiv \\sum_{i=1}^{n}\\widehat{u}_i^2\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\\(R^2 = 1 - SSR/SST\\)\n\n\n\n\n\nWhere did it come from?\n\\[\\begin{align}\n& SST = SSE + SSR  \\\\\n\\Rightarrow & SSE = SST - SSR \\\\\n\\Rightarrow & SSE/SST = 1 - SSR/SST = R^2\\\\\n\\end{align}\\]\nThe value of \\(R^2\\) always lies between \\(0\\) and \\(1\\) as long as an intercept is included in the econometric model.\n\nWhat does it measure?\n\\(R^2\\) is a measure of how much improvement  in predictin the depdent variable  you’ve made by including independent variable(s) \\((y=\\beta_0+\\beta_1 x+u)\\) compared to when simply using the mean of dependent variable as the predictor \\((y=\\beta_0+u)\\).\n\n\nImportant\n\n\\(R^2\\) tells  nothing  about how well you have estimated the causal ceteris paribus impact of \\(x\\) on \\(y\\) \\((\\beta_1)\\).\nAs an economist, we typically do not care about how well we can prefict yield, rather we care about how well we have predicted \\(\\beta\\).\n\nProblem\n\nWhile we observe the dependent variable (otherwise you cannot run regression), we cannot observe \\(\\beta_1\\).\nSo, we get to check how good estimated models are in predicting the dependent variable (which we do not care), but we can  never  test whether they have estimated \\(\\beta_1\\) well.\nThis means that we need to carefully examines whether the  assumptions  necessary for good estimation of \\(\\beta_1\\) is satisfied (next topic)."
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "What variables to include or not",
    "text": "What variables to include or not\nYou often\n\nface the decision of whether you should be including a particular variable or not:  how do you make a right decision? \nmiss a variable that you know is important because it is not simply available:  what are the consequences? \n\nTwo important concepts you need to be aware of:\n\nMulticollinearity\nOmitted Variable Bias"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Multicollinearity and Omitted Variable Bias",
    "text": "Multicollinearity and Omitted Variable Bias\n\nDefinitionObjectiveCases we look atKey consequences of interest\n\n\n\n\n\n\nDefinition: Multicollinearity\n\n\nA phenomenon where two or more variables are highly correlated (negatively or positively) with each other ( consequences? )\n\n\n\n\n\n\n\n\n\nDefinition: Omitted Variable Bias\n\n\nBias caused by not including (omitting)  important  variables in the model\n\n\n\n\n\n\nConsider the following model,\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\n\\]\nYour interest is in estimating the impact of \\(x_1\\) on \\(y\\).\n\n\n\n\nObjective\n\n\nUsing this simple model, we investigate what happens to the coefficient estimate on \\(x_1\\) if you include/omit \\(x_2\\).\n\n\n\n\n\n\nThe model: \\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\]\nCase 1:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 2:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is highly correlated with \\(x_1\\)?\nCase 3:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 4:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is highly correlated with \\(x_1\\)?\n\n\n\nIs \\(\\widehat{\\beta}_1\\) unbiased, that is \\(E[\\widehat{\\beta}_1]=\\beta_1\\)?\n\\(Var(\\widehat{\\beta}_1)\\)? (how accurate the estimation of \\(\\widehat{\\beta}_1\\) is)"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Case 1",
    "text": "Case 1\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\).)\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either of \\(x_2\\) and \\(u\\). So, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 on average because \\(cor(x_1, x_2)=0\\)\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nThey are the same because all the components are the same.\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\) and is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) will be the same as when you do not include \\(x_2\\) as a covariate (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)) and is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Case 2",
    "text": "Case 2\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because\n\n\\(x_1\\) is correlated with \\(x_2\\), but \\(\\beta_2 = 0\\).\n\\(x_1\\) is not correlated with \\(u\\)\n\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 &gt; 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\), but is highly correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is larger compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is highly correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Case 3",
    "text": "Case 3\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either \\(x_2\\) or \\(u\\).\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is (on average) zero because \\(x_1\\) and \\(x_2\\) are not correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is zero (on average).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include a variable that has some explanatory power beyond \\(x_1\\), but is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is smaller compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an variable that has some explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Case 4",
    "text": "Case 4\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nNo, because \\(x_1\\) is correlated with \\(x_2\\) and \\(\\beta_2 \\ne 0\\).\nSo, there will be bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 \\ne 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j) = \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nIt depends.\n\n\n\nIn the MC simulations we saw,\n\n\\(x_1\\) and \\(x_2\\) are highly correlated, so \\(R_j^2\\) is very high for \\(EE_2\\)\n\n\nx1 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\nx2 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\n\n\n\nThe impact of \\(x_2\\) (\\(\\beta_2 = 1\\)) and the variance of \\(x_2\\) is small (approximately 1).\n\n\ny &lt;- 1 + x1 + 1 * x2 + u\n\n\nThese conditions led to lower \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) compared to \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\).\n\n\nNow, let’s reverse the current conditions. We now have:\n\n\\(x_1\\) and \\(x_2\\) are NOT highly correlated, so \\(R_j^2\\) is small for \\(EE_2\\)\nThe impact of \\(x_2\\) (\\(\\beta_2 = 5\\)) and the variance of \\(x_2\\) is large (approximately 5).\n\n\nx1 &lt;- 0.9 * rnorm(N) + 0.1 * mu # independent variable\nx2 &lt;- 2.23 * rnorm(N) + 0.1 * mu # independent variable\ncor(x1, x2)\n\n\n\ny &lt;- 1 + x1 + 5 * x2 + u\n\n\nLet’s rerun MC simulations with this updated data generating process.\n\n\n\n\n\n\n\nThere exists bias-variance trade-off when independent variables are both important (their coefficients are non-zero) and they are correlated\nEconomists tend to opt for unbiasedness"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "title": "04-1: Omitted Variable Bias and Multicollinearity",
    "section": "Omitted Variable Bias (Theory)",
    "text": "Omitted Variable Bias (Theory)\n\nSetupMagnitude and direction of biasExamplesHow does this help?\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(EE_1\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\nLet \\(\\tilde{\\beta_1}\\) denote the estimator of \\(\\beta_1\\) from this model\n\n\\(EE_2\\) \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\nLet \\(\\widehat{\\beta}_1\\) and \\(\\widehat{\\beta}_2\\) denote the estimator of \\(\\beta_1\\) and \\(\\beta_2\\)\n\nRelationship between \\(x_1\\) and \\(x_2\\)\n\\(x_{1,i} = \\sigma_0 + \\sigma_1 x_{2,i} + \\mu_{i}\\)\n\n\n\n\nImportant\n\n\nThen, \\(E[\\tilde{\\beta_1}] = \\beta_1 + \\beta_2 \\cdot \\sigma_1\\), where \\(\\beta_2 \\cdot \\sigma_1\\) is the bias.\n\n\n\nThat is, if you omit \\(x_2\\) and regress \\(y\\) only on \\(x_1\\), then the bias is going to be the multiple of the impact of \\(x_2\\) on \\(y\\) (\\(\\beta_2\\)) and the impact of \\(x_2\\) on \\(x_1\\) (\\(\\sigma_1\\)).\n\n\nDirection of bias\n\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &gt; 0\\)\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &gt; 0\\)\n\n\nMagnitude of bias\n\nThe greater the correlation between \\(x_1\\) and \\(x_2\\), the greater the bias\nThe greater \\(\\beta_1\\) is, the greater the bias\n\n\n\n\nExample 1Example 2Example 3\n\n\n\\[\n\\begin{aligned}\n\\mbox{corn yield} = \\alpha + \\beta \\cdot N + (\\gamma \\cdot \\mbox{soil erodability}  + \\mu)\n\\end{aligned}\n\\]\n\nFamers tend to apply more nitrogen to the field that is more erodible to compensate for loss of nutrient due to erosion\nSoil erodability affects corn yield negatively \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe city planner placed incinerators in the outskirt of a city to avoid their potentially negative health effects\nDistance to city center has a negative impact on house price \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\\(\\mbox{groundwater use}\\): groundwater use by a farmer for irrigated production\n\\(\\mbox{center pivot}\\): 1 if center pivot is used, 0 if flood irrigation (less effective) is used\n\nFarmers who have relatively low precipitation during the growing season tend to adopt center pivot more\ncenter pivot applied water more efficiently than flood irrigation \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\n\n\n\nWhen the direction of the bias is the  opposite  of the expected coefficient on the variable of interest, you can claim that  even after  suffering from the bias, you are still seeing the impact of the variable interest. So, it is a strong evidence that you would have had an even stronger estimated impact.\n\nExample 1Example 2\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-5\\) ( you only observe this )\n\nYou believe the direction of bias is positive (you need provide reasoning behind your belief), and yet, the estimated coefficient is still negative. So, you can be quite confident that the sign of the impact of precipitation is negative. You can say your estimate is a conservative estimate of the impact of precipitation on groundwater use.\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(-5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-15\\) ( you only observe this )\n\nYou believe the direction of bias is negative, and the estimated coefficient is negative. So, unlike the case above, you cannot be confident that \\(\\widehat{\\beta}\\) would have been negative if it were not for the bias (by observing dist to city center and include it as a covariate). It is very much possible that the degree of bias is so large that the estimated coefficient turns negative even though the true sign of \\(\\beta\\) is positive. In this case, there is nothing you can do."
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#univariate-vs-multivariate-regression-models",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#univariate-vs-multivariate-regression-models",
    "title": "02-1: Multivariate Regression",
    "section": "Univariate vs Multivariate Regression Models",
    "text": "Univariate vs Multivariate Regression Models\n\nBi-variate vs. Uni-variateExampleExampleModel (general)\n\n\nUnivariate\nThe most important assumption \\(E[u|x] = 0\\) (zero conditional mean) is almost always violated (unless you data comes from randomized experiments) because all the other variables are sitting in the error term, which can be correlated with \\(x\\).\n\nMultivariate\nMore independent variables mean less factors left in the error term, which makes the endogeneity problem  less severe\n\n\nUni-variate vs. bi-variate\n\\[\\begin{align}\n  \\mbox{Uni-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + u_1 (=u_2+\\beta_2 exper)\\\\\n  \\mbox{Bi-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u_2\n\\end{align}\\]\n\nWhat’s different?\n\nuni-variate: \\(\\widehat{\\beta_1}\\) is biased unless experience is uncorrelated with education because experience was in error term\nbi-variate: able to measure the effect of education on wage,  holding experience fixed  because experience is modeled explicitly ( We say \\(exper\\) is controlled for. )\n\n\n\nThe impact of per student spending (expend) on standardized test score (avgscore) at the high school level\n\\[\\begin{align}\navgscore= & \\beta_0+\\beta_1 expend + u_1 (=u_2+\\beta_2 avginc) \\notag \\\\\navgscore= & \\beta_0+\\beta_1 expend +\\beta_2 avginc + u_2 \\notag\n\\end{align}\\]\n\n\nMore generally,\n\\[\\begin{align}\n  y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + u\n\\end{align}\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\\(\\beta_2\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "title": "02-1: Multivariate Regression",
    "section": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator",
    "text": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator\n\nUni-variate v.s. Bi-variateMean independence condition: example\n\n\nUni-variate\nFor \\(y = \\beta_0 + \\beta_1x + u\\),\n\\(E[u|x]=0\\)\n\nBi-variate\nFor \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\),\n\nMathematically: \\(E[u|x_1,x_2]=0\\)\nVerbally: for any values of \\(x_1\\) and \\(x_2\\), the expected value of the unobservables is zero\n\n\n\nIn the following wage model,\n\\[\\begin{align*}\nwage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\n\\end{align*}\\]\nMean independence condition is\n\\[\\begin{align}\n  E[u|educ,exper]=0\n\\end{align}\\]\nVerbally:\nThis condition would be satisfied if innate ability of students is on average unrelated to education level and experience."
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#the-model-with-k-independent-variables",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#the-model-with-k-independent-variables",
    "title": "02-1: Multivariate Regression",
    "section": "The model with \\(k\\) independent variables",
    "text": "The model with \\(k\\) independent variables\n\nGeneral modelImplementation (R)Present results(Math: derive OLS estimator)\n\n\nModel\n\\[\\begin{align}\n  y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\n\\end{align}\\]\nMean independence assumption?\n\\(\\beta_{OLS}\\) (OLS estimators of \\(\\beta\\)s) is unbiased if,\n\\[\\begin{align}\n    E[u|x_1,x_2,\\dots,x_k]=0\n\\end{align}\\]\nVerbally: this condition would be satisfied if the error term is uncorrelated wtih any of the independent variables, \\(x_1,x_2,\\dots,x_k\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhen you are asked to present regression results in assignments or your final paper, use the msummary() function from the modelsummary package.\n\n\n\n\nlibrary(modelsummary)\n\n#* run regression\nreg_results &lt;- feols(speed ~ dist, data = cars)\n\n#* report regression table\nmsummary(\n  reg_results,\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n)\n\n\n\n\n \n\n  \n    \n    \n    tinytable_ufikz959hk8vdpgjukdo\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                (1)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  (Intercept)\n                  8.284***\n                \n                \n                             \n                  (0.874) \n                \n                \n                  dist       \n                  0.166***\n                \n                \n                             \n                  (0.017) \n                \n                \n                  Num.Obs.   \n                  50      \n                \n                \n                  R2         \n                  0.651   \n                \n                \n                  RMSE       \n                  3.09    \n                \n                \n                  Std.Errors \n                  IID     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\n\n\n\n\n\nOLS\nFind the combination of \\(\\beta\\)s that minimizes the sum of squared residuals\n\nSo,\nDenoting the collection of \\(\\widehat{\\beta}\\)s as \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n    Min_{\\theta} \\sum_{i=1}^n \\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\widehat{\\beta_k} x_{k,i}) \\Big]^2\n\\end{align}\\]\nFind the FOCs by partially differentiating the objective function (sum of squared residuals) wrt each of \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n  \\sum_{i=1}^n(y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]\nOr more succinctly,\n\\[\\begin{align}\n  \\sum_{i=1}^n \\widehat{u}_i = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#unbiasedness-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#unbiasedness-of-ols-estimators",
    "title": "02-1: Multivariate Regression",
    "section": "Unbiasedness of OLS Estimators",
    "text": "Unbiasedness of OLS Estimators\n\nConditions (four)Perfect CollinearityEndogeneity (Definition)\n\n\n\n\n\nImportant\n\n\nOLS estimators of multivariate models if the following conditions are satisfied.\n\n\n\n\nCondition 1\nYour model is correct (Assumption \\(MLR.1\\))\n\nCondition 2\nRandom sampling (Assumption \\(MLR.2\\))\n\nConditions 3\nNo perfect collinearity (Assumption \\(MLR.3\\))\n\nConditions 4\nZero Conditional Mean (Assumption \\(MLR.4\\))\n\\[\\begin{align}\n  E[u|x_1,x_2,\\dots,x_k]=0 \\;\\;\\mbox{(Assumption MLR.4)}\n\\end{align}\\]\nIf all the conditions \\(MLR.1\\sim MLR.4\\) are satisfied, OLS estimators are unbiased.\n\n\nNo Perfect Collinearity (\\(MLR.3\\))\nAny variable cannot be a linear function of the other variables\n\nExample (silly)\n\\[\\begin{align}\n  wage = \\beta_0 + \\beta_1 educ + \\beta_2 (3\\times educ) + u\n\\end{align}\\]\n( More on this later when we talk about dummy variables)\n\n\n\n\n\n\nEndogeneity: Definition\n\n\n\\[\nE[u|x_1,x_2,\\dots,x_k] = f(x_1,x_2,\\dots,x_k) \\ne 0\n\\]\n\n\n\n\n\nWhat could cause endogeneity problem?\n\nfunctional form misspecification\n\n\\[\\begin{align}\n  wage = & \\beta_0 + \\beta_1 log(x_1) + \\beta_2 x_2 + u_1 \\;\\;\\mbox{(true)}\\\\\n  wage = & \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u_2 (=log(x_1)-x_1) \\;\\; \\mbox{(yours)}\n\\end{align}\\]\n\nomission of variables that are correlated with any of \\(x_1,x_2,\\dots,x_k\\) ( more on this soon )\n other sources of enfogeneity later"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#variance-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#variance-of-ols-estimators",
    "title": "02-1: Multivariate Regression",
    "section": "Variance of OLS estimators",
    "text": "Variance of OLS estimators\n\nHomoskedasticity and Variance of \\(\\widehat{\\beta}_{OLS}\\)Estimating \\(\\sigma^2\\)Estimator of \\(Var{\\widehat{\\beta}_j}\\)\n\n\nCondition 5\nError term is homoeskedastic (Assumption \\(MLR.5\\))\n\\[\\begin{align}\nVar(u|x_1,\\dots,x_k)=\\sigma^2\n\\end{align}\\]\n\nUnder conditions \\(MLR.1\\) through \\(MLR.5\\), conditional on the sample values of the independent variables,\n\n\n\n\nVariance of \\(\\widehat{\\beta}_{OLS}\\)\n\n\n\\[\\begin{align}\n    Var(\\widehat{\\beta}_j)= \\frac{\\sigma^2}{SST_j(1-R^2_j)},\n\\end{align}\\]\n\n\n\n\nwhere\n\n\\(SST_j= \\sum_{i=1}^n (x_{ji}-\\bar{x_j})^2\\)\n\\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables including an intercept. ( We will revisit this equation)\n\n\n\nJust like uni-variate regression, you need to estimate \\(\\sigma^2\\) if you want to estimate the variance (and standard deviation) of the OLS estimators.\nuni-variate regression\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-2}\n\\end{align}\\]\nmulti-variate regression\nA model with \\(k\\) independent variables with intercept.\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-(k+1)}\n\\end{align}\\]\nYou solved \\(k+1\\) simultaneous equations to get \\(\\widehat{\\beta}_j\\) \\((j=0,\\dots,k)\\). So, once you know the value of \\(n-k-1\\) of the residuals, you know the rest.\n\n\nUsing the estimator of \\(\\sigma^2\\) in place of \\(\\sigma^2\\), we have the  estimator  of the variance of the OLS estimator.\n\n\n\n\nEstimator of the variance of the OLS estimator\n\n\n\\[\\begin{align}\n\\widehat{Var{\\widehat{\\beta}_j}} = \\frac{\\widehat{\\sigma}^2}{SST_j(1-R^2_j)} = \\left(\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-k}\\right) \\cdot \\frac{1}{SST_j(1-R^2_j)}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#frischwaughlovell-theorem",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#frischwaughlovell-theorem",
    "title": "02-1: Multivariate Regression",
    "section": "Frisch–Waugh–Lovell Theorem",
    "text": "Frisch–Waugh–Lovell Theorem\nConsider the following simple model,\n\\[\\begin{align}\n  y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + u_i\n\\end{align}\\]\nSuppose you are interested in estimating only \\(\\beta_1\\).\nLet’s consider the following two methods,\n\nMethod 1: Regular OLS\nRegress \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\) with an intercept to estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) at the same time (just like you normally do)\n\nMethod 2: 3-step\n\nregress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\nregress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\nregress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\) \\((\\widehat{u}_y=\\alpha_1 \\widehat{u}_{x_1}+v_3)\\)\n\nFrisch-Waugh–Lovell theorem\nMethods 1 and 2 produces the same coefficient estimate on \\(x_1\\)\n\\[\\widehat{\\beta_1} = \\widehat{\\alpha_1}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#partialing-out-interpretation-from-method-2",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#partialing-out-interpretation-from-method-2",
    "title": "02-1: Multivariate Regression",
    "section": "Partialing out Interpretation from Method 2",
    "text": "Partialing out Interpretation from Method 2\nStep 1\nRegress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\n\n\\(\\widehat{u}_y\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(y\\)\n\nStep 2\nRegress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\n\n\\(\\widehat{u}_{x_1}\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(x_1\\)\n\nStep 3\nRegress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\), which produces an estimte of \\(\\beta_1\\) that is identical to that you can get from regressin \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\)"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#interpretation",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#interpretation",
    "title": "02-1: Multivariate Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nRegressing \\(y\\) on all explanatory variables \\((x_1\\), \\(x_2\\), and \\(x_3)\\) in a multivariate regression is as if you are looking at the impact of a single explanatory variable with the effects of all the other effects partiled out\nIn other words, including variables beyond your variable of interest lets you  control for (remove the effect of)  other variables, avoiding confusing the impact of the variable of interest with the impact of other variables."
  }
]