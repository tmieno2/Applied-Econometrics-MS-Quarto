[
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#functional-form-1",
    "href": "lectures/07-econometric-model/07-modeling.html#functional-form-1",
    "title": "07: Econometric Modeling",
    "section": "Functional form",
    "text": "Functional form\n\nIntroductionLinear or Non-linear?Why bother?\n\n\n\nTransformation of variables is allowed without disturbing our analytical framework as long as the model is linear in  parameter .\nTransformation of variables change the interpretation of the coefficients estimates\n\n\n\n\n\nExample models\n\n\n\n\nlog-linear\n\\(log(y_i)= \\beta_0+\\beta_1 x_i + u_i\\)\n\nlinear-log\n\\(y_i= \\beta_0+\\beta_1 log(x_i) + u_i\\)\n\nlog-log\n\\(log(y_i)= \\beta_0+\\beta_1 log(x_i) + u_i\\)\n\nquadratic\n\\(y_i= \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + u_i\\)\n\n\n\n\n\n\n\n\n\n\nIn the models we just saw, the dependent variable and independent variable are non-linearly related, how come are these models called simple  linear  model?\n “linear”  in simple  linear  model means that the model is linear in  parameter , but not in  variable \n\nExamples: Non-linear models\n\\[\\begin{align*}\n  y_i=\\beta_0+x_i^{\\beta_1}+u_i \\\\\n  y_i=\\frac{x_i}{\\beta_0+\\beta_1 x_i}+u_i\n\\end{align*}\\]\n\n\n\n\nNote\n\n\nTransformation of the dependent and independent variables would not affect the properties of the OLS estimator as long as the model is linear in parameter.\n\n\n\n\n\nConsider a following model:\n\\[\\begin{align*}\n\\mbox{corn yield} = \\beta_0 + \\beta_1 \\cdot \\mbox{fertilizer} + \\mu\n\\end{align*}\\]\nQuestion\nWhat is wrong with this model?\n\n\n\nAnswer\n\nIt assumes that fertilizer affects corn yield the same way no matter how much fertilizer you apply. It does not reflect the reality that the impact of fertilizer becomes almost zero at some point. A better model?"
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#various-functional-forms",
    "href": "lectures/07-econometric-model/07-modeling.html#various-functional-forms",
    "title": "07: Econometric Modeling",
    "section": "Various functional forms",
    "text": "Various functional forms\n\nLog-linearLinear-logLog-logQuadratic\n\n\n\nBasicsVisualizationExampleEstimated model\n\n\nModel\n\\[\\begin{align}\n  log(y_i)= \\beta_0+\\beta_1 x_i + u_i \\notag\n\\end{align}\\]\nCalculus\nDifferentiating the both sides wrt \\(x_i\\),\n\\[\\begin{align}\n  \\frac{1}{y_i}\\cdot\\frac{\\partial y_i}{\\partial x_i} = \\beta_1 \\Rightarrow \\frac{\\Delta y_i}{y_i} = \\beta_1 \\Delta x_i \\notag\n\\end{align}\\]\nInterpretation\n\\(\\beta_1\\) measures a percentage change in \\(y_i\\) (once multiplied by 100) when \\(x_i\\) is increased by one unit\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n    log(wage)=\\beta_0 + \\beta_1 educ + u \\notag\n\\end{align}\\]\nCalculus\nDifferentiating both sides with respect to \\(educ\\),\n\\[\\begin{align}\n    \\frac{1}{wage} \\frac{\\partial wage}{\\partial educ} = \\beta_1 \\Rightarrow \\frac{\\Delta wage}{wage} = \\beta_1\\Delta educ\\notag\n\\end{align}\\]\nInterpretation\nIf education increases by 1 year \\((\\Delta educ=1)\\), then wage increases by \\(\\beta_1*100\\%\\) \\((\\frac{\\Delta wage}{wage}=\\beta_1)\\)\n\n\n\n\nWhen you estimate the following model using the wage dataset:\n\\[log(wage)=\\beta_0 + \\beta_1 educ + u \\notag\\]\n\nThen, the estimated equation is the following:\n\\[\\begin{align}\n  \\widehat{log(wage)}=0.584+0.083 educ \\notag\n\\end{align}\\]\n\\[\\begin{align}\n  E[\\widehat{wage}]=e^{0.584+0.083 educ}\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBasicsVisualization\n\n\nModel\n\\[\\begin{align}\n  y_i= \\beta_0+\\beta_1 log(x_i) +u_i \\notag\n\\end{align}\\]\nCalculus\nDifferentiating the both sides wrt \\(x_i\\),\n\\[\\begin{align}\n  \\frac{\\partial y_i}{\\partial x_i} = \\frac{\\beta_1}{x_i} \\Rightarrow \\Delta y_i = \\beta_1\\frac{\\Delta x_i}{x_i} \\notag\n\\end{align}\\]\nInterpretation\nWhen \\(x\\) increases by 0.01 (\\(1\\%\\)) \\(y\\) increases by \\(\\beta_1 \\times 0.01\\).\n\n\n\\[y = \\beta_0 + \\beta_1 log(x) = 1 + 2 \\times log(x)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  log(y_i)= \\beta_0+\\beta_1 log(x_i) +u_i \\notag\n\\end{align}\\]\nCalculus\nDifferentiating the both sides wrt \\(x_i\\),\n\\[\\begin{align}\n  \\frac{\\partial y_i}{y_i}/\\frac{\\partial x_i}{x_i} = \\beta_1 \\Rightarrow \\frac{\\Delta y_i}{y_i} = \\beta_1 \\frac{\\Delta x_i}{x_i}\\notag\n\\end{align}\\]\nInterpretation\nA  percentage  change in \\(x\\) would result in a \\(\\beta_1\\)  percentage  change in \\(y_i\\) (constant elasticity)\n\n\n\nBasicsVisualizationExampleImplementation in REstimated ModelMarginal impact of \\(educ\\)\n\n\nModel\n\\(y_i= \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + u_i\\)\n\nCalculus\nDifferentiating the both sides wrt \\(x_i\\),\n\\(\\frac{\\partial y_i}{\\partial x_i} = \\beta_1 + 2*\\beta_2 x_i\\Rightarrow  \\Delta y_i = (\\beta_1 + 2*\\beta_2 x_i)\\Delta x_i\\)\n\nInterpretation\nWhen \\(x\\) increases by 1 unit \\((\\Delta x_i=1)\\), \\(y\\) increases by \\(\\beta_1 + 2*\\beta_2 x_i\\)\n\n\nQuadratic functional form is quite flexible.\n\n\n\\(y = x + x^2\\) \\((\\beta_1 = 1, \\beta_2 = 1)\\)\n\n\n\n\n\n\n\n\n\n\n\\(y = 3x-2x^2\\) \\((\\beta_1 = 3, \\beta_2 = -2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation impacts of income\nThe marginal impact of education (the impact of a small change in education on income) may differ what level of education you have had:\n\nHow much does it help to have two more years of education when you have had education until elementary school?\nHow much does it help to have two more years of education when you have graduated a college?\nHow much does it help to spend two more years as a Ph.D student if you have already spent six years in a Ph.D program\n\n\nObservation\nThe marginal impact of education does not seem to be linear.\n\n\nWhen you want to include a variable that is a transformation of an existing variable, you can use I() function in which you write the mathematical expression of the desired transformation.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEstimated Model\n\\(wage = 5.60 - 2.12\\times female -0.416\\times educ + 0.039\\times educ^2\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nAccording to the estimated model, the marginal impact of \\(educ\\) is:\n\\(\\frac{\\partial wage}{\\partial educ} = -0.416+0.039\\times 2\\times educ\\)\n\nWhen \\(educ = 4\\), additional year of education is going to increase hourly wage by -0.104 on average\nWhen \\(educ = 10\\), additional year of education is going to increase hourly wage by 0.364 on average"
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#statistical-significance-of-the-marginal-impact-1",
    "href": "lectures/07-econometric-model/07-modeling.html#statistical-significance-of-the-marginal-impact-1",
    "title": "07: Econometric Modeling",
    "section": "Statistical significance of the marginal impact",
    "text": "Statistical significance of the marginal impact\n\nMarginal impcatIn the linear caseThe quadratic case\n\n\nLet’s work with the income model, in which the marginal impact of \\(educ\\) is:\n\\[\\begin{align*}\n\\frac{\\partial wage}{\\partial educ} = -0.416+0.039\\times 2\\times educ\n\\end{align*}\\]\n\n\\(\\beta_{educ}\\): \\(-0.416\\) \\((t\\)-stat \\(= -1.80)\\)\n\\(\\beta_{educ^2}\\): \\(0.039\\) \\((t\\)-stat \\(= 4.10)\\)\n\n\nQuestion\nSo, is the marginal impact of \\(educ\\) statistically significantly different from \\(0\\)?\n\n\n\nModel and marginal impactTesting?\n\n\nRegression\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimated model\n\\(wage = 0.62+0.51 \\times educ\\)\n\n\nQuestion 1Question 2\n\n\nWhat is the marginal impact of \\(educ\\)?\n\n\n\nAnswer\n\n0.51\n\n\n\nDoes the marginal impact of education vary depending on the level of education?\n\n\n\nAnswer\n\nNo, the model we estimated assumed that the marginal impact of education is constant.\n\n\n\n\n\n\n\nYou can just test if \\(\\hat{\\beta}_{educ}\\) (the marginal impact of education) is statistically significantly different from \\(0\\), which is just a t-test.\n\n\n\n\n\n\n\nIssueExample 1R implementationExample 2R implementation\n\n\nWith the quadratic specification\n\nThe marginal impact of education varies depending on your education level\nThere is no single test that tells you whether the marginal impact of education is statistically significant universally\nIndeed, you need different tests for different values education levels\n\n\n\nMarginal impact of education\n\\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times educ\\)\n\nHypothesis testing\nDoes additional year of education has a statistically significant impact (positive or negative) if your current education level is 4?\n\n\\(H_0\\): \\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 4 =0\\)\n\\(H_1\\): \\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 4 \\ne 0\\)\n\n\nQuestion\nIs this\n\ntest of a single coefficient? (t-test)\ntest of a single equation with multiple coefficients? (t-test)\ntest of multiples equations with multiple coefficients? (F-test)\n\nt-statistic\n\\(t = \\frac{\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 4}{se(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 4)} = \\frac{\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 8}{se(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 8)}\\)\n\n\nRemember, a trick to do this test using R is take advantage of the fact that \\(F_{1, n-k-1} \\sim t_{n-k-1}^2\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince the p-value is 0.529, we do not reject the null.\n\n\nMarginal impact of education\n\\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times educ\\)\n\nHypothesis testing\nDoes additional year of education has a statistically significant impact (positive or negative) if your current education level is 10?\n\n\\(H_0\\): \\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 10 =0\\)\n\\(H_1\\): \\(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 10 \\ne 0\\)\n\nQuestion\nIs this\n\ntest of a single coefficient? (t-test)\ntest of a single equation with multiple coefficients? (t-test)\ntest of multiples equations with multiple coefficients? (F-test)\n\nt-statistic\n\\(t = \\frac{\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 10}{se(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 2 \\times 10)} = \\frac{\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 20}{se(\\hat{\\beta}_{educ} + \\hat{\\beta}_{educ^2} \\times 20)}\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSince the much lower than is 0.01, we can reject the null at the 1% level."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#interaction-terms-1",
    "href": "lectures/07-econometric-model/07-modeling.html#interaction-terms-1",
    "title": "07: Econometric Modeling",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nWhat is it?Marginal impactR implementationEstimated modelHypothesis testing\n\n\nA variable that is a multiplication of two variables\n\nExample\n\\(educ\\times exper\\)\n\nA model with an interaction term\n\\(wage = \\beta_0 + \\beta_1 exper + \\beta_2 educ \\times exper + u\\)\n\n\n\nMarginal impact of education:\n\\(\\frac{\\partial wage}{\\partial exper} = \\beta_1+\\beta_2\\times educ\\)\n\nImplications\nThe marginal impact of experience depends on education\n\n\\(\\beta_1\\): the marginal impact of experience when \\(educ=0\\)\nif \\(\\beta_2&gt;0\\): additional year of experience is worth more when you have more years of education\n\n\n\nJust like the quadratic case with \\(educ^2\\), you can use I().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEstimated Model\n\\(wage = 6.121 - 2.418 \\times female - 0.188 \\times exper + 0.020 \\times educ \\times exper\\)\n\nMarginal impact of experience\n\\(\\frac{\\partial wage}{\\partial exper} = - 0.188 + 0.020 \\times educ\\)\n\n\nMarginal impact of \\(exper\\):\n\n\n\n\n\n\n\n\n\n\nHistogram of education:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesisR implementation\n\n\nJust like the case of the quadratic specification of education, marginal impact of experience is not constant\nWe can test if the marginal impact of experience is statistically significant for a given level of education\n\nWhen \\(educ=10\\), \\(\\frac{\\partial wage}{\\partial exper} = - 0.188 + 0.020 \\times 10=0.012\\)\nWhen \\(educ=15\\), \\(\\frac{\\partial wage}{\\partial exper} = - 0.188 + 0.020 \\times 15=0.112\\)\n\nQuestion\nDoes additional year of experience has a statistically significant impact (positive or negative) if your current education level is 10\n\nHypothesis\n\n\\(H_0\\): \\(\\hat{\\beta}_{exper} + \\hat{\\beta}_{exper\\_educ} \\times 10=0\\)\n\\(H_1\\): \\(\\hat{\\beta}_{exper} + \\hat{\\beta}_{exper\\_educ} \\times 10=0\\)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#including-qualitative-information-1",
    "href": "lectures/07-econometric-model/07-modeling.html#including-qualitative-information-1",
    "title": "07: Econometric Modeling",
    "section": "Including qualitative information",
    "text": "Including qualitative information\n\nIssueBinary variablesUse male instead?\n\n\nIssue\nHow do we include qualitative information as an independent variable?\n\nExamples\n\nmale or female (binary)\nmarried or single (binary)\nhigh-school, college, masters, or Ph.D (more than two states)\n\n\n\n\nDummy variableModel with dummy a variableR implementationVisualization\n\n\n\n\n\n\nDummy variable\n\n\n\nRelevant information in binary variables can be captured by a .red[zero-one] variable that takes the value of \\(1\\) for one state and \\(0\\) for the other state\nWe use “dummy variable” to refer to a binary (zero-one) variable\n\n\n\n\n\n\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nModel\n\\(wage = \\beta_0 +\\sigma_f female +\\beta_2 educ + u\\)\n\nInterpretation\n\nfemale: \\(E[wage|female=1,educ] = \\beta_0 + \\sigma_f +\\beta_2 educ\\)\nmale: \\(E[wage|female=0,educ] = \\beta_0 + \\beta_2 educ\\)\n\n\nThis means that\n\\(\\sigma_f = E[wage|female=1,educ]-E[wage|female=0,educ]\\)\nVerbally,\n\n\\(\\sigma_f\\) is the difference in the expected wage conditional on education between female and male\n\\(\\sigma_f\\) measures how much more (less) female workers make compared to male workers ( baseline ) if they were to have the same education level\n\n\n\nR implementation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpretation\nFemale workers make -2.2733619 ($/hour) less than male workers on average even though they have the same education level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel and theoryRegression resultsQuestionPerfect Collinearity\n\n\nModel\n\\(wage = \\beta_0 +\\sigma_m male +\\beta_2 educ + u\\)\n\nInterpretation\n\nmale: \\(E[wage|male = 1,educ] = \\beta_0 + \\sigma_m +\\beta_2 educ\\)\nfemale: \\(E[wage|male = 0,educ] = \\beta_0 + \\beta_2 educ\\)\n\n\nThis means that\n\\(\\sigma_m = E[wage|male=1,educ]-E[wage|male=0,educ]\\)\nVerbally,\n\n\\(\\sigma_m\\) is the difference in the expected wage conditional on education between female and male\n\\(\\sigma_m\\) measures how much more (less) male workers make compared to female workers (.blue[baseline]) if they were to have the same education level\n\n\n\n\nImportant\n\n\nWhichever status that is given the value of \\(0\\) becomes the baseline\n\n\n\n\n\nRegression results\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nInterpretation\nMale workers make NA ($/hour) more than female workers on average even though they have the same education level.\n\n\nWhat do you think will happen if we include both male and female dummy variables?\n\n\n\nAnswer\n\n\nThey contain redundant information\nIndeed, including both of them along with the intercept would cause .blue[perfect collinearity problem]\nSo, you  need to drop either one of them\n\n\n\n\nIn the model, \\(intercept = male + female\\), which causes perfec collinearity.\nHere is what happens if you include both:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOne of the variables that cause perfect collinearity is automatically dropped."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#interactions-with-a-dummy-variable-1",
    "href": "lectures/07-econometric-model/07-modeling.html#interactions-with-a-dummy-variable-1",
    "title": "07: Econometric Modeling",
    "section": "Interactions with a dummy variable",
    "text": "Interactions with a dummy variable\n\nIssueExample using RInterpretation\n\n\n\nIn the previous example, the impact of education on wage was modeled to be exactly the same\nCan we build a more flexible model that allows us to estimate the differential impacts of education on wage between male and female?\n\nA more flexible model\n\\(wage = \\beta_0 + \\sigma_f female +\\beta_2 educ + \\gamma female\\times educ + u\\)\n\nfemale: \\(E[wage|female=1,educ] = \\beta_0 + \\sigma_f +(\\beta_2+\\gamma) educ\\)\nmale: \\(E[wage|female=0,educ] = \\beta_0 + \\beta_2 educ\\)\n\n\nInterpretation\nFor female, education is more effective by \\(\\gamma\\) than it is for male.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nThe marginal benefit of education is 0.086 ($/hour) less for females workers than for male workers on average."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#categorical-variable-more-than-two-states",
    "href": "lectures/07-econometric-model/07-modeling.html#categorical-variable-more-than-two-states",
    "title": "07: Econometric Modeling",
    "section": "Categorical variable: more than two states",
    "text": "Categorical variable: more than two states\n\nIssueModel\n\n\n\nConsider a variable called \\(degree\\) which has three status values: college, master, and doctor.\nUnlike a binary variable, there are three status values.\nHow do we include a categorical variable like this in a model?\n\nWhat do we do about this?\nYou can create three dummy variables likes below:\n\ncollege: 1 if the highest degree is college, 0 otherwise\nmaster: 1 if the highest degree is Master’s, 0 otherwise\ndoctor: 1 if the highest degree is Ph.D., 0 otherwise\n\nYou then include two (the number of status values - 1) of the three dummy variables:\n\n\nModel\n\\(wage = \\beta_0 + \\sigma_m master +\\sigma_d doctor + \\beta_1 educ + u\\)\n\n\\(college\\): \\(E[wage|master=0, doctor = 0, educ] = \\beta_0 + \\beta_1 educ\\)\n\\(master\\): \\(E[wage|master=1, doctor = 0, educ] = \\beta_0 + \\sigma_m + \\beta_1 educ\\)\n\\(doctor\\): \\(E[wage|master=0, doctor = 1, educ] = \\beta_0 + \\sigma_d + \\beta_1 educ\\)\n\n\nInterpretation\n\\(\\sigma_m\\): the impact of having a MS degree .red[relative to] having a .red[college degree]\n\\(\\sigma_d\\): the impact of having a Ph.D. degree .red[relative to] having a .red[college degree]\n\nImportant\nThe omitted category (here, college) becomes the baseline."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#structural-differences-across-groups",
    "href": "lectures/07-econometric-model/07-modeling.html#structural-differences-across-groups",
    "title": "07: Econometric Modeling",
    "section": "Structural differences across groups",
    "text": "Structural differences across groups\n\nDefinitionWhat to do?InterpretationR codeWhat do you see?Test of joint significance\n\n\nStructural difference refers to the fundamental differences in the model of a phenomenon in the population:\n\nExample\nMale: \\(cumgpa = \\alpha_0 + \\alpha_1 sat + \\alpha_2 hsperc + \\alpha_3 tothrs + u\\)\nFemale: \\(cumgpa = \\beta_0 + \\beta_1 sat + \\beta_2 hsperc + \\beta_3 tothrs + u\\)\n\n\\(cumgpa\\): college grade points averages for male and female college athletes\n\\(sat\\): SAT score\n\\(hsperc\\): high school rank percentile\n\\(tothrs\\): total hours of college courses\n\n\nIn this example,\n\\(cumgpa\\) are determined in a fundamentally different manner between female and male students.\nYou do not want to run a single regression that fits a single model for both female and male students.\n\n\nIf you suspect that the underlying process of how the dependent variable is determined vary across groups, then you should test that hypothesis!\n\nTo do so,\nYou estimate the model that allows to estimate separate models across groups within a single regression analysis.\n\nA more flexible model\n\\[cumgpa = \\beta_0 + \\sigma_0 female + \\beta_1 sat + \\sigma_1 (sat \\times female)\\] \\[\\;\\; + \\beta_2 hsperc + \\sigma_2 (hsperc \\times female)\\] \\[\\qquad + \\beta_3 tothrs + \\sigma_3 (tothrs \\times female) + u\\]\n\n\nMale: \\(E[cumgpa] = \\beta_0 + \\beta_1 sat + \\beta_2 hsperc + \\beta_3 tothrs\\) Female: \\(E[cumgpa] = (\\beta_0 +\\sigma_0) + (\\beta_1+\\sigma_1) sat + (\\beta_2+\\sigma_2) hsperc + (\\beta_3+\\sigma_3) tothrs\\)\n\nInterpretation\n\n\\(\\beta\\)s are commonly shared by female and male students\n\\(\\sigma\\)s capture the differences between female and male students\n\nNull Hypothesis\n\n(verbally) The model of GPA for male and female students are not structurally different.\n(mathematically) \\(H_0: \\;\\; \\sigma_0=0,\\;\\; \\sigma_1=0, \\;\\; \\sigma_2=0, \\;\\; \\mbox{and} \\;\\; \\sigma_3=0\\)\n\nQuestion\nWhat test do we do? t-test or F-test?\n\n\n\nAnswer\n\nF-test.\n\n\n\nRun the unrestricted model with all the interaction terms:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nRegression results\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWhat do you see?\n\nNone of the variables that involve \\(female\\) are statistically significant at the 5% level individually.\nDoes this mean that \\(male\\) and \\(female\\) students have the same regression function?\nNo, we are testing the joint significance of the coefficients. We need to do an \\(F\\)-test!\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#r-coding-tips-categorical-variables-and-interaction-terms",
    "href": "lectures/07-econometric-model/07-modeling.html#r-coding-tips-categorical-variables-and-interaction-terms",
    "title": "07: Econometric Modeling",
    "section": "R coding tips: categorical variables and interaction terms",
    "text": "R coding tips: categorical variables and interaction terms\n\nPrepare a datasetDatai()Interactions terms\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTake a look at the data,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nYou can use the i() function inside fixest::feols() like below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nref = \"Indiana U\" sets the base category to \"Indiana U\".\nSo, for example, the highlighted line means that faculty members at Michigan State U make \\(9,118\\) USD less annually than those at Indiana U.\n\nKey\nYou do not have to make bunch of dummy variables like the original dataset. Just use i(catergory_variable).\n\n\nYou can use i() for creating interactions of a categorical variable and a continuous variable.\nSuppose you are interested in understanding the impact of pubindx (continuous) by university (categorical), then\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSo, the marginal impact of pubindex is \\(436\\) greater for those at Michigan State U than those at Indiana U."
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#goodness-of-fit-r2",
    "href": "lectures/07-econometric-model/07-modeling.html#goodness-of-fit-r2",
    "title": "07: Econometric Modeling",
    "section": "Goodness of fit: \\(R^2\\)",
    "text": "Goodness of fit: \\(R^2\\)\n\n\\(R^2\\) does not matter (often)Why?Regression results\n\n\n\n\n\nImportant\n\n\nSmall value of \\(R^2\\) does not mean the end of the world (In fact, we could not care less about it.)\n\n\n\n\n\nExample\n\\[ecolabs = \\beta_0 + \\beta_1 regprc + \\beta_2 ecoprc\\]\n\n\\(ecolabs\\): the (hypothetical) pounds of ecologically friendly (eco-labeled) apples a family would demand\n\\(regprc\\): prices of regular apples\n\\(ecoprc\\): prices of the hypothetical eco-labeled apples\n\n\nKey\n\nThe data was obtained via survey and \\(ecoprc\\) was set randomly (So, we know \\(E[u|x] = 0\\)) by the researcher.\nThe (only) objective of the study is to understand the impact of the price of eco-labeled apple on the demand for eco-labeled apples.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nQuestion\n\n\nNote that \\(R^2\\) is very small. Is this a problem?\n\n\n\n\n\n\n\nAnswer\n\nNo.\n\nTheir goal is not predicting the demand of eco-labeled apple. Understanding the  causal  impact of price on demand.\nThe goal is achieved via randomization of the price variables at the stage of designing the survey!"
  },
  {
    "objectID": "lectures/07-econometric-model/07-modeling.html#scaling",
    "href": "lectures/07-econometric-model/07-modeling.html#scaling",
    "title": "07: Econometric Modeling",
    "section": "Scaling",
    "text": "Scaling\n\nQuestionsLet’s seeInterpretationSummary\n\n\nWhat happens if you scale up/down variables used in regression?\n\ncoefficients\nstandard errors\nt-statistics\n\\(R^2\\)\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nSo,\n\n\n\ncoefficient: 1/12\nstandard error: 1/12\nt-stat: the same\n\\(R^2\\): the same\n\n\n\n\n\n\n\nInterpretation\n\nRegression  without  scaling\n\nhourly wage increases by \\(0.506\\) if education increases by a  year \n\nRegression  with  scaling (e.g., 48 means 4 years)\n\nhourly wage increases by \\(0.0422\\) if education increases by a  month \n\nNote\nAccording to the scaled model, hourly wage increases by \\(0.0422 * 12\\) if education increases by a year (12 months).\nThat is, the estimated marginal impact of education on wage from the scaled model is the same as that from the non-scaled model.\n\n\nWhen an independent variable is scaled,\n\nits coefficient estimate and standard error are going to be scaled up/back to the exact degree the variable is scaled up/back\nt-statistics stays the same (as it should be)\n\\(R^2\\) stays the same (the model does not improve by simply scaling independent variables)\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#population-sample-and-econometrics",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Population, Sample, and Econometrics",
    "text": "Population, Sample, and Econometrics\n\nPopulationSample\n\n\n\n\n\n\nDefinition\n\n\nA set of \\(ALL\\) individuals, items, phenomenon, that you are interested in learning about\n\n\n\n\n\nExample\n\nSuppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.\nSuppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.\n\nImportant\nPopulation differs depending on the scope of your interest\n\nIf you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world\nIf you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.\n\n\n\n\n\n\n\nDefinition\n\n\nSample is a subset of population that you observe\n\n\n\n\n\nCase 1Case 2\n\n\n\nPopulation: you are interested in the impact of education on wage\nSample (example): data on education, income, and many other things for 300 individuals from each State\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?\n\n\n\n\n\n\n\nPopulation: you are interested in the impact of water price on irrigation by farmers in Nebraska\nSample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#simple-univariate-model",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Simple univariate model",
    "text": "Simple univariate model\n\nWhat is it?What does \\(\\beta_1\\) measure?What does \\(\\beta_0\\) measure?Visualized\n\n\nConsider a phenomenon in the population that is correctly represented by the following model ( This is the model you want to learn about using sample ):\n\\[\\begin{equation}\ny=\\beta_0+\\beta_1 x + u\n\\end{equation}\\]\n\n\\(y\\): to be explained by \\(x\\) ( dependent variable)\n\\(x\\): explain \\(y\\) ( independent variable ,  covariate ,  explanatory variable )\n\\(u\\): parts of \\(y\\) that cannot be explained by \\(x\\) ( error term )\n\\(\\beta_0\\) and \\(\\beta_1\\): real numbers that gives the model a quantitative meaning ( parameters )\n\n\n\n\n\nImportant\n\n\nYou will never know the true model. You can try estimating it using sample! That is what statistics is about.\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nIf you change \\(x\\) by \\(1\\) unit while holding \\(u\\) (everything else) constant,\n\\[\\begin{align}\n  y_{before} & = \\beta_0+\\beta_1 x + u \\\\\n  y_{after} & = \\beta_0+\\beta_1 (x + 1) + u\n\\end{align}\\]\nThe difference in \\(y_{before}\\) and \\(y_{after}\\),\n\\[\\begin{align}\n  \\Delta y = \\beta_1\n\\end{align}\\]\nThat is, \\(y\\) changes by \\(\\beta_1\\).\n\n\n\n\n\nSo,\n\n\n\n\\(\\beta_1\\) is the change in \\(y\\) when \\(x\\) increases by 1\nWe call \\(\\beta_1\\) the  ceteris paribus  (with everything else fixed) causal impact of \\(x\\) on \\(y\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nWhen \\(x = 0\\) and \\(u=0\\),\n\\[\\begin{align}\ny=\\beta_0\n\\end{align}\\]\nSo, \\(\\beta_0\\) represents the intercept.\n\n\n\n\n\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): coefficient (slope)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#why-do-we-want-ceteris-paribus-causal-impact",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Why do we want  ceteris paribus  causal impact?",
    "text": "Why do we want  ceteris paribus  causal impact?\n\nExampleWhy ceteris paribus impact?What do you observe?\n\n\nQuality of College\nYou\n\nhave been admitted to University A (better, more expensive) and B (worse, less expensive)\nare trying to decide which school to attend\nare interested in knowing a boost in your future income to make a decision\n\nYou have found the following data\n\n\nUniversityaverage incomesample sizeA130.13500B90.13500\n\n\nQuestion\nShould you assume that the observed difference of 40 is the expected boost you would get if you are to attend University A instead of B?\n\n\nLet’s say your ability score is \\(6\\) out of \\(10\\) (the higher, the better),\n\\[\\mbox{(1)}\\;\\; E[inc|A,ability=9] -E[inc|B,ability=6]\\] \\[\\mbox{(2)}\\;\\; E[inc|A,ability=6] -E[inc|B,ability=6]\\]\nWhich one would like you to know?\n\n\n\n\n\nImportant\n\n\n\nYou want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A\nYou do not want the impact of school quality to be confounded with something else\n\n\n\n\n\n\n\n\n\n\n\nAside: Conditional Expectation\n\n\n\\(E[Y|X]\\) represents expected value of \\(Y\\) conditional on \\(X\\) (For a given value of \\(X\\), the expected value of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nred line: \\(E[income|A, ability]\\)\nblue line: \\(E[income|B, ability]\\)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#example-corn-yield-and-fertilizer",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Example: corn yield and fertilizer",
    "text": "Example: corn yield and fertilizer\n\nModelEstimate \\(\\beta_1\\)Crucial conditionCondition satisfied?Math asides\n\n\nCorn yield and fertilizer\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\nQuestion\nWhat is in the error term?\n\n\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\n\nyou do not know \\(\\beta_0\\) and \\(\\beta_1\\), and would like to estimate them\nyou observe a series of \\(\\{yield_i,fertilizer_i\\}\\) combinations \\((i=1,\\dots,n)\\)\nyou would like to estiamte \\(\\beta_1\\), the impact of fertilizer on yield, ceteris paribus (with everything else fixed)\n\nQuestion\nHow could we possibly find the ceteris paribus impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?\n\n\nIt turns out we can identify the ceteris paribus causal impact of \\(x\\) on \\(y\\) as long as the following condition is satisfied:\n\n\n\n\nZero conditional mean\n\n\n\\(E(u|x) = 0\\)\n\n\n\n\nThis is satisfied when \\(E[u|x]=E[u]\\) and \\(E[u] = 0\\). Practically (and roughtly) speaking, this condition is satisfied if\n\n\n\nImportant\n\n\n\n the error term (\\(u\\)) is not correlated with \\(x\\) \n\nan intercept (\\(\\beta_0\\)) is included in the model (which we almost always do by default)\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer + u\n\\end{align}\\]\n\nData\nYou have collected farm-level yield-fertilizer data from 200 farmers in year 2023.\n\nQuestions\n\nWhat’s in \\(u\\)? (note that factors that do not affect yield are not part of \\(u\\))\nIs it correlated with fertilizer?\n\n\n\n\nMean independenceCorrelation and mean independence\\(E(u)=0\\)\n\n\n\n\n\n\nDefinition: Mean Independence\n\n\n\\(E[u|x]=E[u]\\)\n\n\n\n\n\nverbally: the average value of the error term (collection of all the unobservables) is the same at any value of \\(x\\), and that the common average is equal to the average of \\(u\\) over the entire population\n(almost) interchangeably: the error term is not correlated with \\(x\\)\n\n\n\nMean independence of \\(u\\) and \\(x\\) implies no correlation. But, no correlation does not imply mean independence.\n\\[\\begin{aligned}\n    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\\\\\\n    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\\\\\\n    = & E[ux] \\\\\\\\\n    = & E_x[E_u[u|x]] \\;\\; \\mbox{(iterated law of expectation)}\n\\end{aligned}\\]\nIf zero conditional mean condition \\((E(u|x)=0)\\) is satisfied,\n\\[\\begin{aligned}\n    Cov(u,x)= & E_x[0] = 0\n\\end{aligned}\\]\n\n\nExpected value of the error term is 0 \\((E(u)=0)\\).\nThis is always satisfied as long as an intercept is included in the model:\n\\[y = \\beta_0 + \\beta_1 x + u_1,\\;\\; \\mbox{where}\\;\\; E(u_1)=\\alpha\\]\nRewriting the model,\n\\[\\begin{aligned}\ny & = \\beta_0 + \\alpha + \\beta_1 x + u_1 - \\alpha \\\\\\\\\n  & = \\gamma_0 + \\beta_1 x + u_2\n\\end{aligned}\\]\nwhere, \\(\\gamma_0=\\beta_0+\\alpha\\) and \\(u_2=u_1-\\alpha\\).\nNow, \\(E[u_2]=0\\)."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#going-back-to-the-college-income-example",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Going back to the college-income example",
    "text": "Going back to the college-income example\nThe model\n\\[\nIncome = \\beta_0+\\beta_1 College\\;\\; A + u\n\\]\nwhere \\(College\\;\\; A\\) is 1 if attending college A, 0 if attending college B, and \\(u\\) is the error term that includes ability. \\(u\\) includes ability.\n\nZero conditional mean satisfied?\n\\[\nE[u(ability)|college A] = 0?\n\\]\nThat is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?\n\n\n\n\n\n\n\n\n\nThis is what it would like if college choice and ability are not correlated:"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression.html#exercise",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Exercise",
    "text": "Exercise\n\nconsider a phenomenon you are interested in understanding\n\ndependent variable (variable to be explained)\nexplanatory variable (variable to explain)\n\nconstruct a simple linear model\nidentify what is in the error term\ncheck if they are correlated withe explanatory variable or not\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#instrumental-variable-iv-approach-1",
    "href": "lectures/11-instrumental-variable/11-iv.html#instrumental-variable-iv-approach-1",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "Instrumental Variable (IV) Approach",
    "text": "Instrumental Variable (IV) Approach\n\nReview of EndogeneityCausal DiagramRough ideaEstimation procedureExample\n\n\nEndogeneity\n\\(E[u|x_k] \\ne 0\\) (the error term is not correlated with any of the independent variables)\n\nEndogenous independent variable\nIf the error term is, .red[for whatever reason], correlated with the independent variable \\(x_k\\), then we say that \\(x_k\\) is an endogenous independent variable.\n\nOmitted variable\nSelection\nReverse causality\nMeasurement error\n\n\n\n\nCausal diagram of the modelCausal diagram with an instrument\n\n\nYou want to estimate the causal impact of education on income.\n\nVariable of interest: Education\nDependent variable: Income\n\nCausal diagram\n\n\n\nWe want to find a variable like \\(Z\\) in the diagram below:\n\n\n\n\n\n\n\\(Z\\) does  NOT  affect income  directly \n\\(Z\\) is correlated with the variable of interest (education)\n\ndoes not matter which causes which (associattion is enough)\n\n\\(Z\\) is  NOT  correlated with  any  of the unobservable variables in the error term (including ability) that is making the vairable of interest (education) endogeneous.\n\n\\(Z\\) does not affect ability\nabiliyt does not affect \\(Z\\)\n\n\n\n\n\n\n\n\n\n\n\nThe Model\n\\(y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\)\n\n\\(x_1\\) is endogenous: \\(E[u|x_1] \\ne 0\\) (or \\(Cov(u,x_1)\\ne 0\\))\n\\(x_2\\) is exogenous: \\(E[u|x_1] = 0\\) (or \\(Cov(u,x_1) = 0\\))\n\n\nIdea (very loosely put)\nBring in variable(s) ( Instrumental variable(s) ) that does  NOT  belong to the model, but  IS  related with the endogenous variable,\n\nUsing the instrumental variable(s) (which we denote by \\(Z\\)), make the endogenous variable exogenous, which we call .blue[instrumented] variable(s)\nUse the variation in the instrumented variable instead of the original endogenous variable to estimate the impact of the original variable\n\n\n\n\nStep 1Step 2\n\n\nIdea\nUsing the instrumental variables, make the endogenous variable exogenous, which we call  instrumented  variable.\n\nStep 1: mathematically\n\nRegress the endogenous variable \\((x_1)\\) on the instrumental variable(s) \\((Z=\\{z_1,z_2\\}\\), two instruments here) and all the other exogenous variables \\((x_2\\) here)\n\n\\(x_1 = \\alpha_0 + \\sigma_2 x_2 + \\alpha_1 z_1 +\\alpha_2 z_2 + v\\)\n\nobtain the predicted value of \\(x\\) from the regression\n\n\\(\\widehat{x}_1 = \\widehat{\\alpha}_0 + \\widehat{\\sigma}_2 x_2 + \\widehat{\\alpha}_1 z_1 + \\widehat{\\alpha}_2 z_2\\)\n\n\nIdea\nUse the variation in the instrumented variable instead of the original endogenous variable to estimate the impact of the original variable\n\nStep 2: Mathematically\nRegress the dependent variable \\((y)\\) on the instrumented variable \\((\\widehat{x}_1)\\),\n\\(y= \\beta_0 + \\beta_1 \\widehat{x}_1+ \\beta_2 x_2 + \\varepsilon\\)\nto estimate the coefficient on \\(x\\) in the original model\n\n\n\n\n\n\n\nModel of interestStep 1Step 2\n\n\nModel\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + (\\beta_3 ability + v)\\)\n\nRegress \\(log(wage)\\) on \\(educ\\) and \\(exper\\) \\((ability\\) not included because you do not observe it)\n\\((\\beta_3 ability + v)\\) is the error term\n\\(educ\\) is considered endogenous (correlated with \\(ability\\))\n\\(exper\\) is considered exogenous (not correlated with \\(ability\\))\n\n\nInstruments (Z)\nSuppose you selected the following variables as instruments:\n\nIQ test score \\((IQ)\\)\nnumber of siblings \\((sibs)\\)\n\n\n\nRegress \\(educ\\) on \\(exper\\), \\(IQ\\), and \\(sibs\\):\n\\[\\begin{align*}\neduc = \\alpha_0 + \\alpha_1 exper + \\alpha_2 IQ + \\alpha_3 sibs + u\n\\end{align*}\\]\nUse the coefficient estimates on \\(\\alpha_0\\), \\(\\alpha_1\\), \\(\\alpha_2\\), and \\(\\alpha_3\\) to predict \\(educ\\) as a function of \\(exper\\), \\(IQ\\), and \\(sibs\\).\n\\[\\begin{align*}\n\\widehat{educ} = \\widehat{\\alpha_0} + \\widehat{\\alpha_1} exper + \\widehat{\\alpha_2} IQ + \\widehat{\\alpha_3} sibs\n\\end{align*}\\]\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUse \\(\\widehat{educ}\\) in place of \\(educ\\) to estimate the model of interest:\n\\[\\begin{align*}\nlog(wage) = \\beta_0 + \\beta_1 \\widehat{educ} + \\beta_2 exper + u\n\\end{align*}\\]\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#when-does-iv-work",
    "href": "lectures/11-instrumental-variable/11-iv.html#when-does-iv-work",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "When does IV work?",
    "text": "When does IV work?\n\nIntroductionCondition 1Condition 2Review of the conditions2SLS\n\n\nJust like OLS needs to satisy some conditions for it to consistently estimate the coefficients, IV approach needs to satisy some conditions for it to work.\n\nEstimation Procedure\n\nStep 1: \\(\\widehat{x}_1 = \\widehat{\\alpha}_0 +\\widehat{\\sigma}_2 x_2 + \\widehat{\\alpha}_1 z_1 + \\widehat{\\alpha}_2 z_2\\)\nStep 2: \\(y = \\beta_0 + \\beta_1 \\widehat{x}_1+ \\beta_2 x_2 + \\varepsilon\\)\n\n\nImportant question\nWhat are the conditions under which IV estimation is consistent?\nThe instruments \\((Z)\\) need to satisfy two conditions, which we will discuss.\n\n\nEstimation Procedure\n\nStep 1: \\(\\widehat{x}_1 = \\widehat{\\alpha}_0 +\\widehat{\\sigma}_2 x_2 + \\widehat{\\alpha}_1 z_1 + \\widehat{\\alpha}_2 z_2\\)\nStep 2: \\(y = \\beta_0 + \\beta_1 \\widehat{x}_1+ \\beta_2 x_2 + \\varepsilon\\)\n\n\nQuestion\nWhat happens if \\(Z\\) have no power to explain \\(x_1\\) \\((\\alpha_1=0\\) and \\(\\alpha_2=0)\\)?\n\n\n\nAnswer\n\n\n\\(\\widehat{x}_1=\\widehat{\\alpha}_0+\\widehat{\\sigma}^2 x_2\\)\n\\(\\widehat{\\beta}_1?\\)\n\nThat is, \\(\\widehat{x}_1\\) has no information beyond the information \\(x_2\\) possesses.\nThe instrument(s) \\(Z\\) have jointly significant explanatory power on the endogenous variable \\(x_1\\) .red[after] you control for all the other exogenous variables (here \\(x_2\\))\n\n\n\n\n\nModel of interest\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\)\n\nEstimation Procedure\n\nStep 1: \\(\\widehat{x}_1 = \\widehat{\\alpha}_0 +\\widehat{\\sigma}_2 x_2 + \\widehat{\\alpha}_1 z_1 + \\widehat{\\alpha}_2 z_2\\)\nStep 2: \\(y = \\beta_0 + \\beta_1 \\widehat{x}_1+ \\beta_2 x_2 + \\varepsilon\\)\n\n\n\n\n\nInvestigation of the modelWhen is it unbiased?So,\n\n\nRemember you can break \\(x_1\\) into the predicted part and the residuals.\n\\[\\begin{align*}\nx_1 = \\widehat{x}_1 + \\widehat{\\varepsilon}\n\\end{align*}\\]\nwhere \\(\\widehat{\\varepsilon}\\) is the residual of the first stage estimation.\nPlugging in \\(x_1 = \\widehat{x}_1 + \\widehat{\\varepsilon}\\) into the model of interest,\n\\[\\begin{align*}\ny & =  \\beta_0 + \\beta_1 (\\widehat{x}_1 + \\widehat{\\varepsilon}) + \\beta_2 x_2+ u\\\\\n\n& = \\beta_0 + \\beta_1 \\widehat{x}_1 + \\beta_2 x_2+ (\\beta_1\\widehat{\\varepsilon} + u)\n\\end{align*}\\]\nSo, if you regress \\(y\\) on \\(\\widehat{x}_1\\) and \\(x_2\\), then the error term is \\((\\beta_1\\widehat{\\varepsilon} + u)\\).\n\nQuestion\nWhat is the condition under which the OLS estimation of \\(\\beta_1\\) in the main model is unbiased?\n\n\n\nAnswer\n\n\\(\\widehat{x}_1\\) is not correlated with \\((\\beta_1\\widehat{\\varepsilon} + u)\\)\n\n\n\nWe confirmed that we need the following condition to be satisfied:\n\n\n\n\nNote\n\n\n\\(\\widehat{x}_1\\) is not correlated with \\((\\beta_1\\widehat{\\varepsilon} + u)\\)\n\n\n\n\nThis in turn means that \\(x_2\\), \\(z_1\\), and \\(z_2\\) are not correlated with \\(u\\) (the error term of the true model.)\n\\((\\widehat{x}_1\\) is always not correlated (orthogonal) with \\(\\varepsilon)\\)\n\n\n\n\n\n\nCondition 2\n\n\n\n\\(z_1\\) and \\(z_2\\) do not belong in the main model, meaning they do not have any explanatory power beyond \\(x_2\\) (they should have been included in the model in the first place as independent variables)\n\\(z_1\\) and \\(z_2\\) are not correlated with the error term (there are no unobserved factors in the error term that are correlated with \\(Z\\))\n\n\n\n\n\n\nQuestion\nDo you think we can test condition 2?\n\n\n\nAnswer\n\nNo, because we never observe the error term.\n\n\n\n\n\nImportant\n\n\n\nAll we can do is to  argue  that the instruments are not correlated with the error term.\nIn journal articles that use IV method, they make careful arguments as to why their choice of instruments are not correlated with the error term.\n\n\n\n\n\n\n\n\n\n\nCondition 1\n\nThe instrument(s) \\(Z\\) have jointly significant explanatory power on the endogenous variable \\(x_1\\)  after  you control for all the other exogenous variables (here \\(x_2\\))\n\n\nCondition 2\n\n\\(z_1\\) and \\(z_2\\) do not belong in the main model, meaning they do not have any explanatory power beyond \\(x_2\\) (they should have been included in the model in the first place as independent variables)\n\\(z_1\\) and \\(z_2\\) are not correlated with the error term (there are no unobserved factors in the error term that are correlated with \\(Z)\\)\n\n\n\n\n\nImportant\n\n\n\nCondition 1 is always testable\nCondition 2 is NOT testable (unless you have more instruments than endogenous variables)\n\n\n\n\n\n\nIV estimator is also called two-stage least squares estimator (2SLS) because it involves two stages of OLS.\n\nStep 1: \\(\\widehat{x}_1 = \\widehat{\\alpha}_0 +\\widehat{\\sigma}_2 x_2 + \\widehat{\\alpha}_1 z_1 + \\widehat{\\alpha}_2 z_2\\)\nStep 2: \\(y = \\beta_0 + \\beta_1 \\widehat{x}_1+ \\beta_2 x_2 + \\varepsilon\\)\n\n2SLS framework is a good way to understand conceptually why and how instrumental variable estimation works. But, IV estimation is done in one-step"
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#instrumental-variable-validity",
    "href": "lectures/11-instrumental-variable/11-iv.html#instrumental-variable-validity",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "Instrumental variable validity",
    "text": "Instrumental variable validity\n\nSetupInstrument example 1Instrument example 2Instrument example 3Instrument example 4\n\n\nThe model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; ( = \\beta_3 ability + u)\\)\neduc is endogenous because of its correlation with ability.\n\nQuestion\nWhat conditions would a good instrument \\((z)\\) satisfy?\n\n\n\n\nAnswer\n\n\n\\(z\\) has explanatory power on \\(educ\\) .blue[after] you control for the impact of \\(epxer\\) on \\(educ\\)\n\\(z\\) is uncorrelated with \\(v\\) (\\(ability\\) and all the other important unobservables)\n\n\n\n\nThe model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; ( = \\beta_3 ability + u)\\)\n\nInstrument\nThe last digit of an individual’s Social Security Number? (this has been actually used in some journal articles)\n\nQuestion\n\nIs it uncorrelated with \\(v\\) (\\(ability\\) and all the other important unobservables)?\ndoes it have explanatory power on \\(educ\\) .blue[after] you control for the impact of \\(epxer\\) on \\(educ\\)?\n\n\n\nThe model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; ( = \\beta_3 ability + u)\\)\n\nInstrument\nIQ test score\n\nQuestion\n\nIs it uncorrelated with \\(v\\) (\\(ability\\) and all the other important unobservables)?\ndoes it have explanatory power on \\(educ\\) .blue[after] you control for the impact of \\(epxer\\) on \\(educ\\)?\n\n\n\nThe model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; ( = \\beta_3 ability + u)\\)\n\nInstrument\nMother’s education\n\nQuestion\n\nIs it uncorrelated with \\(v\\) (\\(ability\\) and all the other important unobservables)?\ndoes it have explanatory power on \\(educ\\) .blue[after] you control for the impact of \\(epxer\\) on \\(educ\\)?\n\n\n\nThe model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; ( = \\beta_3 ability + u)\\)\n Instrument\nNumber of siblings\n\nQuestion\n\nIs it uncorrelated with \\(v\\) (\\(ability\\) and all the other important unobservables)?\ndoes it have explanatory power on \\(educ\\) .blue[after] you control for the impact of \\(epxer\\) on \\(educ\\)?"
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#implementation-of-instrumental-variable-iv-estimation-in-r",
    "href": "lectures/11-instrumental-variable/11-iv.html#implementation-of-instrumental-variable-iv-estimation-in-r",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "Implementation of Instrumental Variable (IV) Estimation in R",
    "text": "Implementation of Instrumental Variable (IV) Estimation in R\n\nIntroductionDatasetHowResultsComparisonFixed effectsClustered SE\n\n\nModel\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + v \\;\\; (=\\beta_3 ability + u)\\)\nWe believe\n\n\\(educ\\) is endogenous \\((x_1)\\)\n\\(exper\\) is exogenous \\((x_2)\\)\nwe use the number of siblings \\((sibs)\\) and father’s education \\((feduc)\\) as the instruments (\\(Z\\))\n\n\nTerminology\n\nexogenous variable included in the model (here, \\(exper\\)) is also called .blue[included instruments]\ninstruments that do not belong to the main model (here, \\(sibs\\) and \\(feduc\\)) are also called .blue[excluded instruments]\nwe refer to the collection of included and excluded instruments as .blue[instruments]\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe can continue to use the fixest package to run IV estimation method.\nSyntax\n\nfixest::feols(dep var ~ included instruments | first stage formula, data = dataset)\n\n\nincluded instruments: exogenous included variables (do not include endogenous variables here)\n\n\nfirst stage formula\n\n(endogenous vars ~ excluded instruments)\n\n\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nincluded variable:\n\nexogenous included variables: exper\nendogenous included variables: educ\n\ninstruments:\n\nincluded instruments: exper\nexcluded instruments: sibs and feduc\n\n\n\n\nIV regression results\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNote\n\nWhen variable x is the endogenous variable, fixest changes the name of x to x(fit).\nHere, educ has become educ(fit).\n\n\n\n\n\nComparison of OLS and IV Estimation Results\n\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n5.503***\n4.507***\n\n\n\n(0.112)\n(0.316)\n\n\neduc\n0.078***\n\n\n\n\n(0.007)\n\n\n\nexper\n0.020***\n0.037***\n\n\n\n(0.003)\n(0.006)\n\n\nfit_educ\n\n0.137***\n\n\n\n\n(0.019)\n\n\nNum.Obs.\n935\n741\n\n\nR2\n0.131\n0.053\n\n\nRMSE\n0.39\n0.41\n\n\nStd.Errors\nIID\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\nQuestion\nDo you think \\(sibs\\) and \\(feduc\\) are good instruments?\n\nCondition 1: weak instruments?\nCondition 2: uncorrelated with the error term?\n\n\n\n\n\n\nYou can include fixed effects in your IV estimation.\nSyntax\n\nfixest::feols(dep var ~ included instruments | FE | 1st stage formula, data = dataset)\n\n\nExample\nInclude married and south as fixed effects.\n\nfixest::feols(log(wage) ~ exper | married + south | educ ~ feduc + sibs, data = wage2)\n\nTSLS estimation, Dep. Var.: log(wage), Endo.: educ, Instr.: feduc, sibs\nSecond stage: Dep. Var.: log(wage)\nObservations: 741 \nFixed-effects: married: 2,  south: 2\nStandard-errors: Clustered (married) \n         Estimate Std. Error t value Pr(&gt;|t|)    \nfit_educ 0.124355   0.003627 34.2906 0.018560 *  \nexper    0.032128   0.002260 14.2144 0.044713 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.391178     Adj. R2: 0.116588\n                 Within R2: 0.069595\nF-test (1st stage), educ: stat = 62.0     , p &lt; 2.2e-16 , on 2 and 736 DoF.\n              Wu-Hausman: stat =  8.98498 , p = 0.002814, on 1 and 735 DoF.\n                  Sargan: stat =  0.169226, p = 0.6808  , on 1 DoF.\n\n\n\n\nYou can just add cluster = option just like we previously did.\n\nfixest::feols(log(wage) ~ exper | married + south | educ ~ feduc + sibs, cluster = ~black, data = wage2)\n\nTSLS estimation, Dep. Var.: log(wage), Endo.: educ, Instr.: feduc, sibs\nSecond stage: Dep. Var.: log(wage)\nObservations: 741 \nFixed-effects: married: 2,  south: 2\nStandard-errors: Clustered (black) \n         Estimate Std. Error t value Pr(&gt;|t|)    \nfit_educ 0.124355   0.005258 23.6526 0.026899 *  \nexper    0.032128   0.002798 11.4842 0.055295 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 0.391178     Adj. R2: 0.116588\n                 Within R2: 0.069595\nF-test (1st stage), educ: stat = 61.9     , p &lt; 2.2e-16 , on 2 and 735 DoF.\n              Wu-Hausman: stat =  8.98498 , p = 0.002814, on 1 and 735 DoF.\n                  Sargan: stat =  0.169226, p = 0.6808  , on 1 DoF."
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#weak-instrument",
    "href": "lectures/11-instrumental-variable/11-iv.html#weak-instrument",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "Weak instrument",
    "text": "Weak instrument\n\nWhat is it?How?Consequences of weak instruments\n\n\n\n\n\n\nDefinition: weak instrument\n\n\nThe external instrument(s) does not have enough explanatory power on the instrumented (endogenous) variabl beyond the other controls.\n\n\n\n\n\n\n\n\nImportant\n\n\nWe can always test if the excluded instruments are weak or not!\n\n\n\n\n\n\n\nStepsR implementation\n\n\nRun the 1st stage regression\n\\[\\begin{align*}\neduc = \\alpha_0 + \\alpha_1 exper + \\alpha_2 sibs + \\alpha_3 feduc + v\n\\end{align*}\\]\n\nThen, test the joint significance of \\(\\alpha_2\\) and \\(\\alpha_3\\) (\\(F\\)-test)\nIf excluded instruments \\((sibs\\) and \\(feduc\\), here) are jointly significant, then it would mean that \\(sibs\\) and \\(feduc\\) are not weak instruments, satisfying condition 1.\n\n\n\n\nWhen we ran the IV estimation using fixest::feols() earlier, it automatically calculated the F-statistic for the weak instrument test.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHere, F-test for the null hypothesis of the excluded instruments (sibs and feduc) do not have any explanatory power on the endogenous variable (educ) beyond the included instrument (exper) is rejected.\n\n\n\nAlternatively\n\nYou can access the iv_first_stage component of the regression results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nNote\n\n\n\nIt is generally recommended that you have \\(F\\)-stat of over \\(10\\) (this is not a clear-cut criteria that applied to all the empirical cases)\nEven if you reject the null if \\(F\\)-stat is small, you may have a problem\nYou know nothing about if your excluded instruments satisfy Condition 2.\nIf you cannot reject the null, it is a strong indication that your instruments are weak. Look for other instruments.\nAlways, always report this test. There is no reason not to.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData generating processEstimationComparison of the weak instrument testsMC simulation\n\n\nData generation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCorrelation\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nEstimation with the strong instrumental variable\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimation with the weak instrumental variable\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuestion\nAny notable differences?\n\n\n\nAnswer\n\nThe coefficient estimate on \\(x\\_end\\) is far away from the true value in the weak instrument case.\n\n\n\n\n\ndiagnostics (strong instrument)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\ndiagnostics (weak instrument)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nNote\n\n\nYou cannot reject the null hypothesis of weak instrument in the weak instrument case.\n\n\n\n\n\n\nRunVisualization of the results\n\n\n\nset.seed(238354)\nB &lt;- 1000 # the number of experiments\nN &lt;- 500 # number of observations\nbeta_hat_store &lt;- matrix(0, B, 2) # storage of beta hat\n\nfor (i in 1:B) {\n\n  #--- data generation ---#\n  u_common &lt;- runif(N)\n  z_common &lt;- runif(N)\n  x_end &lt;- u_common + z_common + runif(N)\n  z_strong &lt;- z_common + runif(N)\n  z_weak &lt;- 0.01 * z_common + 0.99995 * runif(N)\n  u &lt;- u_common + runif(N)\n  y &lt;- x_end + u\n  data &lt;- data.table(y, x_end, z_strong, z_weak)\n\n  #--- IV estimation with a strong instrument ---#\n  iv_strong &lt;- fixest::feols(y ~ 1 | x_end ~ z_strong, data = data)\n  beta_hat_store[i, 1] &lt;- iv_strong$coefficients[2]\n\n  #--- IV estimation with a weak instrument ---#\n  iv_weak &lt;- fixest::feols(y ~ 1 | x_end ~ z_weak, data = data)\n  beta_hat_store[i, 2] &lt;- iv_weak$coefficients[2]\n}\n\n\n\n\n\nCode\nmelted &lt;- melt(data.table(beta_hat_store))\nmelted[variable == \"V1\", variable := \"Strong\"]\nmelted[variable == \"V2\", variable := \"Weak\"]\n\nggplot(data = melted[abs(value) &lt; 5, ]) +\n  geom_density(aes(x = value, fill = variable), alpha = 0.3) +\n  geom_vline(xintercept = 1, color = \"red\") +\n  scale_fill_discrete(name = \"\") +\n  theme(\n    legend.position = \"bottom\"\n  )"
  },
  {
    "objectID": "lectures/11-instrumental-variable/11-iv.html#flow-of-iv-estimation-in-practice",
    "href": "lectures/11-instrumental-variable/11-iv.html#flow-of-iv-estimation-in-practice",
    "title": "11: Dealing with Endogeneity: Instrumental Variable",
    "section": "Flow of IV Estimation in Practice",
    "text": "Flow of IV Estimation in Practice\n\n\n\n\nFlow\n\n\n\nIdentify endogenous variable(s) and included instrument(s)\nIdentify potential excluded instrument(s)\n Argue  why the excluded instrument(s) you pick is uncorrelated with the error term (condition 2)\nOnce you decide what variable(s) to use as excluded instruments, .red[test] whether the excluded instrument(s) is weak or not ( condition 1)\nImplement IV estimation and report the results\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#homoskedasticity-and-heteroskedasticity",
    "href": "lectures/06-se-estiation/06-se-estimation.html#homoskedasticity-and-heteroskedasticity",
    "title": "06: Standard Error Estimation",
    "section": "Homoskedasticity and Heteroskedasticity",
    "text": "Homoskedasticity and Heteroskedasticity\n\nReviewVisualizationCentral QuestionsCoefficient estimatorsVariance of the coefficient estimators\n\n\n\n\n\n\nHomoskedasticity\n\n\n\\(Var(u|x) = \\sigma^2\\)\n\n\n\n\n\n\n\n\n\nHeteroskedasticity\n\n\n\\(Var(u|x) = f(x)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat are the consequences of assuming the error is homoskedastic when it is heteroskedastic in reality?\n\nEstimation of coefficients \\((\\widehat{\\beta}_j)\\)?\nEstimation of the variance of \\(\\widehat{\\beta}_j\\)?\n\n\n\n\nQuestionAnswer\n\n\nAre OLS estimators unbiased when error is heteroskedastic?\n\n\nYes. We do not need to use the homoskedasticity assumption to prove that the OLS estimator is unbiased.\n\n\n\n\n\n\n\nhomeskedastic errorheteroskedastic errorConsequences\n\n\nWe learned that when the homoskedasticity assumption holds, then,\n\\(Var(\\widehat{\\beta}_j) = \\frac{\\sigma^2}{SST_x(1-R^2_j)}\\)\nWe used the following as the estimator of \\(Var(\\widehat{\\beta}_j)\\)\n\\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) where \\(\\widehat{\\sigma}^2 = \\frac{\\sum_{i=1}^{N} \\widehat{u}_i^2}{N-k-1}\\)\n\n\n\n\nImportant\n\n\n By default , R and other statistical software uses this formula to get estimates of the variance of \\(\\widehat{\\beta}_j\\).\n\n\n\n\n\nBut, under heteroskedasticity,\n\\(Var(\\widehat{\\beta}_j) \\ne \\frac{\\sigma^2}{SST_x(1-R^2_j)}\\)\n\n\nQuestionAnswer\n\n\nIs \\(E[\\widehat{Var(\\widehat{\\beta}_j)}_{default}] \\equiv E\\Big[\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\Big]=Var(\\widehat{\\beta}_j)\\) under heteroskedasticity?\n\n\nNo.\n\n\n\n\n\n\nSo, what are the consequences of using \\(\\widehat{Var(\\widehat{\\beta}_j)}=\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) under heteroskedasticity?\n\\(\\;\\;\\;\\;\\downarrow\\)\n Your hypothesis testing is going to be biased!! \n\n\nQuestionAnswer\n\n\nWhat does it mean to have hypothesis testing biased?\n\n\nRoughly speaking, it means that you over-reject/under-reject the hypothesis than you intend to."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#consequence-of-heteroskedasticity-on-testing",
    "href": "lectures/06-se-estiation/06-se-estimation.html#consequence-of-heteroskedasticity-on-testing",
    "title": "06: Standard Error Estimation",
    "section": "Consequence of heteroskedasticity on testing",
    "text": "Consequence of heteroskedasticity on testing\n\nMotivation and setupMC simulation\n\n\nLet’s run MC simulations to see the consequence of ignoring heteroskedasticity.\n\nModel\n\\(y = 1 + \\beta x + u\\), where \\(\\beta = 0\\)\n\nTest of interest\n\n\\(H_0:\\) \\(\\beta=0\\)\n\\(H_1:\\) \\(\\beta \\ne 0\\)\n\n\n\nQuestionAnswer\n\n\nIf you test the null hypothesis at the \\(5\\%\\) significance level, what should be the probability that you reject the null hypothesis when it is actually true?\n\\(Pr(\\mbox{reject} \\;\\; H_0|H_0 \\;\\; \\mbox{is true})=?\\)\n\n\n\\(5\\%\\)\n\n\n\n\n\n\n\nconceptual stepsR implementationResults\n\n\n\ngenerate a dataset so that \\(\\beta_1\\) (the coefficient on \\(x\\)) is zero\n\n\\[y=\\beta_0+\\beta_1 x + v\\]\n\nestimate the model and find \\(\\widehat{\\beta}_1\\) and \\(\\widehat{se(\\widehat{\\beta}_1)}\\)\ncalculate \\(t\\)-statistic \\((\\widehat{\\beta}_x-0)/\\widehat{se(\\widehat{\\beta}_x)}\\) and decide whether you reject the null or not\nrepeat the above 1000 times\ncheck how often you reject the null (should be close to 50 times)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nConsequence of ignoring heteroskedasticity\n\n\nWe rejected the null hypothesis 10.8% of the time, instead of \\(5\\%\\).\n\nSo, in this case, you are more likely to claim that \\(x\\) has a statistically significant impact than you are supposed to.\nThe use of the formula \\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) seemed to (over/under)-estimate the true variance of the OLS estimators?\nIn general, the direction of bias is ambiguous."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#how-should-we-address-this-problem",
    "href": "lectures/06-se-estiation/06-se-estimation.html#how-should-we-address-this-problem",
    "title": "06: Standard Error Estimation",
    "section": "How should we address this problem?",
    "text": "How should we address this problem?\n\nWhat to do?Robust estimator of seIn practice\n\n\nNow, we understand the consequence of heteroskedasticity:\n\\(\\frac{\\widehat{\\sigma}^2}{SST_x(1-R^2_j)}\\) is a biased estimator of \\(Var(\\widehat{\\beta})\\), which makes any kind of testings based on it invalid.\nCan we credibly estimate the variance of the OLS estimators?\n\n\n\n\n\nWhite-Huber-Eicker heteroskedasticity-robust standard error estimator\n\n\n\nvalid in the presence of heteroskedasticity of .red[unknown form]\nheteroskedasticity-robust standard error estimator in short\n\n\n\n\n\n\n\nHeteroskedasticity-robust standard error estimator\n\\(\\widehat{Var(\\widehat{\\beta}_j)} = \\frac{\\sum_{i=1}^n \\widehat{r}^2_{i,j} \\widehat{u}^2_i}{SSR^2_j}\\)\n\n\\(\\widehat{u}_i\\): residual from regressing \\(y\\) on all the independent variables\n\\(\\widehat{r}_{i,j}\\): residual from regressing \\(x_j\\) on all other independent variables for \\(i\\)th observation\n\\(SSR^2_j\\): the sum of squared residuals from regressing \\(x_j\\) on all other independent variables\n\n\n\n\nNote\n\n\nWe spend  NO  time to try to understand what’s going on with the estimator.\n\n\n\nWhat you need is\n\nunderstand the consequence of heteroskedasticity\nknow there is an estimator that is appropriate under heteroskedasticity, meaning that it will give you the correct estimate of the variance of the OLS estimator\nknow how to use the heteroskedasticity-robust standard error estimator in practice using \\(R\\) (or some other software)\n\n\n\nHere is the well-accepted procedure in econometric analysis:\n\nEstimate the model using OLS (you do nothing special here)\nAssume the error term is heteroskedastic and estimate the variance of the OLS estimators\n\nThere are tests to whether error is heteroskedastic or not: .red[Breusch-Pagan] test and .red[White] test\nIn practice, almost nobody bothers to conduct these tests\nWe do not learn how to run these tests\n\nReplace the estimates from \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) with those from \\(\\widehat{Var(\\widehat{\\beta})}_{robust}\\) for testing\nBut, we do not replace coefficient estimates (remember, coefficient estimation is still unbiased under heteroskedasticity)"
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#robust-standard-error-estimation-in-r",
    "href": "lectures/06-se-estiation/06-se-estimation.html#robust-standard-error-estimation-in-r",
    "title": "06: Standard Error Estimation",
    "section": "Robust standard error estimation in R",
    "text": "Robust standard error estimation in R\n\nImplementation in RReporting the regression resultsAlternatively (Recommended)Validation\n\n\n\nPreparationObtaining Heteroskedasticity-robust SE estimatesCompare with the Default\n\n\nLet’s run a regression using MLB1.dta.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe use\n\nthe stats::vcov() function to estimate heteroskedasticity-robust standard errors\nthe fixest::se() function from the fixest package to estimate heteroskedasticity-robust standard errors (you can always get SE from VCOV)\nthe summary() function to do tests of \\(\\beta_j = 0\\)\n\nGeneral Syntax\nHere is the general syntax to obtain various types of VCOV (and se) esimaties:\n\n#* vcov\nvcov(regression result, vcov = \"type of vcov\")\n\n#* only the standard errors\nfixest::se(regression result, vcov = \"type of vcov\")\n\n\nheteroskedasticity-robust standard error estimation\nSpecifically for White-Huber heteroskedasticity-robust VCOV and se estimates,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDefault\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nHeteroskedasticity-robust\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nIn presenting the regression results in a nicely formatted table, we used modelsummary::msummary().\nWe can easily swap the default se with the heteroskedasticity-robust se using the statistic_override option in msummary().\n\n\n\n\nvcov_het &lt;- vcov(reg_mlb, vcov = \"hetero\")\nvcov_homo &lt;- vcov(reg_mlb)\n\nmodelsummary::msummary(\n  list(reg_mlb, reg_mlb),\n  statistic_override = list(vcov_het, vcov_homo),\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n) \n\n\n\n\n\n\n\n\n (1)\n  (2)\n\n\n\n\n(Intercept)\n11.042***\n11.042***\n\n\n\n(0.704)\n(0.343)\n\n\nyears\n0.166***\n0.166***\n\n\n\n(0.018)\n(0.013)\n\n\nbavg\n0.005+\n0.005***\n\n\n\n(0.003)\n(0.001)\n\n\nNum.Obs.\n353\n353\n\n\nR2\n0.367\n0.367\n\n\nRMSE\n0.94\n0.94\n\n\nStd.Errors\nCustom\nCustom\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlternatively, you could add the vcov option like below inside fixest::feols(). Then, you do not need statistic_override option to override the default VCOV estimates.\n\n\n\n\nreg_mlb_with_rvcov &lt;- \n  fixest::feols(\n    log(salary) ~ years + bavg,\n    vcov = \"hetero\", \n    data = mlb1\n  ) \n\nmodelsummary::msummary(\n  list(reg_mlb_with_rvcov),\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n) \n\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n11.042***\n\n\n\n(0.704)\n\n\nyears\n0.166***\n\n\n\n(0.018)\n\n\nbavg\n0.005+\n\n\n\n(0.003)\n\n\nNum.Obs.\n353\n\n\nR2\n0.367\n\n\nRMSE\n0.94\n\n\nStd.Errors\nHeteroskedasticity-robust\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMC simulationMC simulation results\n\n\nDoes the heteroskedasticity-robust se estimator really work? Let’s see using MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOkay, not perfect. But, certainly better."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#clustered-error-1",
    "href": "lectures/06-se-estiation/06-se-estimation.html#clustered-error-1",
    "title": "06: Standard Error Estimation",
    "section": "Clustered Error",
    "text": "Clustered Error\n\nWhat is it?Consequences of clustered errorMC simulationsWhat to do?In practice\n\n\n\nOften times, observations can be grouped into clusters\nErrors within the cluster can be correlated\n\n\nExample 1Example 2\n\n\nCollege GPA: cluster by college\n\\(GPA_{col} = \\beta_0 + \\beta_1 income + \\beta_2 GPA_{hs} + u\\)\n\nYour observations consist of students’ GPA scores across many colleges\nBecause of some unobserved (omitted) school characteristics, error terms for the individuals in the same college might be correlated.\n\ngrading policy\n\n\n\n\nEduction Impacts on Income: cluster by individual\n\nYour observations consist of 500 individuals with each individual tracked over 10 years\nBecause of some unobserved (omitted) individual characteristics, error terms for time-series observations within an individual might be correlated.\n\ninnate ability\n\n\n\n\n\n\n\n\n\nQuestion 1Question 2Question 3\n\n\nAre the OLS estimators of the coefficients biased in the presence of clustered error?\n\n\n\nAnswer\n\nNo, the correlation between \\(x\\) and \\(u\\) would hurt you, but not correlation among \\(u\\).\n\n\n\n\nAre \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) unbiased estimators of \\(Var(\\widehat{\\beta})\\)?\n\n\n\nAnswer\n\nNo, \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) is unbiased only under homoskedasticity assumption, which assumes no correlation between errors.\n\n\n\nWhich has more information?\n\ntwo errors that are independent\ntwo errors that are correlated\n\nConsequences\n\nIf you were to use \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) to estimate \\(Var(\\widehat{\\beta})\\) in the presence of clustered error, you would (under/over)-estimate the true \\(Var(\\widehat{\\beta})\\).\nThis would lead to rejecting null hypothesis (more/less) often than you are supposed to.\n\n\n\n\n\n\n\n\nConceptual stepsData Generating Process (R)Visualization of clustered errorMC simulation (R)Results\n\n\nHere are the conceptual steps of the MC simulations to see the consequence of clustered error.\n\ngenerate data according to the generating process in which the error terms \\((u)\\) within the cluster (two clusters in this example) is correlated and \\(\\beta_1\\) is set to 0 in the model below:\n\n\\[\n\\begin{aligned}\ny = \\beta_0 + \\beta_1 x + u\n\\end{aligned}\n\\]\n\nestimate the model and find \\(\\widehat{\\beta}_x\\) and \\(\\widehat{se(\\widehat{\\beta}_x)}\\)\ncalculate \\(t\\)-statistic \\((\\widehat{\\beta}_x/\\widehat{se(\\widehat{\\beta}_x)})\\) for the (correct) null hypothesis of \\(\\beta_1 = 0\\)\nrepeat steps 1-3 for 1000 times\nsee how many times out of 1000 times you reject the null hypothesis: \\(H_0:\\) \\(\\beta_x=0\\)\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nImportant\n\n\n\nclustered error can severely bias your test results\nit tends to make the impact of explanatory variables more significant than they truly are because the default estimator of the variance of the OLS estimator tends to greatly  under-estimate the true variance of the OLS estimator.\n\n\n\n\n\n\n\n\n\n\n\nCluster-robust SE estimationR implementationR DemonstrationCompareAlternatively (Recommended)\n\n\nThere exist estimators of \\(Var(\\widehat{\\beta})\\) that take into account the possibility that errors are clustered.\n\nWe call such estimators  cluster-robust variance covariance estimator  denoted as \\((\\widehat{Var(\\widehat{\\beta})}_{cl})\\)\nWe call standart error estimates from such estimators cluster-robust standard error estimates\n\n I neither derive nor show the mathematical expressions of these estimators. \n\n\n\n\n\nThis is what you need to do\n\n\n\nunderstand the consequence of clustered errors\nknow there are estimators that are appropriate under clustered error\nknow that the estimators we will learn take care of heteroskedasticity at the same time (so, they really are cluster- and heteroskedasticity-robust standard error estimators)\nknow how to use the estimators in \\(R\\) (or some other software)\n\n\n\n\n\n\n\nCluster-robust standard error\nSimilar with the vcov option for White-Huber heteroskedasticity-robust se, we can use the cluster option to get cluster-robust se.\n\nBefore an R demonstration\nLet’s take a look at the MLB data again.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nnl: the group variable we cluster around (1 if in the National league, 0 if in the American league).\n\n\n\nStep 1\nRun a regression\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nStep 2\nApply vcov() or se() with the cluster = option.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nDefault\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCluster-robust standard error\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nOr, you could add the cluster option inside fixest::feols().\n\nSyntax\n\nfixes::feols(y ~ x, cluster = ~ variable to cluster by, data = data)\n\n\nExample\nThis code cluster by nl.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nJust like the heteroskedasticity-present case before,\n\nEstimate the model using OLS (you do nothing special here)\nAssume the error term is clustered and/or heteroskedastic, and estimate the variance of the OLS estimators \\((Var(\\widehat{\\beta}))\\) using cluster-robust standard error estimators\nReplace the estimates from \\(\\widehat{Var(\\widehat{\\beta})}_{default}\\) with those from \\(\\widehat{Var(\\widehat{\\beta})}_{cl}\\) for testing\nBut, we do not replace coefficient estimates."
  },
  {
    "objectID": "lectures/06-se-estiation/06-se-estimation.html#but-does-it-really-work",
    "href": "lectures/06-se-estiation/06-se-estimation.html#but-does-it-really-work",
    "title": "06: Standard Error Estimation",
    "section": "But does it really work?",
    "text": "But does it really work?\nLet’s run MC simulations to see if the use of the cluster-robust standard error estimation method works\n\nMC simulation (R)MC simulation resultsMore groupsMC simulation results\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nWell, we are still rejecting too often than we should, but it is much better than the default VCOV estimator that rejected 74% of the time.\n\n\n\n\nImportant\n\n\n\nCluster-robust standard error estimation gets better as the number of groups gets larger\nThe number of groups of 2 is too small (the MLB case)\nAs a rule of thumb, # of groups larger than 50 is sufficiently large, but we just saw we still over-rejected the null of \\(\\beta = 0\\) three times more than we should.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBetter. But, we are still over-rejecting. Don’t forget it is certainly better than using the default!\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#what-variables-to-include-or-not",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "What variables to include or not",
    "text": "What variables to include or not\nYou often\n\nface the decision of whether you should be including a particular variable or not:  how do you make a right decision? \nmiss a variable that you know is important because it is not simply available:  what are the consequences? \n\nTwo important concepts you need to be aware of:\n\nMulticollinearity\nOmitted Variable Bias"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#multicollinearity-and-omitted-variable-bias",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Multicollinearity and Omitted Variable Bias",
    "text": "Multicollinearity and Omitted Variable Bias\n\nDefinitionObjectiveCases we look atKey consequences of interest\n\n\n\n\n\n\nDefinition: Multicollinearity\n\n\nA phenomenon where two or more variables are highly correlated (negatively or positively) with each other ( consequences? )\n\n\n\n\n\n\n\n\n\nDefinition: Omitted Variable Bias\n\n\nBias caused by not including (omitting)  important  variables in the model\n\n\n\n\n\n\nConsider the following model,\n\\[\ny_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\n\\]\nYour interest is in estimating the impact of \\(x_1\\) on \\(y\\).\n\n\n\n\nObjective\n\n\nUsing this simple model, we investigate what happens to the coefficient estimate on \\(x_1\\) if you include/omit \\(x_2\\).\n\n\n\n\n\n\nThe model: \\[y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\]\nCase 1:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 2:\nWhat happens if \\(\\beta_2=0\\), but include \\(x_2\\) that is highly correlated with \\(x_1\\)?\nCase 3:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is not correlated with \\(x_1\\)?\nCase 4:\nWhat happens if \\(\\beta_2\\ne 0\\), but omit \\(x_2\\) that is highly correlated with \\(x_1\\)?\n\n\n\nIs \\(\\widehat{\\beta}_1\\) unbiased, that is \\(E[\\widehat{\\beta}_1]=\\beta_1\\)?\n\\(Var(\\widehat{\\beta}_1)\\)? (how accurate the estimation of \\(\\widehat{\\beta}_1\\) is)"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-1",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 1",
    "text": "Case 1\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\).)\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either of \\(x_2\\) and \\(u\\). So, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 on average because \\(cor(x_1, x_2)=0\\)\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nThey are the same because all the components are the same.\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\) and is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) will be the same as when you do not include \\(x_2\\) as a covariate (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)) and is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-2",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 2",
    "text": "Case 2\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because\n\n\\(x_1\\) is correlated with \\(x_2\\), but \\(\\beta_2 = 0\\).\n\\(x_1\\) is not correlated with \\(u\\)\n\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = Var(\\beta_2 x_i + u_i) = \\sigma_u^2\n\\] because \\(\\beta_2 = 0\\).\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2=0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 &gt; 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include an irrelevant variable that has no explanatory power beyond \\(x_1\\), but is highly correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is larger compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an irrelevant variable that has no explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is highly correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-3",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 3",
    "text": "Case 3\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nYes, because \\(x_1\\) is not correlated with either \\(x_2\\) or \\(u\\).\nSo, no bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) = 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is (on average) zero because \\(x_1\\) and \\(x_2\\) are not correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is zero (on average).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j)= \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nSo, \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\n\n\n\n\nIf you include a variable that has some explanatory power beyond \\(x_1\\), but is not correlated with \\(x_1\\) (\\(EE_2\\)), then the variance of the OLS estimator on \\(x_1\\) is smaller compared to when you do not include \\(x_2\\) (\\(EE_1\\))\nIf you omit an variable that has some explanatory power beyond \\(x_1\\) (\\(EE_1\\)), but is not correlated with \\(x_1\\), then the the OLS estimator on \\(x_1\\) is still unbiased"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#case-4",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Case 4",
    "text": "Case 4\n\nSetupQuestionMC SimulationMC ResultsInsights: BiasInsights: VarianceSummary\n\n\nTrue Model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\n\n\n\nExample\n\n\n\\(\\mbox{corn yield} = \\beta_0 + \\beta_1 \\times N + \\beta_2 \\mbox{farmers' height} + u\\)\n\n\n\n\n\n\nWe will estimate the following models:\n\n\\(EE_1\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + v_i \\mbox{ , where } (v_i = \\beta_2 x_{2,i} + u_i)\\)\n\\(EE_2\\): \\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n(Only \\(x_1\\) is included in \\(EE_1\\), while \\(x_1\\) and \\(x_2\\) are included in \\(EE_2\\))\n\n\n\n\n\nQuestion\n\n\nWhat do you think is gonna happen? Any guess?\n\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_1\\)? (bias?)\n\\(E[\\widehat{\\beta}_1]=\\beta_1\\) in \\(EE_2\\)? (bias?)\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\n\nSet up simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nRun MC simulations:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVisualize the results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(EE_1\\)\\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (v_i = \\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[v_i|x_{1,i}]=0?\\)\n\n\n\nAnswer\n\nNo, because \\(x_1\\) is correlated with \\(x_2\\) and \\(\\beta_2 \\ne 0\\).\nSo, there will be bias.\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nEstimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion\n\n\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)?\n\n\n\nAnswer\n\nYes, because \\(x_1\\) and \\(x_2\\) are not correlated with \\(u\\) (by assumption). So, no bias.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(EE_1\\)\\(EE_2\\)\\(EE_1\\) v.s. \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&lt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(&gt;\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_v^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_1\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n0 because there are no other variables included in the model.\n\n\n\n\\(Var(v_i) = Var(\\beta_2 x_i + u_i)\\)?\n\n\n\nAnswer\n\n\\[\\begin{align}\nVar(error) & = Var(v_i) \\\\\n  & = Var(\\beta_2 x_i + u_i) \\\\\n  & = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(cor(x_1,x_2) \\ne 0\\)\n\\(\\beta_2 \\ne 0\\)\n\\(E[u_i|x_{1,i},x_{2,i}]=0\\)\n\n\nVariance\n\\(Var(\\widehat{\\beta}_j)= \\frac{\\sigma_u^2}{SST_j(1-R^2_j)}\\)\nwhere \\(R^2_j\\) is the \\(R^2\\) when you regress \\(x_j\\) on all the other covariates.\n\nThe estimated model\n\\(EE_2\\): \\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\n\n\n\n\n\nQuestion (\\(R_j^2\\))Question (variance of the error term)\n\n\n\\(R_j^2\\)?\n\n\n\nAnswer\n\n\\(R_j^2\\) is non-zero because \\(x_1\\) and \\(x_2\\) are correlated. If you regress \\(x_1\\) on \\(x_2\\), then its \\(R^2\\) is non-zero.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\\(Var(u_i)\\)?\n\n\n\nAnswer\n\n\\[\nVar(error) = Var(u_i) = \\sigma_u^2\n\\]\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) \\(\\gtreqqless\\) \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\)?\n\n\n\n\n\n\n\n\\(EE_1\\)\n\n\\(R_j^2 = 0\\)\n\\(Var(error) = Var(v_i) = \\beta_2^2\\cdot Var(x_i) + \\sigma_u^2\\)\n\n\n\\(EE_2\\)\n\n\\(R_j^2 \\ne 0\\)\n\\(Var(error) = Var(u_i) = \\sigma_u^2\\)\n\n\nVariance formula\n\\(Var(\\widehat{\\beta}_j) = \\frac{Var(error)}{SST_j(1-R^2_j)}\\)\n\n\n \n\n\nAnswer\n\nIt depends.\n\n\n\nIn the MC simulations we saw,\n\n\\(x_1\\) and \\(x_2\\) are highly correlated, so \\(R_j^2\\) is very high for \\(EE_2\\)\n\n\nx1 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\nx2 &lt;- 0.1 * rnorm(N) + 0.9 * mu # independent variable\n\n\n\nThe impact of \\(x_2\\) (\\(\\beta_2 = 1\\)) and the variance of \\(x_2\\) is small (approximately 1).\n\n\ny &lt;- 1 + x1 + 1 * x2 + u\n\n\nThese conditions led to lower \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_1\\) compared to \\(Var(\\widehat{\\beta}_1)\\) in \\(EE_2\\).\n\n\nNow, let’s reverse the current conditions. We now have:\n\n\\(x_1\\) and \\(x_2\\) are NOT highly correlated, so \\(R_j^2\\) is small for \\(EE_2\\)\nThe impact of \\(x_2\\) (\\(\\beta_2 = 5\\)) and the variance of \\(x_2\\) is large (approximately 5).\n\n\nx1 &lt;- 0.9 * rnorm(N) + 0.1 * mu # independent variable\nx2 &lt;- 2.23 * rnorm(N) + 0.1 * mu # independent variable\ncor(x1, x2)\n\n\n\ny &lt;- 1 + x1 + 5 * x2 + u\n\n\nLet’s rerun MC simulations with this updated data generating process.\n\n\n\n\n\n\n\nThere exists bias-variance trade-off when independent variables are both important (their coefficients are non-zero) and they are correlated\nEconomists tend to opt for unbiasedness"
  },
  {
    "objectID": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "href": "lectures/04-MultiColinearityOmittedVariableBias/OmittedMulticollinear.html#omitted-variable-bias-theory",
    "title": "04: Omitted Variable Bias and Multicollinearity",
    "section": "Omitted Variable Bias (Theory)",
    "text": "Omitted Variable Bias (Theory)\n\nSetupMagnitude and direction of biasExamplesHow does this help?\n\n\nTrue model\n\\(y_i=\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_i\\)\n\n\\(EE_1\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{1,i} + v_{i} \\;\\; (\\beta_2 x_{2,i} + u_{i})\\)\nLet \\(\\tilde{\\beta}_1\\) denote the estimator of \\(\\beta_1\\) from this model\n\n\\(EE_2\\)\n\\(y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + u_{i}\\)\nLet \\(\\widehat{\\beta}_1\\) and \\(\\widehat{\\beta}_2\\) denote the estimator of \\(\\beta_1\\) and \\(\\beta_2\\)\n\nRelationship between \\(x_1\\) and \\(x_2\\)\n\\(x_{1,i} = \\sigma_0 + \\sigma_1 x_{2,i} + \\mu_{i}\\)\n\n\n\n\nImportant\n\n\nThen, \\(E[\\tilde{\\beta}_1] = \\beta_1 + \\beta_2 \\cdot \\sigma_1\\), where \\(\\beta_2 \\cdot \\sigma_1\\) is the bias.\n\n\n\nThat is, if you omit \\(x_2\\) and regress \\(y\\) only on \\(x_1\\), then the bias is going to be the multiple of the impact of \\(x_2\\) on \\(y\\) (\\(\\beta_2\\)) and the impact of \\(x_2\\) on \\(x_1\\) (\\(\\sigma_1\\)).\n\n\nDirection of bias\n\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &gt; 0\\)\n\\(Cor(x_1, x_2) &gt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &gt;0\\), then \\(bias &lt; 0\\)\n\\(Cor(x_1, x_2) &lt; 0\\) and \\(\\beta_2 &lt;0\\), then \\(bias &gt; 0\\)\n\n\nMagnitude of bias\n\nThe greater the correlation between \\(x_1\\) and \\(x_2\\), the greater the bias\nThe greater \\(\\beta_1\\) is, the greater the bias\n\n\n\n\nExample 1Example 2Example 3\n\n\n\\[\n\\begin{aligned}\n\\mbox{corn yield} = \\alpha + \\beta \\cdot N + (\\gamma \\cdot \\mbox{soil erodability}  + \\mu)\n\\end{aligned}\n\\]\n\nFamers tend to apply more nitrogen to the field that is more erodible to compensate for loss of nutrient due to erosion\nSoil erodability affects corn yield negatively \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe city planner placed incinerators in the outskirt of a city to avoid their potentially negative health effects\nDistance to city center has a negative impact on house price \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\\(\\mbox{groundwater use}\\): groundwater use by a farmer for irrigated production\n\\(\\mbox{center pivot}\\): 1 if center pivot is used, 0 if flood irrigation (less effective) is used\n\nFarmers who have relatively low precipitation during the growing season tend to adopt center pivot more\ncenter pivot applied water more efficiently than flood irrigation \\((\\gamma &lt; 0)\\)\n\nWhat is the direction of bias on \\(\\hat{\\beta}\\)?\n\n\n\n\n\n\nWhen the direction of the bias is the  opposite  of the expected coefficient on the variable of interest, you can claim that  even after  suffering from the bias, you are still seeing the impact of the variable interest. So, it is a strong evidence that you would have had an even stronger estimated impact.\n\nExample 1Example 2\n\n\n\\[\n\\begin{aligned}\n\\mbox{groundwater use} = \\alpha + \\beta \\cdot \\mbox{precipitation} + (\\gamma \\cdot \\mbox{center pivot}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-5\\) ( you only observe this )\n\nYou believe the direction of bias is positive (you need provide reasoning behind your belief), and yet, the estimated coefficient is still negative. So, you can be quite confident that the sign of the impact of precipitation is negative. You can say your estimate is a conservative estimate of the impact of precipitation on groundwater use.\n\n\n\\[\n\\begin{aligned}\n\\mbox{house price} = \\alpha + \\beta \\cdot \\mbox{dist to incinerators} + (\\gamma \\cdot \\mbox{dist to city center}  + \\mu)\n\\end{aligned}\n\\]\n\nThe true \\(\\beta\\) is \\(-10\\) ( you do not observe this )\nThe bias on \\(\\widehat{\\beta}\\) is \\(-5\\) ( you do not observe this )\n\\(\\widehat{\\beta}\\) is \\(-15\\) ( you only observe this )\n\nYou believe the direction of bias is negative, and the estimated coefficient is negative. So, unlike the case above, you cannot be confident that \\(\\widehat{\\beta}\\) would have been negative if it were not for the bias (by observing dist to city center and include it as a covariate). It is very much possible that the degree of bias is so large that the estimated coefficient turns negative even though the true sign of \\(\\beta\\) is positive. In this case, there is nothing you can do.\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "The data set and model",
    "text": "The data set and model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSet upR code to get dataData visualization\n\n\nData\nObservations of house price and lot size for 546 houses.\n\nModel\n\\[price_i = \\beta_0 + \\beta_1 lotsize_i+u_i\\]\n\n\\(price_i\\): house price ($) of house \\(i\\)\n\\(lotsize_i\\): lot size of house \\(i\\)\n\\(u_i\\): error term (everything else) of house \\(i\\)\n\n\nObjective\nEstimate the impact of lot size on house price\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Estimation with OLS",
    "text": "Estimation with OLS\n\nRough ideaExamplesResidualsOLSVisualizationDerivationEstimators vs Estimates\n\n\n\nWe want to draw a line like this, the slope of which is an estimate of \\(\\beta_1\\)\nA way: Ordinary Least Squares (OLS)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEx. 1: \\(\\widehat{\\beta}_0=20000\\), \\(\\widehat{\\beta}_1=7\\)Ex. 2: \\(\\widehat{\\beta}_0=70000\\), \\(\\widehat{\\beta}_1=3.8\\)So,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion\n\n\n\nAmong all the possible values of \\(\\beta_0\\) and \\(\\beta_1\\), which one is the best?\nWhat criteria do we use (what does the best even mean?)\n\n\n\n\n\n\n\n\n\n\n\nFor particular values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) you pick, the modeled value of \\(y\\) for individual \\(i\\) is \\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\).\nThen, the residual for individual \\(i\\) is:\n\\[\n\\widehat{u}_i =  y_i - (\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i)\n\\]\nThat is, residual is the observed value of the dependent variable less the value of modeled value. For different values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\), you have a different value of residual.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIdea of OLS (Ordinary Least Squares)\nLet’s find the value of \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the sum of the squared residuals!\n\nMathematically\nSolve the following minimization problem:\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n \\widehat{u}_i^2, \\mbox{where} \\;\\; \\widehat{u}_i=y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestions\n\nWhy do we square the residuals, and then sum them up together? What’s gonna happen if you just sum up residuals?\nHow about taking the absolute value of residuals, and then sum them up?\n\n\n\nMinimization problem to solve\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nSteps\n\npartial differentiation of the objective function with respect to \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\nsolve for \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\n\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nFOC\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{align}\n\\frac{\\partial }{\\partial \\widehat{\\beta}_0}=& 2 \\sumn [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]=0 \\\\\\\\\n\\frac{\\partial }{\\partial \\widehat{\\beta}_1}=& 2 \\sumn x_i\\cdot [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]= \\sumn x_i\\cdot \\widehat{u}_i = 0\n\\end{align}\n\\]\nOLS estimators: analytical formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2},\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}, \\\\\\\\\n  \\mbox{where} & \\;\\; \\bar{y} = \\sumn y_i/n \\;\\; \\mbox{and} \\;\\;\\bar{x} = \\sumn x_i/n\n\\end{aligned}\n\\]\n\n\nEstimators\nSpecific  rules (formula)  to use once you get the data\n\nEstimates\nNumbers you get once you plug values (your data) into the formula"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "OLS demonstration in R",
    "text": "OLS demonstration in R\n\nR code: hard wayR code: a better waypost-estimation\n\n\nOLS Estimator Formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\]\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe can use the feols() function from the fixest package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLots of information is stored in the regression results (here, uni_reg), which is of class list.\nApply ls() to see its elements:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimated coefficients:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPredicted values at the observation points:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nResiduals:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can have a nice quick summary of the regression results with summary() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Once the model is estimated",
    "text": "Once the model is estimated\n\nEstimated modelPredicted values (R)New predictions (R)Exercise: The impact of lotsize\n\n\nModel to be estimated\n\\[\nprice = \\beta_0 + \\beta_1 lotsize + u\n\\]\n\nEstimated Model\nThis is the estimated version of the expected value of \\(y\\) conditional on \\(x\\).\n\\[\nprice =  3.4136\\times 10^{4} + 6.599 \\times lotsize\n\\]\nThis is called  sample regression function (SRF) , and it is an estimation of \\(E[price|lotsize]\\), the  population regression function (PRF).\n\n\n\nImportant\n\n\n\nOLS regression predicts the  expected  value of the dependent variable  conditional on the explanatory variables.\n\\(\\widehat{\\beta}_1\\) is an estimate of how a change in \\(x\\) affects the  expected  value of \\(y\\).\n\n\n\n\n\n\nYou can access the predicted values at the observed points by looking at the fitted.value element of the regression results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTo calculate the predicted value at arbitrary values of \\(x\\),\n\ncreate a new data.frame with values of \\(x\\) of your choice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\napply predict() to the data.frame using the regression results.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nProblemSolution\n\n\nYour current lot size is 3000. You are thinking of expanding your lot by 1000 (with everything else fixed), which would cost you 5,000 USD. Should you do it? Use R to figure it out.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "\\(R^2\\): Goodness of fit",
    "text": "\\(R^2\\): Goodness of fit\n\nWhat is it?Decompose \\(y\\)Visualization\\(R^2\\) componentsDefinition of \\(R^2\\)Caveat\n\n\n\\(R^2\\) is a measure of how good your model is in predicting the dependent variable (explaining variations in the dependent variable)  compared  to just using the average of the dependent variable as the predictor.\n\n\nYou can decompose observed value of \\(y\\) into two parts: fitted value and residual\n\\[\ny_i=\\widehat{y}_i +\\widehat{u}_i, \\;\\;\\mbox{where}\\;\\; \\widehat{y}_i = \\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i\n\\]\nnow, subtracting \\(\\bar{y}\\) (sample average of \\(y\\)),\n\\[\ny_i-\\bar{y}=\\widehat{y}_i-\\bar{y}+\\widehat{u}_i\n\\]\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\n\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ntotal sum of squares (SST)\n\\[\nSST\\equiv \\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\]\nexplained sum of squares (SSE) \\[\nSSE\\equiv \\sum_{i=1}^{n}(\\widehat{y}_i-\\bar{y})^2\n\\]\nresidual sum of squares (SSR) \\[\nSSR\\equiv \\sum_{i=1}^{n}\\widehat{u}_i^2\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\\(R^2 = 1 - SSR/SST\\)\n\n\n\n\n\nWhere did it come from?\n\\[\\begin{align}\n& SST = SSE + SSR  \\\\\n\\Rightarrow & SSE = SST - SSR \\\\\n\\Rightarrow & SSE/SST = 1 - SSR/SST = R^2\\\\\n\\end{align}\\]\nThe value of \\(R^2\\) always lies between \\(0\\) and \\(1\\) as long as an intercept is included in the econometric model.\n\nWhat does it measure?\n\\(R^2\\) is a measure of how much improvement  in predictin the depdent variable  you’ve made by including independent variable(s) \\((y=\\beta_0+\\beta_1 x+u)\\) compared to when simply using the mean of dependent variable as the predictor \\((y=\\beta_0+u)\\).\n\n\nImportant\n\n\\(R^2\\) tells  nothing  about how well you have estimated the causal ceteris paribus impact of \\(x\\) on \\(y\\) \\((\\beta_1)\\).\nAs an economist, we typically do not care about how well we can prefict yield, rather we care about how well we have predicted \\(\\beta\\).\n\nProblem\n\nWhile we observe the dependent variable (otherwise you cannot run regression), we cannot observe \\(\\beta_1\\).\nSo, we get to check how good estimated models are in predicting the dependent variable (which we do not care), but we can  never  test whether they have estimated \\(\\beta_1\\) well.\nThis means that we need to carefully examines whether the  assumptions  necessary for good estimation of \\(\\beta_1\\) is satisfied (next topic)."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno\n\nEmail: tmieno2@unl.edu\nOffice: 209 Filley Hall\n\n\n\n\nMona Mosavi:\n\nEmail: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno\n\nEmail: tmieno2@unl.edu\nOffice: 209 Filley Hall"
  },
  {
    "objectID": "syllabus.html#ta",
    "href": "syllabus.html#ta",
    "title": "Personnel",
    "section": "",
    "text": "Mona Mosavi:\n\nEmail: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Personnel",
    "section": "Lectures and Labs:",
    "text": "Lectures and Labs:\n\nLectures: MW 3:00 - 4:30 PM\nLabs: F 1:00 - 2:30 PM"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Personnel",
    "section": "Office Hours:",
    "text": "Office Hours:\nWednesday, 10:00 to 11:30 pm or by appointment"
  },
  {
    "objectID": "syllabus.html#lecture",
    "href": "syllabus.html#lecture",
    "title": "Personnel",
    "section": "Lecture",
    "text": "Lecture\n\nIntroduction to econometrics\nSimple univariate regression\nMonte Carlo Simulation\nMultivariate regression\nMulti-collinearity and omitted variable\nHypothesis Testing\nHetereoskedasticity and robust standard error estimation\nClustered error and bootstrap\nFunctional form and scaling\nDummy variables\nPanel data methods\nCausal Inference\nCausal Inference\nLimited dependent variable"
  },
  {
    "objectID": "syllabus.html#computer-lab-r",
    "href": "syllabus.html#computer-lab-r",
    "title": "Personnel",
    "section": "Computer Lab (R)",
    "text": "Computer Lab (R)\n\nIntroduction to R\nRmarkdown\nData wrangling\nData visualization\nResearch Flow and R"
  },
  {
    "objectID": "lectures/08-asymptotics/08-asymptotics.html#ols-asymptotics",
    "href": "lectures/08-asymptotics/08-asymptotics.html#ols-asymptotics",
    "title": "08: OLS Asymptotics",
    "section": "OLS Asymptotics",
    "text": "OLS Asymptotics\nLarge Sample Properties of OLS\n\nProperties of OLS that hold only when the sample size is infinite\n(loosely put) How OLS estimators behave when the number of observations goes  infinite (really large) \n\n\nSmall Sample Properties of OLS\nUnder certain conditions:\n\nOLS estimators are unbiased\nOLS estimators are efficient\n\nThese hold  whatever the sample size is ."
  },
  {
    "objectID": "lectures/08-asymptotics/08-asymptotics.html#consistency-1",
    "href": "lectures/08-asymptotics/08-asymptotics.html#consistency-1",
    "title": "08: OLS Asymptotics",
    "section": "Consistency",
    "text": "Consistency\n\nDefinitionMC simulationMC simualation: Inconsistency\n\n\nVerbally (and very loosely)\nAn estimator is  consistent  if the probability that the estimator produces the true parameter is 1 when sample size is infinite.\n\n\n\nSetupConceptual steps (Pseudo Code)R codeResults\n\n\nOLS estimator of the coefficient on \\(x\\) in the following model with all \\(MLR.1\\) through \\(MLR.4\\) satisfied:\n\\[y_i = \\beta_0 + \\beta_1 x_i + u_i\\]\nwith all the conditions necessary for the unbiasedness property of OLS satisfied.\n\n\n\nGenerate data according to \\(y_i = \\beta_0 + \\beta_1 x_i + u_i\\)\nEstimate the coefficients and store them\nRepeat the above experiment 1000 times\nExamine how the coefficient estimates are distributed\n\nWhat you should see is\nAs \\(N\\) gets larger (more observations), the distribution of \\(\\widehat{\\beta}_1\\) get more tightly centered around its true value (here, \\(1\\)). Eventually, it becomes so tight that the probability you get the true value becomes 1.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConsistency of OLS estimators\nUnder \\(MLR.1\\) through \\(MLR.4\\), OLS estimators are consistent\n\n\n\n\n\n\n\nConceptual steps (Pseudo Code)R codeResults\n\n\nPseudo Code\n\ngenerate data (\\(N\\) observations) according to \\(y_i = \\beta_0 + \\beta_1 x_i + u_i\\) with \\(E[u_i|x_i]\\ne 0\\)\nEstimate the coefficients and store them\nrepeat the above experiment 1000 times\nexamine how the coefficient estimates are distributed\n\nQuestion\nWould the bias disappear as N gets larger?\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/08-asymptotics/08-asymptotics.html#asymptotic-normality-1",
    "href": "lectures/08-asymptotics/08-asymptotics.html#asymptotic-normality-1",
    "title": "08: OLS Asymptotics",
    "section": "Asymptotic Normality",
    "text": "Asymptotic Normality\n\nNormality assumptionCentral Limit Theorem (CLT)DemonstrationOLS estimator\n\n\nWhen we talked about hypothesis testing, we made the following assumption:\n\nNormality assumption\nThe population error \\(u\\) is .blue[independent] of the explanatory variables \\(x_1,\\dots,x_k\\) and is .blue[normally] distributed with zero mean and variance \\(\\sigma^2\\):\n\\(u\\sim Normal(0,\\sigma^2)\\)\n\nRemember\n\nIf the normality assumption is violated, t-statistic and F-statistic we constructed before are no longer distributed as t-distribution and F-distribution, respectively\nSo, whenever \\(MLR.6\\) is violated, our t- and F-tests are invalid\n\n\nFortunately\nYou can continue to use t- and F-tests because (slightly transformed) OLS estimators are .blue[approximately] normally distributed when the sample size is .blue[large enough].\n\n\n\n\n\n\nCentral Limit Theorem\n\n\nSuppose \\(\\{x_1,x_2,\\dots\\}\\) is a sequence of idetically independently distributed random variables with \\(E[x_i]=\\mu\\) and \\(Var[x_i]=\\sigma^2&lt;\\infty\\). Then, as \\(n\\) approaches infinity,\n\\[\\sqrt{n}(\\frac{1}{n} \\sum_{i=1}^n x_i-\\mu)\\overset{d}{\\longrightarrow} N(0,\\sigma^2)\\]\n\n\n\n\n\nVerbally\nSample mean less its expected value multiplied by \\(\\sqrt{n}\\) (square root of the sample size) is going to be distributed as Normal distribution as \\(n\\) goes infinity where its expected value is 0 and variance is the variance of \\(x\\).\n\n\n\nExampleDemonstration\n\n\n\n\nSetup\nConside a random variable (\\(x\\)) that follows Bernouli distribution with \\(p = 0.3\\). That is, it takes 0 and 1 with probability of 0.7 and 0.3, respectively.\n\\(x_i \\sim Bern(p = 0.3)\\)\n\n\\(\\mu = E[x_i] = p = 0.3\\)\n\\(\\sigma^2 \\equiv Var[x_i] = p(1-p) = 0.21\\)\n\n\nThis is what 10 random draws and the transformed version of their sum look like:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nAccording to CLT\n\\(\\sqrt{n}(\\frac{1}{n} \\sum_{i=1}^n x_i-\\mu)\\overset{d}{\\longrightarrow} N(0,\\sigma^2)\\)\n\nSo,\n\\(\\sqrt{n}(\\frac{1}{n} \\sum_{i=1}^n x_i-0.3)\\overset{d}{\\longrightarrow} N(0,0.21)\\)\n\n\nIn each of the 5000 iterations, this application draws the number of samples you specify (\\(N\\)) from \\(x_i \\sim Bern(p=0.3)\\) and then calculate \\(\\sqrt{N}(\\frac{1}{N} \\sum_{i=1}^N x_i-0.3)\\). Then the histogram of the 5000 values is presented.\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(bslib)\n\n# Define UI for CLT demonstration\n\nui &lt;- page_sidebar(\n  title = \"CLT demonstration\",\n  sidebar = sidebar(\n    numericInput(\"n_samples\",\n        \"Number of samples:\",\n        min = 1, max = 100000, value = 10, step = 100\n      ),\n    br(),\n    actionButton(\"simulate\", \"Simulate\"),\n    open = TRUE\n  ),\n  plotOutput(\"cltPlot\")\n)\n\n# Define server logic to simulate the Central Limit Theorem\nserver &lt;- function(input, output) {\n  observeEvent(input$simulate, {\n    output$cltPlot &lt;- renderPlot({\n      N &lt;- input$n_samples # number of observations #&lt;&lt;\n      B &lt;- 5000 # number of iterations\n      p &lt;- 0.3 # mean of the Bernoulli distribution\n      storage &lt;- rep(0, B)\n\n      for (i in 1:B) {\n        #--- draw from Bern[0.3] (x distributed as Bern[0.3]) ---#\n        x_seq &lt;- runif(N) &lt;= p\n\n        #--- sample mean ---#\n        x_mean &lt;- mean(x_seq)\n\n        #--- normalize ---#\n        lhs &lt;- sqrt(N) * (x_mean - p)\n\n        #--- save lhs to storage ---#\n        storage[i] &lt;- lhs\n      }\n\n      #--- create a figure to present ---#\n      ggplot() +\n        geom_histogram(\n          data = data.frame(x = storage),\n          aes(x = x),\n          color = \"blue\",\n          fill = \"gray\"\n        ) +\n        xlab(\"Transformed version of sample mean\") +\n        ylab(\"Count\") +\n        theme_bw()\n    })\n  })\n}\n\n# Run the application\nshinyApp(ui = ui, server = server)\n\n\n\n\n\n\n\nAsymptotic normalitytest-statistic (small v.s. large sample)\n\n\nUnder assumptions \\(MLR.1\\) through \\(MLR.5\\), with  any  distribution of the error term (the normality assumption of the error term not necessary), OLS estimator is asymptotically normally distributed.\n\nAsymptotic Normality of OLS\n\\(\\sqrt{n}(\\widehat{\\beta}_j-\\beta_j)\\overset{a}{\\longrightarrow} N(0,\\sigma^2/\\alpha_j^2)\\)\nwhere \\(\\alpha_j^2=plim(\\frac{1}{n}\\sum_{i=1}^n r^2_{i,j})\\), where \\(r^2_{i,j}\\) are the residuals from regressing \\(x_j\\) on the other independent variables.\n\nConsistency\n\\(\\widehat{\\sigma}^2\\equiv \\frac{1}{n-k-1}\\sum_{i=1}^n \\widehat{u}_i^2\\) is a consistent estimator of of \\(\\sigma^2\\) \\((Var(u))\\)\n\n\nSmall sample (any sample size)\nUnder \\(MLR.1\\) through \\(MLR.5\\) .blue[and] \\(MLR.6\\) \\((u_i\\sim N(0,\\sigma^2))\\),\n\n\\((\\widehat{\\beta}_j-\\beta_j)/se(\\widehat{\\beta}_j) \\sim N(0,1)\\)\n\\((\\widehat{\\beta}_j-\\beta_j)/\\widehat{se(\\widehat{\\beta}_j)} \\sim t_{n-k-1}\\)\n\n\nLarge sample (when \\(n\\) goes infinity)\nUnder \\(MLR.1\\) through \\(MLR.5\\) .blue[without] \\(MLR.6\\),\n\n\\((\\widehat{\\beta}_j-\\beta_j)/se(\\widehat{\\beta}_j) \\overset{a}{\\longrightarrow} N(0,1)\\)\n\\((\\widehat{\\beta}_j-\\beta_j)/\\widehat{se(\\widehat{\\beta}_j)} \\overset{a}{\\longrightarrow} N(0,1)\\)"
  },
  {
    "objectID": "lectures/08-asymptotics/08-asymptotics.html#testing-under-large-sample",
    "href": "lectures/08-asymptotics/08-asymptotics.html#testing-under-large-sample",
    "title": "08: OLS Asymptotics",
    "section": "Testing under large sample",
    "text": "Testing under large sample\n\nHow?t-distribution and normal distributionnon-homosketastic error case\n\n\nIt turns out,\nYou can proceed exactly the same way as you did before (practically speaking)!!\n\ncalculate \\((\\widehat{\\beta}_j-\\beta_j)/\\widehat{se(\\widehat{\\beta}_j)}\\)\ncheck if the obtained value is greater than (in magnitude) the critical value for the specified significance level under \\(t_{n-k-1}\\)\n\n\nBut,\nShouldn’t we use \\(N(0,1)\\) when you find the critical value?\n\n\n\n\n\n\n\n\n\n\n\n\nSince \\(t_{n-k-1}\\) and \\(N(0,1)\\) are almost identical when \\(n\\) is large, there is very little error in using \\(t_{n-k-1}\\) instead of \\(N(0,1)\\) to find the critical value.\n\n\n\n\nThe consistency of the default estimation of \\(\\widehat{Var(\\widehat{\\beta})}\\)  DOES  require the homoskedasticity assumption (MLR.5).\nIn other words, the problem of using the default variance estimator under the hteroskedasticity does not go away even when the sample size is large.\nSo, we should use heteroskedasticity-robust or cluster-robust standard error estimators even when the sample size is large\n\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n00-0: Logistics\n\n\n\n\n00: Introduction to Econometrics\n\n\n\n\n01-1: Univariate Regression: Introduction\n\n\n\n\n01-2: Univariate Regression: OLS Mechanics and Implementation\n\n\n\n\n01-3: Univariate Regression: OLS Small Sample Property\n\n\n\n\n02: Multivariate Regression\n\n\n\n\n03: Monte Carlo Simulation\n\n\n\n\n04: Omitted Variable Bias and Multicollinearity\n\n\n\n\n05: Hypothesis Testing\n\n\n\n\n06: Standard Error Estimation\n\n\n\n\n07: Econometric Modeling\n\n\n\n\n08: OLS Asymptotics\n\n\n\n\n09: Endogeneity\n\n\n\n\n10: Panel Data Methods\n\n\n\n\n11: Dealing with Endogeneity: Instrumental Variable\n\n\n\n\nA1: Final Paper Project Expectation\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "href": "lectures/00-Introduction/00-0-Logistics.html#instructors",
    "title": "00-0: Logistics",
    "section": "Instructors",
    "text": "Instructors\n\n Instructor : Taro Mieno (Office: 209, E-mail: tmieno2@unl.edu)\n Teaching Assistant :\n\nTBD"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "href": "lectures/00-Introduction/00-0-Logistics.html#goals-of-the-course",
    "title": "00-0: Logistics",
    "section": "Goals of the course",
    "text": "Goals of the course\n\nLearn modern introductory econometric theory\nApply econometric theories to real economic problems\nLearn how to use statistical software (R) so you can conduct research independently (without technical help from your advisor)\n\nmanage data\nvisualize data\nrun regressions\ninterpret results"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "href": "lectures/00-Introduction/00-0-Logistics.html#text-books",
    "title": "00-0: Logistics",
    "section": "Text Books",
    "text": "Text Books\nRecommended: Wooldridge, Jeffrey M. 2006. “Introductory Econometrics: A Modern Approach (5th edition).” Mason, OH: Thomson/South-Western."
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "href": "lectures/00-Introduction/00-0-Logistics.html#course-schedule",
    "title": "00-0: Logistics",
    "section": "Course Schedule",
    "text": "Course Schedule\n\nLectures (MW): 3:00-4:30pm\nLab sessions (F): 1:00-2:30pm"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "href": "lectures/00-Introduction/00-0-Logistics.html#grading",
    "title": "00-0: Logistics",
    "section": "Grading",
    "text": "Grading\n\nProblem sets (3 assignments): 30%\nSmall-size midterms (2): 40%\nPaper: 40%"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "href": "lectures/00-Introduction/00-0-Logistics.html#assignments",
    "title": "00-0: Logistics",
    "section": "Assignments",
    "text": "Assignments\nProblem sets + Most questions are from the required text book + Some questions come from what we cover in lab sessions\n** Quarto to do and submit your problem sets** + You are required to present your R codes + You learn how to compile your assignment with your R code written in a document using  Quarto , which will be covered in the second lab session\nCaution + 2nd year students have answers to all the questions I will assign (I will use exactly the same problems because they are really good to learn econometrics) + You are free to copy and paste (or rephrase) the answers for your assignment. I won’t bother to try to tell if you have copied and pasted answers. + However, you are simply doing dis-service to yourself by depriving yourself of learning opportunities + Moreover, your lack of understanding of the material will be clearly manifested on your performance at midterms and final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "href": "lectures/00-Introduction/00-0-Logistics.html#midterms",
    "title": "00-0: Logistics",
    "section": "Midterms",
    "text": "Midterms\nIn-class open-book midterms\n\nMidterm 1: Oct, 9 (M)\nMidterm 2: Nov, 20 (M)"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIn this assignment, + you write a paper with a particular emphasis on econometric analysis using a real world data set + you are encouraged to use the data set you are using for your masters thesis (talk with your advisor) + you need to ensure that you use a  panel  dataset + No presentation of your final paper\nTime line\n\n Oct, 16 : identify a research topic and the data set you will be using, and get an approval from the instructor\n Oct, 23 : paper proposal\n Dec, 15 : final paper"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "href": "lectures/00-Introduction/00-0-Logistics.html#paper-proposal",
    "title": "00-0: Logistics",
    "section": "Paper Proposal",
    "text": "Paper Proposal\nIntroduction + clear identification of what you are trying to find out (research question) + why the research question is worthwhile answering\nSimple Model + dependent variable (the variable to be explained) + explanatory variable (variables to be explain)\nData Source + where you get data"
  },
  {
    "objectID": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "href": "lectures/00-Introduction/00-0-Logistics.html#final-paper-1",
    "title": "00-0: Logistics",
    "section": "Final Paper",
    "text": "Final Paper\nIntroduction\n\nclear identification of what you are trying to find out (research question) [1 point]\nwhy the research question is worthwhile answering [1 point]\n\nData description\n\nthe nature of the data with summary statistics table [1 point]\nvisualize a few key variables in a meaningful way [3 points]\n\nEconometric Methods\nThe  process  of how you end up with the final econometric models and methods. [40 points ( or more )]\n\njustification of your choice of independent variables\npotential endogeneity problems\nwhat did you do to address the endogeneity problems?\njustification of econometric model(s) and method(s)\nidentify appropriate standard error estimation methods\n\nResults, Discussions, and Conclusions + interpret and describe the results [2 points] + implications of the results [1 point] + conclusions [1 point]"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignment 1",
    "section": "",
    "text": "Assignment 1\nInstruction\n\nGo here and download all the files in the folder\nWork on assignment-1-student.qmd\nSubmit both the qmd and resulting html files here\n\nSolution\nGo here and look at assignment-1-gen.html for the answers.\n\n\nAssignment 2\nInstruction\n\nGo here and download all the files in the folder\nWork on assignment-1-student.qmd\nSubmit both the qmd and resulting html files here"
  },
  {
    "objectID": "LabLectures.html",
    "href": "LabLectures.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "Visit here for R lab lecture notes. Chapter 1 through Chapter 4 will be covered in this course."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (MS)",
    "section": "",
    "text": "This website hosts course materials for Applied Econometrics (AECN 896-04) at UNL."
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#introduction-3-points",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#introduction-3-points",
    "title": "A1: Final Paper Project Expectation",
    "section": "Introduction (3 points)",
    "text": "Introduction (3 points)\n\nclear identification of what you are trying to find out (research question) [1.5 points]\nwhy the research question is worthwhile answering [1.5 points]"
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#data-description-and-exploration-7-points",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#data-description-and-exploration-7-points",
    "title": "A1: Final Paper Project Expectation",
    "section": "Data description and exploration (7 points)",
    "text": "Data description and exploration (7 points)\n\nthe nature of the data with summary statistics table [2 point]\nvisualize a few key variables in a  meaningful  way [5 points]\n\nImportant\nDo not just present summary statistics and graphs. Discuss what you get out of them. Without any such discussions, you will get \\(0\\)."
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#econometric-methods-35-points",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#econometric-methods-35-points",
    "title": "A1: Final Paper Project Expectation",
    "section": "Econometric methods (35 points)",
    "text": "Econometric methods (35 points)\nThe  process and thought history  of how you end up with the final econometric models and methods."
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#results-discussions-and-conclusions-5-points",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#results-discussions-and-conclusions-5-points",
    "title": "A1: Final Paper Project Expectation",
    "section": "Results, Discussions, and Conclusions (5 points)",
    "text": "Results, Discussions, and Conclusions (5 points)\n\ninterpret and describe the results [3 points]\nconclusions [2 points]"
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#data-generating-process",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#data-generating-process",
    "title": "A1: Final Paper Project Expectation",
    "section": "Data generating process",
    "text": "Data generating process\n\nWhat variables are involved in explaining the dependent variable?\n\nList all the variables both observed and  unobserved .\n\nHow are they related with each other?\n\nMulticollinearity?\n\nWhat would be the appropriate functional form?\n\nNon-linear impact (e.g., quadratic, log)?\nInteractions terms?\nStructural difference?"
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#endogeneity-and-econometric-methods",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#endogeneity-and-econometric-methods",
    "title": "A1: Final Paper Project Expectation",
    "section": "Endogeneity and Econometric Methods",
    "text": "Endogeneity and Econometric Methods\n\nExtensive discussions on why you may have endogeneity problems. What are the sources?\n\nOmitted variable?\nSelection bias?\nReverse causality?\nMeasurement errors?\n\nHypothetical discussion of what econometric methods you can use to deal with the endogeneity problem\n\nRandomization of the variable of interest?\nPanel data approach?\nInstrumental variable approach?\nDifference in difference?\n\nDetailed descriptions of what you end up doing (explain the process)\n\nWhy do you think your approach address the endogeneity problem you identified above or not? ( It is not at all a problem that you cannot solve the endogeneity problem entirely )\n\nUse the appropriate standard error estimation approach (heterogeneity, clustered error, etc)"
  },
  {
    "objectID": "lectures/A0-appendix/A0-paper-expectation.html#key-to-writing-a-successful-paper-high-grade",
    "href": "lectures/A0-appendix/A0-paper-expectation.html#key-to-writing-a-successful-paper-high-grade",
    "title": "A1: Final Paper Project Expectation",
    "section": "Key to writing a successful paper (high grade)",
    "text": "Key to writing a successful paper (high grade)\n\nJustify and explain  everything  you did in the paper!!\n\nI tested the joint statistical significance of these interactions terms because …\n\n Re-emphasized:  I do not care about your results. What I care is the  process !!\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Applied Econometrics (AECN 896-004)",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.3.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\ndates_data &lt;- \n  data.table(\n    date = seq( as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by=\"+1 day\")\n  ) %&gt;%\n  .[, day := weekdays(date)]\n\nw_month_data &lt;- dates_data[month(date) == 8,]\n\nimp_dates &lt;- \n  rep(NA, nrow(w_month_data))\n\nlecture_ind &lt;- \n  w_month_data[, day %in% c(\"Monday\", \"Wednesday\") & day(date) &gt;= 19]\n\nlab_ind &lt;- \n  w_month_data[, day %in% c(\"Friday\") & day(date) &gt;= 19 ]\n\n# Add the events to the desired days\nimp_dates[lecture_ind] &lt;- \"Lecture\"\nimp_dates[lab_ind] &lt;- \"Lab\"\n\n# Create a calendar with a legend\n\ntemp &lt;- \n  calendR::calendR(\n    year = 2024, \n    month = 8, \n    special.days = imp_dates,\n    special.col = c(\n      \"lightcyan2\", \"tan\"),\n    weeknames = c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ),\n    mbg.col = \"15\",\n    months.col = \"blue\",\n    legend.pos = \"bottom\"\n  )"
  },
  {
    "objectID": "lectures/09-endogeneity/09-endogeneity.html#endogeneity",
    "href": "lectures/09-endogeneity/09-endogeneity.html#endogeneity",
    "title": "09: Endogeneity",
    "section": "Endogeneity",
    "text": "Endogeneity\nEndogeneity\n\\(E[u|x_k] \\ne 0\\) (the error term is not correlated with any of the independent variables)\n\nEndogenous independent variable\nIf the error term is, .red[for whatever reason], correlated with the independent variable \\(x_k\\), then we say that \\(x_k\\) is an endogenous independent variable.\n\nSources of endogeneity\n\nOmitted variable\nSelection\nReverse causality\nMeasurement error"
  },
  {
    "objectID": "lectures/09-endogeneity/09-endogeneity.html#omitted-variable",
    "href": "lectures/09-endogeneity/09-endogeneity.html#omitted-variable",
    "title": "09: Endogeneity",
    "section": "Omitted Variable",
    "text": "Omitted Variable\nTrue Model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 ablility + u\\)\n\nIncorrectly specified (your) model\n\\(log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper  + v \\;\\;(u + \\beta_3 ablility)\\)"
  },
  {
    "objectID": "lectures/09-endogeneity/09-endogeneity.html#bias-from-self-selection",
    "href": "lectures/09-endogeneity/09-endogeneity.html#bias-from-self-selection",
    "title": "09: Endogeneity",
    "section": "Bias from self-selection",
    "text": "Bias from self-selection\n\nContextAdoption decisionCommon factorsSo,Omitted variable bias\n\n\nResearch Question\nDoes a soil moisture sensor reduce water use for farmers?\n\nData\nObservational (non-experimental) data on soil moisture sensor adoption and irrigation amount\n\nModel of interest\n\\(irrigation  = \\beta_0 + \\beta_1 sensor + u\\)\n\n\\(irrigation\\): the amount of irrigation by the farmer\n\\(sensor\\): dummy variable that indicates whether the farmer has adopted soil moisture sensor or not\n\n\nQuestion\nIs \\(sensor\\) endogenous (is \\(sensor\\) correlated with the error term)?\n\n\nFarmers do not just randomly adopt a soil moisture sensor, they consider available information to determine it is beneficial for them to adopt it or not.\n\nAdoption (selection) equation\n\\(sensor  = \\beta_0 + \\beta_1 x_2 + \\dots + \\beta_k x_k + v\\)\n\nQuestion\nWhat would be variables that farmers look at when they decide whether they should get a soil moisture sensor or not?\n\nQuestion\nAre any of the variables listed above also affect irrigation demand?\n\n\nExample\nSoil quality/type (hard to accurately measure)\n\nfarmers whose fields are sandy are more likely to adopt a soil moisture sensor (this is just a conjecture)\nfarmers whose fields are sandy are likely to use more water\n\n\nKey\nSoil quality/type affect  both  the decision of soil moisture sensor adoption and irrigation.\n\n\\(sensor\\) is a function of soil quality/type\n\\(irrigation\\) is a function of soil quality/type, which is in the error term uncontrolled for\n\n\n\n\\[\n\\begin{aligned}\nirrigation  = \\beta_0 + \\beta_1 sensor(\\mbox{soil type}) + u\\;\\;(= \\beta_s \\mbox{soil type} + v)\n\\end{aligned}\n\\] where \\(v\\) include all the unobservable variables except soil type.\n\n\n\nNote\n\n\nSo, \\(sensor\\) and the error term in the irrigation model are correlated through soil type, leading to biased estimation of the impact of a sensor.\n\n\n\n\n\n\n\n\nNote\n\n\nSelection bias is a form of omitted variable bias.\n\n\n\n\nIf you accurately measure the common factors in the two equations, you can simply include them explicitly in the main model.\nFor example,\n\\[\n\\begin{aligned}\nirrigation  = \\beta_0 + \\beta_1 sensor(\\mbox{soil type}) + \\beta_s \\mbox{soil type} + u\n\\end{aligned}\n\\]\nThis will get the common factor (soil type) out of the error in the main model, which means the adoption variable and the error term are no longer correlated in the main model."
  },
  {
    "objectID": "lectures/09-endogeneity/09-endogeneity.html#reverse-causality",
    "href": "lectures/09-endogeneity/09-endogeneity.html#reverse-causality",
    "title": "09: Endogeneity",
    "section": "Reverse Causality",
    "text": "Reverse Causality\n\nContextTreatment decisionReverse causalityAnother example\n\n\nResearch Question\nDoes a particular type of medical treatment improve health?\n\nData\nObservational (non-experimental)  cross-sectional  data on a particular type of medical treatment and health. Whether patients get the treatment or not is not randomized, rather it is determined by doctors (like in the real world).\n\nModel\n\\(health  = \\beta_0 + \\beta_1 treatment + u\\)\n\n\\(health\\): indicator of the health of patients\n\\(treatment\\): dummy variable that indicates whether the patient is treated or not\n\nThis model basically compares the health of patients who have and have not had the treatment (no before-after comparison, yes this is dumb).\n\nQuestion\nIs \\(treatment\\) endogenous? (Is \\(treatment\\) correlated with the error term?)\n\n\nQuestion\nHow do doctors decide whether to put their patients under a medical treatment?\nAnswer\nPatients’ health condition!!!\n\nSelection (treatment decision) model\n\\(\\mbox{treatment}  = \\alpha_0 + \\alpha_1 \\mbox{health} + u\\)\nLess healthy people are more likely to be treated.\n\n\nConsequence\n\\(health  = \\beta_0 + \\beta_1 treatment( = \\alpha_0 + \\alpha_1 \\mbox{health} + u) + u\\)\n\\(treatment\\) is endogenous because it is a function of health itself (\\(treatment\\) is a function of \\(u\\), so correlated with the error term)!\n\nReverse Causality\nThis type of endogeneity problem is called  reverse causality  because the independent variable of interest is causally affected by the dependent variable even though your interest is in the estimation of the impact of the independent variable on the dependent variable.\n\n\n\nContextReverse causality\n\n\nContext\n\nUnder the Clean Water Act, some of those who discharge wastes into water (e.g., oil refinery) need to comply with water quality criteria of their discharges set under the law.\nEPA (Environmental Protection Agency) can take enforcement actions (e.g., financial penalties) to those who violate the requirements.\n\nResearch Question\nAre enforcement actions effective in improving the water quality of waster discharges?\n\nData\nAnnual data on\n\nwater quality measures of waster discharges by individual firms\nenforcement actions taken on firms by EPA\n\n\n\nModel of Interest\n\\(\\mbox{water quality}  = \\beta_0 + \\beta_1 \\mbox{enforcement actions} + u\\)\n\nSelection (enforcement decision) model\n\\(\\mbox{enforcement actions}  = \\beta_0 + \\beta_1 \\mbox{water quality} + u\\)\n\nwater quality is affected by enforcement actions\nenforcement actions is affected by water quality\n\nConsequence\nenforcement actions is endogenous because it is a function of water quality itself!"
  },
  {
    "objectID": "lectures/09-endogeneity/09-endogeneity.html#measurement-error",
    "href": "lectures/09-endogeneity/09-endogeneity.html#measurement-error",
    "title": "09: Endogeneity",
    "section": "Measurement Error",
    "text": "Measurement Error\n\nWhat is it?ME in the Dependent VariableME in Independent Variables\n\n\nDefinition\nInaccuracy in the values observed as opposed to the actual values\n\nExamples\n\nreporting errors (any kind of survey has the potential of mis-reporting)\n\nhousehold survey on income and savings\nsurvey on rice yield by farmers in developing countries\n\nthe use of estimated values\n\nspatially interpolated weather conditions (precipitation)\nimputed irrigation costs\n\n\nQuestion\nWhat are the consequences of having measurement errors in variables you use in regression?\n\n\nTrue Model\n\\(y^*=  \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k + u\\)\nwith MLR.1 through MLR.6 satisfied \\((u\\) is not correlated with any of the independent variables).\n\nMeasurement Errors\nThe difference between the observed \\((y)\\) and actual values \\(y^*\\)\n\\(e = y-y^*\\)\n\nEstimable Model\nPlugging the second equation into the first equation, your model is\n\\(y =  \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k + v, \\;\\;\\mbox{where}\\;\\; v = (u + e)\\)\n\nQuestion\nWhat are the conditions under which OLS estimators are unbiased?\n\n\n\nAnswer\n\n\\(E[e|x_1, \\dots, x_k] = 0\\)\nSo, as long as the measurement error is uncorrelated with the independent variables, OLS estimators are still unbiased.\n\n\n\n\nSetupClassical errors-in-variables (CEV)Direction of bias\n\n\nTrue Model\nConsider the following general model\n\\(y =  \\beta_0 + \\beta_1 x_1^* + u\\)\nwith MLR.1 through MLR.6 satisfied.\n\nMeasurement Errors\nThe difference between the observed \\((x_1)\\) and actual values \\((x_1^*)\\)\n\\(e_1 = x_1-x_1^*\\)\n\nEstimable Model\nPlugging the second equation into the first equation,\n\\(y =  \\beta_0 + \\beta_1 x_1 + v, \\;\\;\\mbox{where}\\;\\; v = (u - \\beta e_1)\\)\n\nQuestion\n\\(E[e_1|x_1] = 0\\) needs to be satisfied for the OLS estimators to be unbiased. Does this hold?\n\n\n\n\n\n\nDefinition\n\n\nThe correctly observed variable \\((x_1^*)\\) is uncorrelated with the measurement error \\((e_1)\\):\n\\(Cov(x_1^*, e_1) = 0\\)\n\n\n\n\nUnfortunately, \\(E[e_1|x_1] = 0\\) never holds with CEV. The incorrectly observed variable \\((x_1)\\) must be correlated with the measurement error \\((e_1)\\):\n\\[\\begin{align*}\nCov(x_1,e_1) & = E[x_1 e_1]-E[x_1]E[e_1] \\\\\n& = E[(x_1^*+e_1)e_1]-E[x_1^*+e_1)]E[e_1] \\\\\n& = E[x_1^*e_1+e_1^2]-E[x_1^*+e_1)]E[e_1] \\\\\n& = \\sigma_{e_1}^2 =\\sigma_{e_1}^2\n\\end{align*}\\]\nSo, the mis-measured variable \\((x_1)\\) is always correlated with the measurement error \\((e_1)\\).\n\n\nQuestion\nSo, what is the direction of the bias?\n\nNote\nThe sign of the bias on \\(x_1\\) is the sign of the correlation between \\(x_1\\) and \\(v = (u - \\beta e_1)\\).\n\nDirection of bias\n\nCorrelation between \\(x_1\\) and \\(u\\) is zero\nThe sign of the correlation between \\(x_1\\) and \\(e_1\\) is positive (see the previous slide), which means that the sign of the correlation between \\(x_1\\) and \\(- \\beta e_1\\) is the sign of \\(- \\beta\\).\n\nif \\(\\beta &gt; 0\\), then the sign of the bias is negative\nif \\(\\beta &lt; 0\\), then the sign of the bias is positive\n\n\n\n\n\n\n\nAttenuation Bias\n\n\n\nSo, the bias is such that your estimate of the coefficient on \\(x_1\\) is biased toward 0.\nIn other words, your estimated impact of a mis-measured independent variable will look less influential than it actually is\n\n(Imagine you mislabeled the treatment status of your experiment)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Small sample property of OLS estimators",
    "text": "Small sample property of OLS estimators\n\nSmall sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\widehat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS estimator",
    "text": "Unbiasedness of OLS estimator\n\nUnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nDirection\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\widehat{\\beta}_1]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\widehat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\nLinear in Parameters\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\nRandom sampling\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\nVariation in covariates\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\nRoughly speaking\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\widehat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\widehat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\widehat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\widehat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\widehat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\widehat{\\beta}_1|x] = & E[\\widehat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS in practice",
    "text": "Unbiasedness of OLS in practice\n\nGood empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Variance of OLS estimator",
    "text": "Variance of OLS estimator\n\nIntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\widehat{\\beta}_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\widehat{\\beta}_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\widehat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\widehat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\widehat{\\beta}_1) = \\sqrt{Var(\\widehat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Efficiency of OLS Estimators",
    "text": "Efficiency of OLS Estimators\n\nNature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the variance of error",
    "text": "Estimating the variance of error\n\nWhy?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\widehat{u}_i^2\\)\n\n\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\nNote\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-introduction-1",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Introduction",
    "text": "Monte Carlo Simulation: Introduction\n\nWhat is ti?QuestionKey part of MC simulations\n\n\nIt is a way to test econometric theories via simulation.\n\nHow is it used in econometrics?\n\nconfirm ecoometric theory numerically\n\nOLS estimators are unbiased if \\(E[u|x]=0\\) along with other conditions (theory)\nI know the above theory is right, but let’s check if it is true numerically\n\nYou kind of sense that something in your data may cause problems, but there is no proven econometric theory about what’s gonna happen (I used MC simulation for this purpose a lot)\nassist students in understanding econometric theories by providing actual numbers instead of a series of Greek letters\n\n\n\nSuppose you are interested in checking what happens to OLS estimators if \\(E[u|x]=0\\) (the error term and \\(x\\) are not correlated) is violated.\n\n\n\n\nQuestion\n\n\nCan you use the real data to do this?\n\n\n\n\n\n\n\n\nAnswer\n\nNo because you will never observe either error term or true value of \\(\\beta\\)s.\n\n\n\n You  generate data (you have control over how data are generated)\n\nYou know the true parameter unlike the real data generating process\nYou can change only the part that you want to change about data generating process and econometric methods with everything else fixed"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#generating-data",
    "title": "03: Monte Carlo Simulation",
    "section": "Generating data",
    "text": "Generating data\n\nRNGPseudo?Normal Distribution\n\n\nPseudo random number generators (Pseudo RNG)\nAlgorithms for generating a sequence of numbers whose properties  approximate  the properties of sequences of random numbers\n\nExamples\nDraw from a uniform distribution:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nNumbers drawn using pseudo random number generators are not truly random\n\nWhat numbers you will get are pre-determined\nWhat numbers you will get can be determined by setting a  seed \n\nDemonstration\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nQuestion\nWhat benefits does setting a seed have?\n\n\n\n\n\\(x \\sim N(0, 1)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\\(x \\sim N(2, 2)\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions-for-often-used-distributions",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#r-functions-for-often-used-distributions",
    "title": "03: Monte Carlo Simulation",
    "section": "R functions for often-used distributions",
    "text": "R functions for often-used distributions\n\nDistribution typesdnorm()pnorm()qnorm()\n\n\n\n\n\nNormal\nUniform\nBeta\nChi-square\nF\nLogistic\nLog-normal\nmany others\n\n\nFor each distribution, you have four different kinds of functions:\n\n dnorm: density function\n pnorm: distribution function\n qnorm: quantile function\n rnorm: random draw\n\n\n\n\n\n\ndnorm(x) gives you the height of the density function at \\(x\\).\ndnorm(-1) and dnorm(2)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\npnorm(x) gives you the probability that a single random draw is  less  than \\(x\\).\n\npnorm(-1)pnorm(2)Exercise\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the probability that a single random draw from a Normal distribution with mean = 1 and sd = 2 is less than 1?\n\nWork here\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nAnswer\n\npnorm(1, mean = 1, sd = 2) \n\n\n\n\n\n\n\n\nWhat is it?qnorm(0.95)Exercise\n\n\nqnorm(x), where \\(0 &lt; x &lt; 1\\), gives you a number \\(\\pi\\), where the probability of observing a number from a single random draw is less than \\(\\pi\\) with probability of \\(x\\).\nWe call the output of qnorm(x), \\(x%\\) quantile of the standard Normal distribution (because the default is mean = 0 and sd = 1 for rnorm()).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhat is the 88% quantile of Normal distribution with mean = 0 and sd = 9?\n\nWork hereAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nCode\nqnorm(0.88, mean = 0, sd = 9)"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-steps",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Steps",
    "text": "Monte Carlo Simulation: Steps\n\nspecify the data generating process\ngenerate data based on the data generating process\nget an estimate based on the generated data (e.g. OLS, mean)\nrepeat the above steps many many times\ncompare your estimates with the true parameter\n\nQuestion\nWhy do the steps \\(1-3\\) many many times?"
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-1",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 1",
    "text": "Monte Carlo Simulation: Example 1\n\nProblemSteps 1-3Sample Mean: Step 4Loop: for loopStep 4Step 5\n\n\n\n\n\n\nQuestion\n\n\nIs sample mean really an unbiased estimator of the expected value?\n\n\n\n\n\nThat is, is \\(E[\\frac{1}{n}\\sum_{i=1}^n x_i] = E[x]\\), where \\(x_i\\) is an independent random draw from the same distribution,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nrepeat the above steps many times\nWe use a  loop  to do the same (similar) thing over and over again\n\n\n\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nVerbally\nFor each of \\(i\\) in \\(1:B\\) \\((1, 2, \\dots, 1000)\\), do print(i).\n\ni takes the value of 1, and then print(1)\ni takes the value of 2, and then print(2)\n…\ni takes the value of 999, and then print(999)\ni takes the value of 1000, and then print(1000)\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nCompare your estimates with the true parameter\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-2",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 2",
    "text": "Monte Carlo Simulation: Example 2\n\nQuestionR codeCompare\n\n\n\n\n\n\nQuestion\n\n\nWhat happens to \\(\\beta_1\\) if \\(E[u|x]\\ne 0\\) when estimating \\(y=\\beta_0+\\beta_1 x + u\\)?\n\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#monte-carlo-simulation-example-3-optional",
    "title": "03: Monte Carlo Simulation",
    "section": "Monte Carlo Simulation: Example 3 (optional)",
    "text": "Monte Carlo Simulation: Example 3 (optional)\n\nQuestionR codeCheckDistribution\n\n\nModel\n\\[\\begin{aligned}\n    y = \\beta_0 + \\beta_1 x + u \\\\\n\\end{aligned}\\]\n\n\\(x\\sim N(0,1)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u|x]=0\\)\n\n\nVariance of the OLS estimator\nTrue Variance of \\(\\hat{\\beta_1}\\): \\(V(\\hat{\\beta_1}) = \\frac{\\sigma^2}{\\sum_{i=1}^n (x_i-\\bar{x})^2} = \\frac{\\sigma^2}{SST_X}\\)\nIts estimator: \\(\\widehat{V(\\hat{\\beta_1})} =\\frac{\\hat{\\sigma}^2}{SST_X} = \\frac{\\sum_{i=1}^n \\hat{u}_i^2}{n-2} \\times \\frac{1}{SST_X}\\)\n\nQuestion\nDoes the estimator really work? (Is it unbiased?)\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nTrue Variance\n\n\\(SST_X = 112.07\\)\n\\(\\sigma^2 = 4\\)\n\n\\[V(\\hat{\\beta}) = 4/112.07 = 0.0357\\]\n\nCheck\nYour Estimates of Variance of \\(\\hat{\\beta_1}\\)?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "href": "lectures/03-monte-carlo-simulation/03-mc-simulation.html#exercise-optional",
    "title": "03: Monte Carlo Simulation",
    "section": "Exercise (optional)",
    "text": "Exercise (optional)\n\nProblemSolutionResults visualization\n\n\nUsing MC simulations, find out how the variation in \\(x\\) affects the OLS estimators\n\nModel setup\n\\[\\begin{align}\n  y = \\beta_0 + \\beta_1 x_1 + u \\\\\n  y = \\beta_0 + \\beta_1 x_2 + u\n\\end{align}\\]\n\n\\(x_1\\sim N(0,1)\\) and \\(x_2\\sim N(0,9)\\)\n\\(u\\sim N(0,1)\\)\n\\(E[u_1|x]=0\\) and \\(E[u_2|x]=0\\)\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "href": "lectures/00-Introduction/00-1-Introduction.html#what-econometrics-is-about",
    "title": "00: Introduction to Econometrics",
    "section": "What econometrics is about",
    "text": "What econometrics is about\n\nWhat?Steps in Econometric Analysis\n\n\nWhat are we doing?\nEstimate quantitative relationships between variables.\n\nExamples\n\nthe impact of fertilizer on crop yield\nthe impact of political campaign expenditure on voting outcomes\nthe impact of education on wage\n\n\n\n\nformulation of the question of interest (what are you trying to find out?)\ndevelop an economic model of the phenomenon you are interested in understanding (identify variables that matter)\nturn the economic model into an econometric model\ncollect data\n estimate the model using econometrics \n test hypotheses"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "href": "lectures/00-Introduction/00-1-Introduction.html#go-through-the-steps",
    "title": "00: Introduction to Econometrics",
    "section": "Go through the steps",
    "text": "Go through the steps\n\nStep 2: Economic ModelStep 3: Econometric modelStep 4: Collect dataSteps 5 and 6\n\n\nExample: Job training and worker productivity\n\\[wage = f(educ,exper,training)\\]\n\n\\(wage\\): hourly wage\n\\(educ\\): years of formal education\n\\(exper\\): years of workforce experience\n\\(training\\): weeks spent in job training\n\nNote\nDepending on questions you would like to answer, the economic model can (and should) be much more involved\n\n\nWe have built a conceptual model:\n\\[wage = f(educ,exper,training)\\]\nNow, the form of the function \\(f(\\cdot)\\) must be specified (almost always) before we can undertake an econometric analysis\n\\[\nwage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\n\\]\n\\(\\beta_0,\\beta_1,\\beta_2,\\beta_3\\)\n\nare the  parameters  of the econometric model.\ndescribe the directions and strengths of the relationship between \\(wage\\) and the factors used to determine \\(wage\\) in the model\n\n\\(u\\)\n\nis called error term\nincludes  ALL  the other factors that can affect wage other than the included variables (like innate ability)\n\n\n\nWe can collect data using various ways. Some of them include survey, websites, experiments. Let’s look at different data types:\n\nCross-sectional DataTime-series DataPanel (Longitudinal) Data\n\n\n\nSample of individuals, households, firms, cities, states, countries, or a variety of other units, taken at a given point in time\nThe data on all units do not correspond to precisely the same time period\n\nsome families surveyed during different weeks within a year\n\n\nWhat a cross-sectional data looks like on R\n\n\n      wage  educ exper female married\n     &lt;num&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n  1:  3.10    11     2      1       0\n  2:  3.24    12    22      1       1\n  3:  3.00    11     2      0       0\n  4:  6.00     8    44      0       1\n  5:  5.30    12     7      0       1\n ---                                 \n522: 15.00    16    14      1       1\n523:  2.27    10     2      1       0\n524:  4.67    15    13      0       1\n525: 11.56    16     5      0       1\n526:  3.50    14     5      1       0\n\n\n\n\nObservations on a variable or several variables over time + corn price + oil price\n\nNote\n\nThe econometric frameworks necessary to analyze time series data are quite different from those for cross-sectional data\nWe do  NOT  learn time-series econometric methods\n\n\n\nTime series data for each cross-sectional member in the data set ( same  cross-sectional units are tracked over a given period of time)\nExample\n\nwage data for individuals collected every five years over the past 30 years\nyearly GDP data for 60 countries over the past 10 years\n\nWhat a panel data looks like on R\n\n\n     county  year    crmrte   prbarr  prbpris\n      &lt;int&gt; &lt;int&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n  1:      1    81 0.0398849 0.289696 0.472222\n  2:      1    82 0.0383449 0.338111 0.506993\n  3:      1    83 0.0303048 0.330449 0.479705\n  4:      1    84 0.0347259 0.362525 0.520104\n  5:      1    85 0.0365730 0.325395 0.497059\n ---                                         \n626:    197    83 0.0155747 0.226667 0.428571\n627:    197    84 0.0136619 0.204188 0.372727\n628:    197    85 0.0130857 0.180556 0.333333\n629:    197    86 0.0128740 0.112676 0.244444\n630:    197    87 0.0141928 0.207595 0.360825\n\n\n\n\n\n\n\n\nThis is what you learn for the next few months!!\n\nestimate the model using econometrics\ntest hypothesis"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#causality-and-association-1",
    "title": "00: Introduction to Econometrics",
    "section": "Causality and Association",
    "text": "Causality and Association\n\nDistinction between causality and associationGlasses?\n\n\nAssociation\nAn association of two variables arise because  either of or both  variables affect the other variable\n\\[\\begin{align}\n  A \\longleftrightarrow B \\\\\n  A \\longrightarrow B \\\\\n  A \\longleftarrow B\n\\end{align}\\]\nAssociation does  NOT  concern which affects which. Under all the three cases above, A and B are  associated. Or, we say there is an association between A and B. This is what  correlation coefficient  measures.\n\nCausality\nWhen A has a causal impact on B,\n\\[\nA \\longrightarrow B\n\\]\nHere, changes in \\(A\\) cause changes in \\(B\\), not the other way around\n\n\n\nVideoClaimsBut,\n\n\nLet’s watch this interesting CM.\n\n\nPeople who wear glasses are\n\nmuch smarter than those who don’t\nmore likely to pursue higher education\n200% more likely to graduate college\n\nFor you to be convinced to buy glasses, these claims needs to be causal, not association:\n\nDoes wearing glasses make you much smarter?\nDoes wearing glasses make it more likely for you to pursue higher education?\nDoes wearing glasses make it 200% more likely for you to graduate college?\n\n\n\nHowever, this seems to be a more likely explanation of the association:\n\nOne spends more time studying academic subjects\n\nsmarter (or knowledgeable) \\(\\Rightarrow\\) pursue higher education and graduate college\nworsened eyesight \\(\\Rightarrow\\) wear glasses\n\n\n\n\n\nImportant\n\n\n\nWe care about isolating causal effects, but not association\nIdentifying association is super easy\nIdentifying causal effects is extremely hard (this is what we tackle)"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "href": "lectures/00-Introduction/00-1-Introduction.html#endogeneity-your-nemesis-1",
    "title": "00: Introduction to Econometrics",
    "section": "Endogeneity: Your Nemesis",
    "text": "Endogeneity: Your Nemesis\n\nEndogeneityExampleWhat happened?EndogeneityAnother example\n\n\nIt is super easy to find an association of multiple variables, but it is incredibly hard to find a causal effect (at least in Economics)!!\nThat is due to the problem called  endogeneity , which is going to be defined formally later.\n\n\nYou are interested in the causal impact of fire fighters on the number of death tolls in fire events\n\n\nfire eventdeath toll# of firefighters deployed11020203351043555050\n\n\nQuestions\n\nHow are they  associated ?\nCan you say anything about the causal effect of fire fighters deployment on the number of death tolls?\n\n\n\nYou ignored an important variable!!\n\n\nfire eventdeath toll# of firefighters deployedscale of fire110202020353510204351055050100\n\n\n\n\n\n\n\n\nDefinition\n\n\nVariables of interest are correlated with some  unobservables  (variables that cannot be observed or are missing) that have non-zero impacts on the variable that you want to explain\n\n\n\n\n\nThe unobserved variables are also called  confounder/confounding factor .\n\nThe example\nIn the the firefighter example,\n\n variable of interest : the number of firefighters\n unobservables/confounder : the scale of fire events (and other factors)\n variable to explain : death toll\n\nThe model\n\\[\\begin{align}\n  \\mbox{death toll} & = \\alpha + \\beta\\; \\mbox{# of fire fighters} + \\mu\\\\\n  ,\\mbox{where } \\mu & = (\\gamma\\; \\mbox{scale} + v) \\mbox{ is the error term (collection of unobservables)}\n\\end{align}\\]\nEndogeneity Problem\n# of fire fighters is correlated with scale, which we ignored\n\n\n\n\n\\[wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 training + u\\]\nWhat are unobservables in \\(u\\) that are likely to be correlated with \\(educ\\)?\nAn important unobservable\n\ninnate ability \\(\\Rightarrow\\) wage\ninnate ability \\(\\Rightarrow\\) education"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "href": "lectures/00-Introduction/00-1-Introduction.html#how-to-deal-with-endogeneity",
    "title": "00: Introduction to Econometrics",
    "section": "How to deal with endogeneity",
    "text": "How to deal with endogeneity\n\nThe questionHow to deal with endogeneity?\n\n\nProblem\nMost of the time, you will be faced with endogeneity problems caused by at least one of the followings,\n\nomitted variables (the scale of fire events, innate ability)\nself-selection\nsimultaneity\nmeasurement error\n\nCentral Question\nHow can we avoid or solve endogeneity problems?\n\n\n\nYou have two opportunities to deal with endogeneity problems\n\nat the design (design to collect data) stage\nat the regression stage (what you will learn in this course)\n\nEconometrics has evolved mostly to address endogeneity problems at the  regression stage  because randomized experiments are infeasible most of the time\nHow about econometrics and other fields of statistics: Statistics, Psychometrics, and Biometrics?\n\n\n\nFieldDesignEstimation MethodEconometricsnot feasible (often)intricateMany other fieldsfeasiblerelatively simple"
  },
  {
    "objectID": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "href": "lectures/00-Introduction/00-1-Introduction.html#randomized-experiments",
    "title": "00: Introduction to Econometrics",
    "section": "Randomized-experiments",
    "text": "Randomized-experiments\nIn randomized experiments,\n\nyou have a liberty to determine the level of the variable of interest\nby randomizing the value of the variable of interest, you can effectively break the link (association) with whatever is included in the error term\n\n\nExample (Non-Randomized)RandomizedRandomized Experiments on Education?\n\n\n\nDataFarmer’s decisionBias\n\n\nYield and nitrogen rate data obtained from a field that is managed by a farmer\n\n\n\n\n\n\n\n\n\n\n\nFarmer\n\ndecide nitrogen rate based on soil/field characteristics (some of them we researchers do not get to observe)\n\nResearcher\n\nsoil characteristics is not observable, so it is in the error term\n\n\\[yield = \\beta_0 + \\beta_1 N + (\\gamma SC + \\mu)\\]\n\nN (nitrogen rate) and SC (soil characteristics) are correlated\n\n\n\nSuppose the farmer applied more nitrogen to the area where its soil characteristics lead to higher corn yield\nQuestion If the researcher estimate the model (which ignores soil characteristics), do you over- or under-estimate the impact of nitrogen rate on corn yield?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nSoil quality (in error term) is no longer correlated with N!!\n\n\n\n\n\nRandomized Experiment?\nResearchers determine randomly how much education subjects (people) can get?\nEndogeneity Problem in Economics\n\nEconomics is about understanding human behavior\nAlmost always, you need to deal with endogeneity problem because people are smart: we make decisions based on available information (not just randomly) so that our decisions lead to good outcomes (whether our decisions turn out to be good or not is irrelevant)\n\nhow much education one get is determined based on their judgment of their own ability (not by rolling a dice)\nhow many fire fighters to be deployed was determined based on the scale of fire (not by rolling a dice)\nhow much nitrogen to apply based on soil characteristics (not by rolling a dice)\n\nIf people are not smart and just roll a dice for their decision making, we would have much easier time identifying causal effects"
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#panel-longitudinal-data-methods",
    "href": "lectures/10-panel/10-panel.html#panel-longitudinal-data-methods",
    "title": "10: Panel Data Methods",
    "section": "Panel (longitudinal) Data Methods",
    "text": "Panel (longitudinal) Data Methods\n\nPanel dataExample panel dataCentral Question\n\n\nDefinition\nData follow the  same  individuals, families, firms, cities, states or whatever, across time.\n\nExample\n\nRandomly select people from a population at a given point in time\nThen the same people are reinterviewed at several subsequent points in time, which would result in data on wages, hours, education, and so on, for the same group of people in different years.\n\n\n\nPanel Data as data.frame\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nyear: year\nfcode: factory id\nemploy: the number of employees\nsales: sales in USD\n\n\n\nCan we do anything to deal with endogeneity problem taking advantage of the panel data structure?"
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#panel-data-estimation-methods",
    "href": "lectures/10-panel/10-panel.html#panel-data-estimation-methods",
    "title": "10: Panel Data Methods",
    "section": "Panel Data Estimation Methods",
    "text": "Panel Data Estimation Methods\n\nExampleWhat’s hidden?Mathenmaticallytwo-period panel\n\n\nDemand for massage (cross-sectional)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLocation\nYear\nP\nQ\n\n\n\n\nChicago\n2003\n75\n2.0\n\n\nPeoria\n2003\n50\n1.0\n\n\nMilwaukee\n2003\n60\n1.5\n\n\nMadison\n2003\n55\n0.8\n\n\n\n\n\n\n\n\nP: the price of one massage\nQ: the number of massages received per capita\n\n\nQuestion 1Question 2\n\n\nAcross the four cities, how are price and quantity are associated? Positive or negative?\n\n\n\nAnswer\n\nThey are positively correlated. So, does that mean people want more massages as their price increases? Probably not.\n\n\n\nWhat could be causing the positive correlation?\n\n\n\nAnswer\n\n\nIncome (can be observed)\nQuality of massages (hard to observe)\nHow physically taxing jobs are (?)\n\n\n\n\n\n\n\n\nDemand for massage (cross-sectional)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLocation\nYear\nP\nQ\nQl\n\n\n\n\nChicago\n2003\n75\n2.0\n10\n\n\nPeoria\n2003\n50\n1.0\n5\n\n\nMilwaukee\n2003\n60\n1.5\n7\n\n\nMadison\n2003\n55\n0.8\n6\n\n\n\n\n\n\n\n\nKey\nMassage quality was hidden (omitted) affecting .red[both] price and massages per capita.\n\nProblem\nMassage quality is not observable, and thus cannot be controlled for.\n\n\nMathematically\n\\(Q  = \\beta_0 + \\beta_1 P + v \\;\\;( = \\beta_2 + Ql + u)\\)\n\n\\(P\\): the price of one massage\n\\(Q\\): the number of massages received per capita\n\n\\(Ql\\): the quality of massages\n\\(u\\): everything else that affect \\(P\\)\n\n\nEndogeneity Problem\n\\(P\\) is correlated with \\(Ql\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLocation\nYear\nP\nQ\nQl\n\n\n\n\nChicago\n2003\n75\n2.0\n10\n\n\nChicago\n2004\n85\n1.8\n10\n\n\nPeoria\n2003\n50\n1.0\n5\n\n\nPeoria\n2004\n48\n1.1\n5\n\n\nMilwaukee\n2003\n60\n1.5\n7\n\n\nMilwaukee\n2004\n65\n1.4\n7\n\n\nMadison\n2003\n55\n0.8\n6\n\n\nMadison\n2004\n60\n0.7\n6\n\n\n\n\n\n\n\n\nKeywithin-cityQuestion\n\n\nThere are two kinds of variations:\n\ninte-rcity (across city) variation\nintra-city (within city) variation\n\nThe cross-sectional data offers only the inte-rcity (across city) variations.\n\n\nNow, compare the massage price and massages per capita  within  each city (over time). What do you see?\n\n\n\nAnswer\n\nPrice and quantity are  negatively  correlated!\n\n\n\nWhy looking at the  intra-city (within city)  variation seemed to help us estimate the impact of massage price on demand more credibly?\n\n\n\nAnswer\n\nThe omitted variable, massage quality, did not change over time within city, which means it is controlled for as long as you look only at the intra-city variations (you do not compare .red[across] cities)."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#using-only-the-intra-city-variations",
    "href": "lectures/10-panel/10-panel.html#using-only-the-intra-city-variations",
    "title": "10: Panel Data Methods",
    "section": "Using only the intra-city variations",
    "text": "Using only the intra-city variations\n\nfirst-differencingfirst-differenced modelEstimate the modelSummary\n\n\nQuestion\nSo, how do we use only the intra-city variations in a regression framework?\n\nfirst-differencing\nOne way to do this is to compute the changes in prices and th changes in quantities in each city \\((\\Delta P\\) and \\(\\Delta Q)\\) and then regress \\(\\Delta Q\\) and \\(\\Delta P\\).\n\n\n\nFirst-differenced Data\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLocation\nYear\nP\nQ\nQl\nP_dif\nQ_dif\nQl_dif\n\n\n\n\nChicago\n2003\n75\n2.0\n10\nNA\nNA\nNA\n\n\nChicago\n2004\n85\n1.8\n10\n10\n-0.2\n0\n\n\nPeoria\n2003\n50\n1.0\n5\nNA\nNA\nNA\n\n\nPeoria\n2004\n48\n1.1\n5\n-2\n0.1\n0\n\n\nMilwaukee\n2003\n60\n1.5\n7\nNA\nNA\nNA\n\n\nMilwaukee\n2004\n65\n1.4\n7\n5\n-0.1\n0\n\n\nMadison\n2003\n55\n0.8\n6\nNA\nNA\nNA\n\n\nMadison\n2004\n60\n0.7\n6\n5\n-0.1\n0\n\n\n\n\n\n\n\n\nKey\nVariations in quality is eliminated after first differentiation!! (quality is controlled for)\n\n\n\n\n\nA new way of writing a model\n\\(Q_{i,t}  = \\beta_0 + \\beta_1 P_{i,t} + v_{i,t} \\;\\; ( = \\beta_2 Ql_{i,t} + u_{i,t})\\)\n\ni: indicates city\nt: indicates time\n\n\nFirst differencing\n\\(Q_{i,1}  = \\beta_0 + \\beta_1 P_{i,1} + v_{i,1} \\;\\; ( = \\beta_2 Ql_{i,1} + u_{i,1})\\)\n\\(Q_{i,2}  = \\beta_0 + \\beta_1 P_{i,2} + v_{i,2} \\;\\; ( = \\beta_2 Ql_{i,2} + u_{i,2})\\)\n\\(\\Rightarrow\\)\n\\(\\Delta Q  = \\beta_1 \\Delta P + \\Delta v ( = \\beta_2 \\Delta Ql + \\Delta u)\\)\n\nEndogeneity Problem?\nSince \\(Ql_{i,1} = Ql_{i,2}\\), \\(\\Delta Ql = 0 \\Rightarrow \\Delta Q  = \\beta_0 + \\beta_1 \\Delta P + \\Delta u\\)\nNo endogeneity problem after first differentiation!\n\n\nData\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nOLS on the original data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nOLS on the first-differenced data:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nAs long as the omitted variable that affects both the dependent and independent variables are constant over time (time-invariant), then using only the variations over time (ignoring variations across cross-sectional units) can eliminate the omitted variable bias\nFirst-differencing the data and then regressing changes on changes does the trick of ignoring variations across cross-sectional units\nOf course, first-differencing is possible only because the same cross-sectional units are observed multiple times over time."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#multi-year-general-panel-datasets",
    "href": "lectures/10-panel/10-panel.html#multi-year-general-panel-datasets",
    "title": "10: Panel Data Methods",
    "section": "Multi-year (general) panel datasets",
    "text": "Multi-year (general) panel datasets\n\ndeviation from the meanFixed Effects Regression\n\n\nwithin-transformation\n\nIf we have lots of years of data, we could, in principle, compute all of the first differences (i.e., 2004 versus 2003, 2005 versus 2004, etc.) and then run a single regression. But there is an easier way.\nInstead of thinking of each year’s observation in terms of how much it differs from the prior year for the same city, let’s think about how much each observation differs from the average for that city.\n\nExample\nHow much each observation differs from the average for that city?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nLocation\nYear\nP\nP_mean\nP_dev\nQ\nQ_mean\nQ_dev\nQl\nQl_mean\nQl_dev\n\n\n\n\nChicago\n2003\n75\n80.0\n-5.0\n2.0\n1.90\n0.10\n10\n10\n0\n\n\nChicago\n2004\n85\n80.0\n5.0\n1.8\n1.90\n-0.10\n10\n10\n0\n\n\nPeoria\n2003\n50\n49.0\n1.0\n1.0\n1.05\n-0.05\n5\n5\n0\n\n\nPeoria\n2004\n48\n49.0\n-1.0\n1.1\n1.05\n0.05\n5\n5\n0\n\n\nMilwaukee\n2003\n60\n62.5\n-2.5\n1.5\n1.45\n0.05\n7\n7\n0\n\n\nMilwaukee\n2004\n65\n62.5\n2.5\n1.4\n1.45\n-0.05\n7\n7\n0\n\n\nMadison\n2003\n55\n57.5\n-2.5\n0.8\n0.75\n0.05\n6\n6\n0\n\n\nMadison\n2004\n60\n57.5\n2.5\n0.7\n0.75\n-0.05\n6\n6\n0\n\n\n\n\n\n\n\n\nNote\nWe call this data transformation  within-transformation  or  demeaning .\n\n\n\nExamplewithin-transformationEndogeneity Problem?\n\n\nModel\n\nDependent variable: Q_dev\nIndependent variable: P_dev\n\nKey\nIn calculating P_dev (deviation from the mean by city), Ql_dev is eliminated.\n\n\n\\(Q_{i,1}  = \\beta_0 + \\beta_1 P_{i,1} + v_{i,1} \\;\\; ( = \\beta_2 Ql_{i,1} + u_{i,1})\\)\n\\(Q_{i,2}  = \\beta_0 + \\beta_1 P_{i,2} + v_{i,2} \\;\\; ( = \\beta_2 Ql_{i,2} + u_{i,2})\\)\n\\(\\vdots\\)\n\\(Q_{i,T}  = \\beta_0 + \\beta_1 P_{i,T} + v_{i,T} \\;\\; ( = \\beta_2 Ql_{i,T} + u_{i,T})\\)\n\\(\\Rightarrow\\)\n\\(Q_{i,t} - \\bar{Q_{i}}   = \\beta_1 [P_{i,t} - \\bar{P_{i}}] + [v_{i,t} - \\bar{v_{i}}] ( = \\beta_2 [Ql_{i,t} - \\bar{Ql_{i}}] + [u_{i,t} - \\bar{u_{i}}])\\)\n\n\n\\(Ql_{i,1} = Ql_{i,2} = \\dots = Ql_{i,T} = \\bar{Ql_i}\\)\n\\(\\Rightarrow\\)\n\\(Q_{i,t} - \\bar{Q_{i}}   = \\beta_1 [P_{i,t} - \\bar{P_{i}}] + [u_{i,t} - \\bar{u_{i}}]\\)\n\nNo endogeneity problem after the within-transformation because \\(Ql\\) is gone."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#fixed-effects-fe-estimation-in-general",
    "href": "lectures/10-panel/10-panel.html#fixed-effects-fe-estimation-in-general",
    "title": "10: Panel Data Methods",
    "section": "Fixed Effects (FE) Estimation (in general)",
    "text": "Fixed Effects (FE) Estimation (in general)\n\nFE estimationWhen is FE estimation unbiased?Example\n\n\nConsider the following general model\n\\(y_{i,t}=\\beta_1 x_{i,t} + \\alpha_i + u_{i,t}\\)\n\n\\(\\alpha_i\\): the impact of time-invariant  unobserved  factor that is specific to \\(i\\) (also termed .blue[individual fixed effect])\n\\(\\alpha_i\\) is thought to be correlated with \\(x_{i,t}\\)\n\n\nFind individual averageswithin-transformationEstimation of \\(\\beta\\)s\n\n\nFor each \\(i\\), average this equation over time, we get\n\\(\\frac{\\sum_{t=1}^T y_{i,t}}{T} = \\frac{\\sum_{t=1}^T x_{i,t}}{T} + \\alpha_i  + \\frac{\\sum_{t=1}^T u_{i,t}}{T}\\)\nWe use \\(\\bar{z}_i\\) to indicate the average of \\(\\bar{z}_{i,t}\\) over time for individual \\(i\\). Using this notation,\n\\(\\bar{y}_i = \\bar{x}_i + \\alpha_i  + \\bar{u}_i\\)\nNote that \\(\\frac{\\sum_{t=1}^T \\alpha_{i}}{T} = \\alpha_i\\)\n\n\nSubtracting the equation of the average from the original model,\n\\((y_{i,t}-\\bar{y}_i=\\beta_1 (x_{i,t} -\\bar{x}_i) + (u_{i,t} -\\bar{u}_i) + a_i - a_i\\)\n\n\n\n\nImportant\n\n\n\\(\\alpha_i\\) is gone!\n\n\n\n\n\nWe then regress \\((y_{i,t}-\\bar{y}_i)\\) on \\((x_{i,t}-\\bar{x}_i)\\) to estimate \\(\\beta_1\\).\n\n\n\n\n\n\nHere is the model after within-transformation:\n\\[\\begin{align*}\ny_{i,t}-\\bar{y}_i=\\beta_1 (x_{i,t} -\\bar{x}_i) + (u_{i,t} -\\bar{u}_i)\n\\end{align*}\\]\nSo,\n\\(x_{i,t} -\\bar{x}_i\\) needs to be uncorrelated with \\(u_{i,t} -\\bar{u}_i\\).\n\n\n\n\nImportant\n\n\nThe above condition is satisfied if\n\\(E[u_{i,s}|x_{i,t}] = 0 \\;\\; ^\\forall s, \\;\\; t, \\;\\;\\mbox{and} \\;\\;j\\)\ne.g., \\(E[u_{i,1}|x_{i,4}]=0\\)\n\n\n\n\n\nFixed effects estimation\nRegress within-transformed Q on within-transformed P:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#an-alternative-way-to-view-the-fixed-effects-estimation-methods",
    "href": "lectures/10-panel/10-panel.html#an-alternative-way-to-view-the-fixed-effects-estimation-methods",
    "title": "10: Panel Data Methods",
    "section": "An alternative way to view the Fixed Effects estimation methods",
    "text": "An alternative way to view the Fixed Effects estimation methods\n\ntwo equivalent modelsEstimation with the alternative modelWhat does this tell us?\n\n\nImportant\nThe two approaches below will result in the same coefficient estimates (mathematically identical).\n\nRunning OLS on the within-tranformed (demeaned) data\nRunning OLS on the untransformed data but including the dummy variables for the individuals (city in our example)\n\n\n\nYou can use the original data (no within-transformation) and include dummy variables for all the cities except one.\n\nCreate dummy variablesEstimate with dummy variables\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nNote that the coefficient estimate on P is exactly the same as the one we saw earlier when we regressed Q_dev on P_dev.\n\n\n\n\n\n\nBy including individual dummies (individual fixed effects), you are effectively eliminating the between (inter-city) variations and using only the clean within (within-city) variations for estimation.\n\n\n\n\n\nVery Important\n\n\nMore generally, including dummy variables of a categorical variable (like city in the example above), eliminates the variations  between  the elements of the category (e.g., different cities), and use only the variations  within  each of the element of the category."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#fixed-effects-estimation-in-practice-using-r",
    "href": "lectures/10-panel/10-panel.html#fixed-effects-estimation-in-practice-using-r",
    "title": "10: Panel Data Methods",
    "section": "Fixed Effects Estimation in Practice Using R",
    "text": "Fixed Effects Estimation in Practice Using R\n\nHowDemonstrationRandom Effects (RE) Model\n\n\nAdvice\n\nDo not within-transform the data yourself and run a regression\nDo not create dummy variables yourself and run a regression with the dummies\n\n\nIn practice\nWe will use the fixest package.\n\nSyntax\n\nfixest::feols(dep_var ~ indep_vars | FE, data)\n\n\nFE: the name of the variable that identifies the cross-sectional units that are observed over time (Location in our example)\ndep_var: (non-transformed) dependet variable\nindep_vars: list of (non-transformed) independent variables\n\n\n\nData\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nHow is it different from the FE model?\n\nCan be more efficient (lower variance) than FE under certain cases\nIf \\(\\alpha_i\\) and independent variables are correlated, then RE estimators are biased\nUnless \\(\\alpha_i\\) and independent variables are not correlated (which does not hold most of the time unless you got data from controlled experiments), \\(RE\\) is not an attractive option\nYou almost never see this estimation method used in papers that use non-experimental data\n\n\nNote\nWe do not cover this estimation method as you almost certainly would not use this estimation method."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#fixed-effects-dummy-variables-to-harness-clean-variations",
    "href": "lectures/10-panel/10-panel.html#fixed-effects-dummy-variables-to-harness-clean-variations",
    "title": "10: Panel Data Methods",
    "section": "Fixed effects (dummy variables) to harness clean variations",
    "text": "Fixed effects (dummy variables) to harness clean variations\n\nAvocado dataObservationsEndogeneity problemClean variationsWhat FE to includeKey message\n\n\n\n\n\n\n\nWeekly Sales of Avocados in California, Jan 2015 - Match 2018\n\n\n\n\n\nObjective\nYou are interested in understanding the impact of avocado price on its consumption.\n\n\n\n\nObservations\n\nThey are negatively associated with each other\n\nAvocado sales tend to be lower in weeks where the price of avocados is high.\nPrices tend to be higher in weeks where fewer avocados are sold\n\n\n\nQuestion\nIf you just regress avocado sales on its price, is the estimation of the coefficient on the pirce unbiased?\n\n\n\nAnswer\n\nNo.\n\nReverse causality\n\nprice affects demand\ndemand affects price\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem\nReverse Causality: Price affects demand and demand affects price.\n\ncontextual knowledge\nNow, suppose you learned the following fact after studying the supply and purchasing mechanism on the avocado market:\n At the beginning of each month, avocado suppliers make a plan for what avocado prices will be each week in that month, and never change their plans until the next month. \nThis means that within the same month changes in avocado price every week is not a function of how much avocado has been bought in the previous weeks, effectively breaking the causal effect of demand on price.\nSo, our estimation strategy would be to just look at the variations in demand and price  within  individual months, but ignore variations in price  between months.\n\n\nThe figure below presents avocado sales and price of avocado in March, 2015. This is an example of clean variations in price (intra-month observations).\n\n\n\n\n\n\n\n\n\n\n\n\nCase 1Case 2\n\n\nWe have three months of avocado purchase and price observed weekly.\n\nQuestion\nWhat should we do?\n\n\n\nAnswer\n\nInclude month dummy variables.\n\n\n\nWe have two years of avocado purchase and price observed weekly.\n\nQuestion\nWhat should we do?\n\n\n\nAnswer\n\n\nInclude  month-year  dummy variables.\nIncluding  month  dummy variables will not do it. Because the observations in the same month in two different years are considered to belong to the same group. That is, variations between two different years of the same month will be used for estimation. (e.g., January in 2014 and January in 2015)\n\n\n\n\n\n\n\n\nMessage 1\nBy understanding the data generating process (knowing how any economic market works), we recognize the problem of simply looking at the relationship between the avocado price and demand to conclude the causal impact of price on demand (reverse causality).\n\nMessage 2\nWe study the context very well and how the avocado market works in California (of course it is not really how CA avocado market works in reality) and make use of the information to identify the “clean” variations in avocado price to identify its impact on demand."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#year-fixed-effects",
    "href": "lectures/10-panel/10-panel.html#year-fixed-effects",
    "title": "10: Panel Data Methods",
    "section": "Year Fixed Effects",
    "text": "Year Fixed Effects\n\nWhat is it?What do year FEs do?RecommendationR implementation\n\n\nJust a collection of year dummies, which takes 1 if in a specific year, 0 otherwise.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nThey capture anything that happened to  all  the individuals for a specific year relative to the base year\n\nExample\nEducation and wage data from \\(2012\\) to \\(2014\\),\n\\(log(income) = \\beta_0 + \\beta_1 educ + \\beta_2 exper  + \\sigma_1 FE_{2012} + \\sigma_2 FE_{2013}\\)\n\n\\(\\sigma_1\\): captures the difference in \\(log(income)\\) between \\(2012\\) and \\(2014\\) (base year)\n\\(\\sigma_2\\): captures the difference in \\(log(income)\\) between \\(2013\\) and \\(2014\\) (base year)\n\n\nInterpretation\n\\(\\sigma_1=0.05\\) would mean that \\(log(income)\\) is greater in \\(2012\\) than \\(2014\\) by \\(5\\%\\) on average for whatever reasons with everything else fixed.\n\n\nRecommendation\nIt is almost always a good practice to include year FEs if you are using a panel dataset with annual observations.\n\nWhy?\n\nRemember year FEs capture .blue[anything] that happened to all the individuals for a specific year relative to the base year\nIn other words, .blue[all the unobserved factors] that are common to all the individuals in a specific year is .blue[controlled for (taken out of the error term)]\n\n\nExample\nEconomic trend in:\n\\(log(income) =  \\beta_0 + \\beta_1 educ + \\sigma_1 FE_{2012} + \\sigma_2 FE_{2013}\\)\n\nEducation is non-decreasing through time\nEconomy might have either been going down or up during the observed period\n\nWithout year FE, \\(\\beta_1\\) may capture the impact of overall economic trend.\n\n\nIn order to include year FEs to individual FEs, you can simply add the variable that indicates year like below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nCaveats\n\n\n\nYear FEs would be perfectly collinear with variables that change only across time, but not across individuals.\nIf your variable of interest is such a variable, you cannot include year FEs, which would then make your estimation subject to omitted variable bias due to .red[other] unobserved yearly-changing factors."
  },
  {
    "objectID": "lectures/10-panel/10-panel.html#standard-error-estimation-for-panel-data-methods",
    "href": "lectures/10-panel/10-panel.html#standard-error-estimation-for-panel-data-methods",
    "title": "10: Panel Data Methods",
    "section": "Standard Error Estimation for Panel Data Methods",
    "text": "Standard Error Estimation for Panel Data Methods\n\nnon-homegeneous errorConsequences?What to do and R implementation\n\n\nHeteroskedasticity\nJust like we saw for OLS using cross-sectional data, heteroskedasticity leads to biased estimation of the standard error of the coefficient estimators if not taken into account\n\nSerial Correlation\nCorrelation of errors over time, which we call .blue[serial correlation]\n\n\n\njust like heteroskedasticity, serial correlation could lead to biased estimation of the standard error of the coefficient estimators if not taken into account\ndo not affect the unbiasedness and consistency property of your estimators\n\n\n\n\nImportant\n\n\n\nTaking into account the potential of serial correlation when estimating the standard error of the coefficient estimators can dramatically change your conclusions about the statistical significance of some independent variables!!\nWhen serial correlation is ignored, you tend to underestimate the standard error (why?), inflating \\(t\\)-statistic, which in turn leads to over-rejection that you should.\n\nBertrand, Duflo, and Mullainathan (2004)\n\nExamined how problematic serial correlation is in terms of inference via Monte Carlo simulation\n\ngenerate a fake treatment dummy variable in a way that it has no impact on the outcome (dependent variable) in the dataset of women’s wages from the Current Population Survey (CPS)\nrun regression of the oucome on the treatment variable\ntest if the treatment variable has statistically significant effect via \\(t\\)-test\n\nThey rejected the null \\(67.5\\%\\) at the \\(5\\%\\) significance level!!\n\n\n\n\n\n\nSE robust to heteroskedasticity and serial correlation\n\nYou can take into account  both  heteroskedasticity and serial correlation by clustering by individual (whatever the unit of individual is: state, county, farmer)\nCluster by individual can take into account the correlation within individuals (over time)\n\n\nR implementation\nThe last partition is used for clustering standard error estimation by variable like below.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#univariate-vs-multivariate-regression-models",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#univariate-vs-multivariate-regression-models",
    "title": "02: Multivariate Regression",
    "section": "Univariate vs Multivariate Regression Models",
    "text": "Univariate vs Multivariate Regression Models\n\nBi-variate vs. Uni-variateExampleExampleModel (general)\n\n\nUnivariate\nThe most important assumption \\(E[u|x] = 0\\) (zero conditional mean) is almost always violated (unless you data comes from randomized experiments) because all the other variables are sitting in the error term, which can be correlated with \\(x\\).\n\nMultivariate\nMore independent variables mean less factors left in the error term, which makes the endogeneity problem  less severe\n\n\nUni-variate vs. bi-variate\n\\[\\begin{align}\n  \\mbox{Uni-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + u_1 (=u_2+\\beta_2 exper)\\\\\n  \\mbox{Bi-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u_2\n\\end{align}\\]\n\nWhat’s different?\n\nuni-variate: \\(\\widehat{\\beta}_1\\) is biased unless experience is uncorrelated with education because experience was in error term\nbi-variate: able to measure the effect of education on wage,  holding experience fixed  because experience is modeled explicitly ( We say \\(exper\\) is controlled for. )\n\n\n\nThe impact of per student spending (expend) on standardized test score (avgscore) at the high school level\n\\[\\begin{align}\navgscore= & \\beta_0+\\beta_1 expend + u_1 (=u_2+\\beta_2 avginc) \\notag \\\\\navgscore= & \\beta_0+\\beta_1 expend +\\beta_2 avginc + u_2 \\notag\n\\end{align}\\]\n\n\nMore generally,\n\\[\\begin{align}\n  y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + u\n\\end{align}\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\\(\\beta_2\\): measure the change in \\(y\\) with respect to \\(x_2\\), holding other factors fixed"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "title": "02: Multivariate Regression",
    "section": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator",
    "text": "The Crucial Condition (Assumption) for Unbiasedness of the OLS Estimator\n\nUni-variate v.s. Bi-variateMean independence condition: example\n\n\nUni-variate\n\\(y = \\beta_0 + \\beta_1x + u\\),\n\\(E[u|x]=0\\)\n\nBi-variate\n\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\),\n\nMathematically: \\(E[u|x_1,x_2]=0\\)\nVerbally: for any values of \\(x_1\\) and \\(x_2\\), the expected value of the unobservables is zero\n\n\n\nIn the following wage model,\n\\[\\begin{align*}\nwage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\n\\end{align*}\\]\nMean independence condition is\n\\[\\begin{align}\n  E[u|educ,exper]=0\n\\end{align}\\]\nVerbally:\nThis condition would be satisfied if innate ability of students is on average unrelated to education level and experience."
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-model-with-k-independent-variables",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#the-model-with-k-independent-variables",
    "title": "02: Multivariate Regression",
    "section": "The model with \\(k\\) independent variables",
    "text": "The model with \\(k\\) independent variables\n\nGeneral modelImplementation (R)Present results(Math: derive OLS estimator)\n\n\nModel\n\\[\\begin{align}\n  y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\n\\end{align}\\]\n\nMean independence assumption?\n\\(\\beta_{OLS}\\) (OLS estimators of \\(\\beta\\)s) is unbiased if,\n\\[\\begin{align}\n    E[u|x_1,x_2,\\dots,x_k]=0\n\\end{align}\\]\nVerbally: this condition would be satisfied if the error term is uncorrelated wtih any of the independent variables, \\(x_1,x_2,\\dots,x_k\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhen you are asked to present regression results in assignments or your final paper, use the msummary() function from the modelsummary package.\n\n\n\n\nlibrary(modelsummary)\n\n#* run regression\nreg_results &lt;- feols(speed ~ dist, data = cars)\n\n#* report regression table\nmsummary(\n  reg_results,\n  # keep these options as they are\n  stars = TRUE,\n  gof_omit = \"IC|Log|Adj|F|Pseudo|Within\"\n)\n\n\n\n\n\n\n\n\n (1)\n\n\n\n\n(Intercept)\n8.284***\n\n\n\n(0.874)\n\n\ndist\n0.166***\n\n\n\n(0.017)\n\n\nNum.Obs.\n50\n\n\nR2\n0.651\n\n\nRMSE\n3.09\n\n\nStd.Errors\nIID\n\n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOLS\nFind the combination of \\(\\beta\\)s that minimizes the sum of squared residuals\n\nSo,\nDenoting the collection of \\(\\widehat{\\beta}\\)s as \\(\\widehat{\\theta} (=\\{\\widehat{\\beta}_0,\\widehat{\\beta}_1,\\dots,\\widehat{\\beta}_k\\})\\),\n\\[\\begin{align}\n    Min_{\\theta} \\sum_{i=1}^n \\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\widehat{\\beta}_k x_{k,i}) \\Big]^2\n\\end{align}\\]\nFind the FOCs by partially differentiating the objective function (sum of squared residuals) wrt each of \\(\\widehat{\\theta} (=\\{\\widehat{\\beta}_0,\\widehat{\\beta}_1,\\dots,\\widehat{\\beta}_k\\})\\),\n\\[\\begin{align}\n  \\sum_{i=1}^n(y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\Big[ y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_{1,i} + \\widehat{\\beta}_2 x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]\nOr more succinctly,\n\\[\\begin{align}\n  \\sum_{i=1}^n \\widehat{u}_i = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#unbiasedness-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#unbiasedness-of-ols-estimators",
    "title": "02: Multivariate Regression",
    "section": "Unbiasedness of OLS Estimators",
    "text": "Unbiasedness of OLS Estimators\n\nConditions (four)Perfect CollinearityEndogeneity (Definition)\n\n\n\n\n\nImportant\n\n\nOLS estimators of multivariate models are unbiased if the following conditions are satisfied.\n\n\n\n\n\n\nCondition 1\nYour model is correct (Assumption \\(MLR.1\\))\n\nCondition 2\nRandom sampling (Assumption \\(MLR.2\\))\n\n\n\n\n\nCondition 3\nNo perfect collinearity (Assumption \\(MLR.3\\))\n\nCondition 4\nZero Conditional Mean (Assumption \\(MLR.4\\)) \\(E[u|x_1,x_2,\\dots,x_k]=0 \\;\\;\\mbox{(Assumption MLR.4)}\\)\n\n\n\n\n\nNo Perfect Collinearity (\\(MLR.3\\))\nAny variable cannot be a linear function of the other variables\n\nExample (silly)\n\\[\\begin{align}\n  wage = \\beta_0 + \\beta_1 educ + \\beta_2 (3\\times educ) + u\n\\end{align}\\]\n( More on this later when we talk about dummy variables)\n\n\n\n\n\n\nEndogeneity: Definition\n\n\n\\[\nE[u|x_1,x_2,\\dots,x_k] = f(x_1,x_2,\\dots,x_k) \\ne 0\n\\]\n\n\n\n\n\nWhat could cause endogeneity problem?\n\nfunctional form misspecification\n\n\\[\\begin{align}\n  wage = & \\beta_0 + \\beta_1 log(x_1) + \\beta_2 x_2 + u_1 \\;\\;\\mbox{(true)}\\\\\n  wage = & \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u_2 (=log(x_1)-x_1) \\;\\; \\mbox{(yours)}\n\\end{align}\\]\n\nomission of variables that are correlated with any of \\(x_1,x_2,\\dots,x_k\\) ( more on this soon )\n other sources of enfogeneity later"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#variance-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#variance-of-ols-estimators",
    "title": "02: Multivariate Regression",
    "section": "Variance of OLS estimators",
    "text": "Variance of OLS estimators\n\nHomoskedasticity and Variance of \\(\\widehat{\\beta}_{OLS}\\)Estimating \\(\\sigma^2\\)Estimator of \\(Var{\\widehat{\\beta}_j}\\)\n\n\nCondition 5\nError term is homoeskedastic (Assumption \\(MLR.5\\))\n\\[\\begin{align}\nVar(u|x_1,\\dots,x_k)=\\sigma^2\n\\end{align}\\]\n\nUnder conditions \\(MLR.1\\) through \\(MLR.5\\), conditional on the sample values of the independent variables,\n\n\n\n\nVariance of \\(\\widehat{\\beta}_{OLS}\\)\n\n\n\\[\\begin{align}\n    Var(\\widehat{\\beta}_j)= \\frac{\\sigma^2}{SST_j(1-R^2_j)},\n\\end{align}\\]\n\n\n\n\nwhere\n\n\\(SST_j= \\sum_{i=1}^n (x_{ji}-\\bar{x_j})^2\\)\n\\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables including an intercept. ( We will revisit this equation)\n\n\n\nJust like uni-variate regression, you need to estimate \\(\\sigma^2\\) if you want to estimate the variance (and standard deviation) of the OLS estimators.\nuni-variate regression\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-2}\n\\end{align}\\]\nmulti-variate regression\nA model with \\(k\\) independent variables with intercept.\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-(k+1)}\n\\end{align}\\]\nYou solved \\(k+1\\) simultaneous equations to get \\(\\widehat{\\beta}_j\\) \\((j=0,\\dots,k)\\). So, once you know the value of \\(n-k-1\\) of the residuals, you know the rest.\n\n\nUsing the estimator of \\(\\sigma^2\\) in place of \\(\\sigma^2\\), we have the  estimator  of the variance of the OLS estimator.\n\n\n\n\nEstimator of the variance of the OLS estimator\n\n\n\\[\\begin{align}\n\\widehat{Var{\\widehat{\\beta}_j}} = \\frac{\\widehat{\\sigma}^2}{SST_j(1-R^2_j)} = \\left(\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-k-1}\\right) \\cdot \\frac{1}{SST_j(1-R^2_j)}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#frischwaughlovell-theorem",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#frischwaughlovell-theorem",
    "title": "02: Multivariate Regression",
    "section": "Frisch–Waugh–Lovell Theorem",
    "text": "Frisch–Waugh–Lovell Theorem\nConsider the following simple model,\n\\[\\begin{align}\n  y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + u_i\n\\end{align}\\]\nSuppose you are interested in estimating only \\(\\beta_1\\).\nLet’s consider the following two methods,\n\nMethod 1: Regular OLS\nRegress \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\) with an intercept to estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) at the same time (just like you normally do)\n\nMethod 2: 3-step\n\nregress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\nregress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\nregress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\) \\((\\widehat{u}_y=\\alpha_1 \\widehat{u}_{x_1}+v_3)\\)\n\nFrisch-Waugh–Lovell theorem\nMethods 1 and 2 produces the same coefficient estimate on \\(x_1\\)\n\\[\\widehat{\\beta}_1 = \\widehat{\\alpha_1}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#partialing-out-interpretation-from-method-2",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#partialing-out-interpretation-from-method-2",
    "title": "02: Multivariate Regression",
    "section": "Partialing out Interpretation from Method 2",
    "text": "Partialing out Interpretation from Method 2\nStep 1\nRegress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\n\n\\(\\widehat{u}_y\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(y\\)\n\nStep 2\nRegress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\n\n\\(\\widehat{u}_{x_1}\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(x_1\\)\n\nStep 3\nRegress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\), which produces an estimte of \\(\\beta_1\\) that is identical to that you can get from regressin \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\)"
  },
  {
    "objectID": "lectures/02-multivariate-regression/02-1multivariate-regression.html#interpretation",
    "href": "lectures/02-multivariate-regression/02-1multivariate-regression.html#interpretation",
    "title": "02: Multivariate Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nRegressing \\(y\\) on all explanatory variables \\((x_1\\), \\(x_2\\), and \\(x_3)\\) in a multivariate regression is as if you are looking at the impact of a single explanatory variable with the effects of all the other effects partiled out\nIn other words, including variables beyond your variable of interest lets you  control for (remove the effect of)  other variables, avoiding confusing the impact of the variable of interest with the impact of other variables.\n\n\n\n\nback to course website with lecture slides"
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#hypothesis-testing",
    "href": "lectures/05-testing/05-testing.html#hypothesis-testing",
    "title": "05: Hypothesis Testing",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nGeneral stepsExampleBy the waySummaryExercise\n\n\nHere is the general step of any hypothesis testing:\n\nStep 1: specify the null \\((H_0)\\) and alternative \\((H_1)\\) hypotheses\nStep 2: find the distribution of the test statistic  if the null hypothesis is true \nStep 3: calculate the test statistic based on the data and regression results\nStep 4: define the significance level\nStep 5: check how unlikely that you get the actual test statistic (found at Step 3)  if indeed the null hypothesis is true \n\n\n\n\nSettingdistribution of \\(\\theta\\)distribution of \\(\\theta\\) if \\(H_0\\) is trueCase 1Case 2\n\n\nGoal\nSuppose you want to test if the expected value of a normally distributed random variable \\((x)\\) is 1 or not.\n\nState of Knowledge\nWe do know \\(x\\) follows a normal distribution and its variance is 4 for some reason.\n\nYour Estimator\nYour estimator is the sample mean: \\(\\theta = \\sum_{i=1}^J x_i/J\\)\n\n\n\n\n\n\nMath Aside 1\n\n\n\\(Var(ax) = a^2 Var(X)\\)\n\n\n\n\nSo, we know that \\(\\theta \\sim N(\\alpha, 4/J)\\) (of course \\(\\alpha\\) is not known).\n\n\n\n\n\nMath Aside 2\n\n\nIf \\(x \\sim N(a, b)\\), then, \\(x-a \\sim N(0, b)\\) (shift)\n\n\n\n\nSo, \\(\\frac{x-a}{\\sqrt{b}} \\sim N(0, 1)\\) (combined with Math Aside 1)\n\nThis means,\nSince \\(\\theta = \\sum_{i=1}^J x_i/J\\) and \\(x_i \\sim N(\\alpha, 4)\\),\n\n\\(Var(\\theta) = J \\times \\frac{1}{J^2}Var(x) = 4/J\\)\n\\(\\frac{\\sqrt{J}}{2} \\cdot (\\theta - \\alpha)\\sim N(0, 1)\\).\n\n\n\n\n\nWe established that \\(\\frac{\\sqrt{J}}{2} \\cdot (\\theta - \\alpha)\\sim N(0, 1)\\).\nThe null hypothesis is \\(\\alpha = 1\\).\nIf \\(\\alpha = 1\\) is indeed true, then \\(\\sqrt{J} \\times (\\theta - 1)/2 \\sim N(0, 1)\\).\nIn other words, if you multiply the sample mean by the square root of the number of observations and divide it by 2, then it follows the standard normal distribution like below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have obtained 100 samples \\((J = 100)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 2.\nThen, your test statistic is \\(\\sqrt{100} \\times (2-1)/2 = 5\\).\nHow unlikely is it to get the number you got (5) if the null hypothesis is indeed true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose you have obtained 400 samples \\((J = 400)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 1.02.\nThen, your test statistic is \\(\\sqrt{400} \\times (1.02-1)/2 = 0.2\\).\nHow unlikely is it to get the number you got (0.2) if the null hypothesis is indeed true?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote that you do not really need to use \\(\\sqrt{J} \\times (\\theta - \\alpha)/2\\) as your test statistic.\nYou could alternatively use \\(\\theta - \\alpha\\). But, in that case, you need to be looking at \\(N(0, 4/J)\\) instead of \\(N(0, 1)\\) to see how unlikely you get the number you got.\nFor example, when the number of observations is 100 \\((J = 100)\\), the distribution of \\(\\theta-\\alpha\\) looks like the figure on the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\nReconsider the case 1\nSuppose you have obtained 100 samples \\((J = 100)\\) and calculated \\(\\theta\\) (sample mean), which turned out to be 2.\nThen, your test statistic is \\(2-1 = 1\\).\nIs it unlikely for you to get 1 if the null hypothesis is true?\nThe conclusion would be exactly the same as using \\(\\sqrt{J} \\times (\\theta - \\alpha)/2\\) because the distribution under the null is adjusted according to the test statistic you use.\n\n\n\nNote\n\n\nWe always use normalize test statistic so that we can always look up the same distribution.\n\n\n\n\n\nWhat do we need?\n\ntest-statistic of which we know the distribution (e.g., t-distribution, Normal distribution) assuming the null hypothesis\n\n\nWhat do we (often) do?\n\ntransform (most of the time) a raw random variable (e.g., sample mean in the example above) into a test statistic of which we know the distribution assuming that the null hypothesis is true\n\ne.g., we transformed the sample mean so that it follows the standard Normal distribution.\n\ncheck if the actual number you got from the test statistic is likely to happen or not (formal criteria has not been discussed yet)\n\n\n\n\nProblemAnswer\n\n\nYou have collected data on annual salary for those who graduated from University A and B. You are interested in testing whether the difference in annual salary between the universities (call it \\(x\\)) is 10 on average. You know (for unknown reasons) know that the difference is distributed as \\(N(\\alpha, 16)\\).\n\nWhat is the null hypothesis?\nUnder the null hypothesis, what is the distribution of the sample mean when the number of observation is 400?\nNormalize the test statistic so that the transformed version follows \\(N(0, 1)\\).\nThe actual difference you observed is 10.2. What is the probability that you observe a number greater than 10.2 if the null hypothesis is true? Use prnom().\n\n\n\n\nNote\n\n\nIn reality,\n\nwe need to find out what the distribution of the test statistic is\nwe need to formerly define when we accept or not accept the null hypothesis\n\n\n\n\n\n\n\n\\(\\alpha = 10\\)\n\\(\\theta \\sim N(\\alpha, 16/400)\\)\n\\(\\sqrt{\\frac{400}{16}}\\cdot (\\theta - \\alpha)\\)\nThe test statistic is \\(5 \\times (10.2 - 10) = 1\\)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#hypothesis-testing-of-the-coefficients",
    "href": "lectures/05-testing/05-testing.html#hypothesis-testing-of-the-coefficients",
    "title": "05: Hypothesis Testing",
    "section": "Hypothesis Testing of the Coefficients",
    "text": "Hypothesis Testing of the Coefficients\n\nExampleDistribution of \\(\\beta_j\\)t-testRecap on notations\n\n\nConsider the following model,\n\\[wage = \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\\]\n\n\n\n\nExample Hypotheses\n\n\n\neducation has no impact on wage \\((\\beta_1=0)\\)\nexperience has a positive impact on wage \\((\\beta_2&gt;0)\\)\n\n\n\n\n\n\n\nIf \\(\\widehat{\\beta}_1\\) is non-random, but just a scalar, all you have to do is just check if \\(\\widehat{\\beta}_1=0\\) or not\nBut, the estimate you get is  just one realization  of the range of values \\(\\widehat{\\beta}_1\\) could take because it is a random variable\nThis means that even if \\(\\beta_1=0\\) in the population, it is possible to get an estimate that is very far from 0\n\n\n\n\nAssumptionAdditional assumptionImplications of the Assumptions\n\n\nSo far, we learned that:\n\nOLS estimators are unbiased under MLR.1 ~ MLR.4\nVariance of the OLS estimator of \\(\\beta_j\\) is \\(\\frac{\\sigma^2}{SST_x\\cdot (1-R^2_j)}\\) under MLR.1 ~ MLR.5\n\nWe have  NOT  made any assumptions about the distribution of the error term!!\nIn order to perform hypothesis testing, we need to make assumptions about the distribution of error term (this is not strictly true, but more on this later)\n\n\nFor the purpose of hypothesis testing, we will make the following assumption:\n\n\n\n\nNormality of the error term\n\n\nThe population error \\(u\\) is  independent  of the explanatory variables \\(x_1,\\dots,x_k\\) and is  normally  distributed with zero mean and variance \\(\\sigma^2\\):\n\\[u\\sim N(0,\\sigma^2)\\]\n\n\n\n\n\n\n\nNote\n\n\nThe normality assumption is much more than error term being distributed as Normal.\nIndependence of the error term implies\n\n\\(E[u|x] = 0\\)\n\\(Var[u|x]= \\sigma^2\\)\n\nSo, we are necessarily assuming MLR.4 and MLR.5 hold by the independence assumption.\n\n\n\n\n\n\n\ndistribution of the dependent variable\nThe distribution of \\(y\\) conditional on \\(x\\) is a Normal distribution\n\\(y|x \\sim N(\\beta_0+\\beta_1 x_1+\\dots+\\beta_k x_k,\\sigma^2)\\)\n\n\\(E[y|x]\\) is \\(\\beta_0+\\beta_1 x_1+\\dots+\\beta_k x_k\\)\n\\(u|x\\) is \\(N(0,\\sigma^2)\\)\n\n\ndistribution of the OLS estimator\nIf the MLR.1 through MLR.6 are satisfied, \\(OLS\\) estimators are also Normally distributed!\n\\(\\widehat{\\beta}_j \\sim N(\\beta_j,Var(\\widehat{\\beta}_j))\\)\nwhich means,\n\\(\\frac{\\widehat{\\beta}_j-\\beta_j}{se(\\widehat{\\beta}_j)} \\sim N(0,1)\\)\n\n\n\n\n\n\n\nQuestion\n\n\nOkay, so are we going to use this for testing involving \\(\\beta_j\\)?\n\n\n\n\n\n\n\n\n\n\nIn practice, we need to estimate \\(se(\\beta_j)\\). If we use \\(\\widehat{se(\\widehat{\\beta}_j)}\\) instead of \\(se(\\widehat{\\beta}_j)\\), then,\n\\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}} \\sim t_{n-k-1}\\)\nwhere \\(n-k-1\\) is the degree of freedom of residual.\n(Note: \\(\\widehat{se(\\widehat{\\beta}_j)} = \\widehat{\\sigma}^2/\\big[SST_X\\cdot (1-R_j^2)\\big]\\))\n\n\n\n\\(k\\): the number of explanatory variables included except the intercept\n\\(\\sigma^2\\): the  true  variance of the error term\n\\(\\widehat{\\sigma^2}\\): the estimator (estimate) of the variance of the error term\n\n\\(\\widehat{\\sigma^2} = \\frac{\\sum \\widehat{u}_i^2}{n-k-1}\\)\n\n\\(SST_X = \\sum (x_i - \\bar{x})^2\\)\n\\(\\widehat{\\beta}_j\\): OLS estimator (estimate) on explanatory variable \\(x_j\\)\n\n\\(\\widehat{\\beta} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{SST_X}\\) (for simple univariate regression)\n\n\\(var(\\widehat{\\beta}_j)\\): the  true  variance of \\(\\widehat{\\beta}_j\\)\n\\(R^2_j\\): \\(R^2\\) when you regres \\(x_j\\) on all the other covariates (mathematical expression omitted)\n\\(\\widehat{var(\\widehat{\\beta}_j)}\\): Estimator (estimate) of \\(var(\\widehat{\\beta}_j)\\)\n\n\\(\\widehat{var(\\widehat{\\beta}_j)} = \\frac{\\widehat{\\sigma^2}}{SST_X\\cdot(1-R^2_j)}\\)\n\n\\(se(\\widehat{\\beta}_j)\\): square root of \\(var(\\widehat{\\beta}_j)\\)\n\\(\\widehat{se(\\widehat{\\beta}_j)}\\): square root of \\(\\widehat{var(\\widehat{\\beta}_j)}\\)"
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#details-of-hypothesis-testing-on-beta_j",
    "href": "lectures/05-testing/05-testing.html#details-of-hypothesis-testing-on-beta_j",
    "title": "05: Hypothesis Testing",
    "section": "Details of hypothesis testing on \\(\\beta_j\\)",
    "text": "Details of hypothesis testing on \\(\\beta_j\\)\n\nNull and alternative hypothesisSignificance levelp-valueExample\n\n\n\nIntroductionOne-sided and Two-sided Alternatives Hypotheses\n\n\nStatistical hypothesis testing involves two hypotheses: Null and Alternative hypotheses.\nPretend that you are an attorney who indicted a defendant who you think committed a crime.\n\nNull Hypothesis\nHypothesis that you would like to reject (defendant is not guilty)\n\nAlternative Hypothesis\nHypothesis you are in support of (defendant is guilty)\n\n\none-sided alternative\n\\(H_0:\\) \\(\\beta_j = 0\\) \\(H_1:\\) \\(\\beta_j &gt; 0\\)\nYou look at the positive end of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level).\n\ntwo-sided alternative\n\\(H_0:\\) \\(\\beta_j = 0\\) \\(H_1:\\) \\(\\beta_j \\ne 0\\)\nYou look at the both ends of the t-distribution to see if the t-statistic you obtained is more extreme than the level of error you accept (significance level).\n\n\n\n\n\n\n\nDefinitionOne-sided: 5% significanceOne-sided: 1% significanceTwo-sided: 5% significance\n\n\n\n\n\n\nDefinition\n\n\nThe probability of rejecting the null when the null is actually true (The probability that you wrongly claim that the null hypothesis is wrong even though it’s true in reality: Type I error)\n\n\n\n\nThe lower the significance level, you are more sure that the null is indeed wrong when you reject the null hypothesis\n\n\n\n\n\nFigure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value larger than 1.66 is 5% (0.05 in area).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the t-statistic is greater than 1.66 (95% quantile of the t-distribution)\n\n\n\n\nIf you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n5% is the  significance level\n1.66 is the  critical value above which you will reject the null\n\n\n\n\n\n\nThe figure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value larger than 2.37 is 1% (0.01 in area).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the t-statistic is greater than 2.37 (99% quantile of the t-distribution)\n\n\n\n\nIf you follow the decision rule, then you have a 1% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n1% is the  significance level \n2.37 is the  critical value  above which you will reject the null\n\n\n\n\n\n\nThe figure on the right presents the distribution of \\(\\frac{\\widehat{\\beta}_j-\\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) if \\(\\beta_j = 0\\) (the null hypothesis is true).\nThe probability that you get a value more extreme than 1.96 or -1.96 is 5% (0.05 in area cobining the two area at the edges).\n\n(Note: irrespective of the type of tests, the distribution of t-statistics is the same.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Rule\n\n\nReject the null hypothesis if the absolute value of the t-statistic is greater than 1.96.\n\n\n\n\nIf you follow the decision rule, then you have a 5% chance that you are wrong in rejecting the null hypothesis of \\(\\beta_j = 0\\). Here,\n\n5% is the  significance level \n1.96 is the  critical value  above which you will reject the null\n\n\n\n\n\n\n\n\n\n\n\nDefinition\n\n\nThe smallest significance level at which the null hypothesis would be rejected (the probability of observing a test statistic at least as extreme as we did if the null hypothesis is true)\n\n\n\n\n\n\n\nSuppose the t-statistic you got is 2.16. Then, there’s a 3.1% chance you reject the null when it is actually true, if you use it as the critical value.\nSo, the lower significance level the null hypothesis is rejected is 3.1%, which is the definition of p-value.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision rule\n\n\nIf the p-value is lower than your choice of significance level, then you reject the null.\n\n\n\n\nThis decision rule of course results in the same test results as the one we saw that uses a t-value and critical value.\n\n\nEstimated Model\nThe impact of experience on wage:\n\n\\(log(wage) = 0.284+0.092\\times educ+0.0041\\times exper + 0.022 \\times tenure\\)\n\\(\\widehat{se(\\widehat{\\beta}_{exper})} = 0.0017\\)\n\\(n = 526\\)\n\n\nHypothesis\n\n\\(H_0\\): \\(\\beta_{exper}=0\\)\n\\(H_1\\): \\(\\beta_{exper}&gt;0\\)\n\n\nTest\nt-statistic \\(= 0.0041/0.0017 = 2.41\\)\nThe critical value is the 99% quantile of \\(t_{526-3-1}\\), which is \\(2.33\\) (it can be obtained by qt(0.95, 522))\nSince \\(2.41 &gt; 2.33\\), we reject the null in favor of the alternative hypothesis at the 1% level."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#r-implementation",
    "href": "lectures/05-testing/05-testing.html#r-implementation",
    "title": "05: Hypothesis Testing",
    "section": "R implementation",
    "text": "R implementation\n\nRun regressionObtain t-statisticsR implementation (by hand)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nYou can apply the broom::tidy() function from the broom package to access the regression results (reg_wage here) as a tibble (data.frame).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nestimate: coefficient estimates \\((\\widehat{\\beta}_j)\\)\nstd.error: \\(\\widehat{se(\\widehat{\\beta}_j}\\))\nstatistic: t-statistic for the null of \\(\\beta_j = 0\\)\np.value: p-value (for the two sided test with the null of \\(\\beta_j = 0\\))\n\nSo, for the t-test of of \\(\\beta_j = 0\\) is already there. You do not need to do anything further.\nFor the null hypothesis other than \\(\\beta_j = 0\\), you need further work.\n\n\nSuppose you are testing the null hypothesis of \\(\\beta_{educ} = 1\\) against the alternative hypothesis of \\(\\beta_{educ} \\ne 1\\) (so, this is a two-sided test).\nThe t-value for this test is not available from the summary.\n\nget t-valuecritical value and t-test\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nThe degree of freedom \\((n-k-1)\\) of the t-distribution can be obtained by applying degrees_freedom(reg_wage, \"resid\"):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can get the 97.5% quantile of the \\(t_{522}\\) using the qt() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSince the absolute value of the t-value (-7.8199528) is greater than the critical value (1.9645189), you reject the null."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#confidence-interval-ci-1",
    "href": "lectures/05-testing/05-testing.html#confidence-interval-ci-1",
    "title": "05: Hypothesis Testing",
    "section": "Confidence Interval (CI)",
    "text": "Confidence Interval (CI)\n\nDefinitionHow to get CI (in general)How to get the CI of coefficientsExample (R implementation)\n\n\n\n\n\n\nDefinition\n\n\nIf you calculate the 95% confidence interval on multiple different samples, 95% of the time, the calculated CI includes the true parameter\n\n\n\n\n\nWhat confidence interval is not\nThe probability that a realized CI calculated from specific sample data includes the true parameter\n\n\n\nGeneral ProcedureExample\n\n\nFor the assumed distribution of statistic \\(x\\), the \\(A\\%\\) confidence interval of \\(x\\) is the range with\n\nlower bound: 100 − A/2 percent quantile of \\(x\\)\nupper bound: 100-(100 − A)/2 percent quantile of \\(x\\)\n\n\n\n\n\nFor the 95% CI (A = 95),\n\nlower bound: 2.5 (100 − 95/2) percent quantile of \\(x\\)\nupper bound: 97.5 (100-(100 − 95)/2) percent quantile of \\(x\\)\n\nIf \\(x\\) follows the standard normal distribution \\((x \\sim N(0, 1))\\), then,the 2.5% and 97.5% quantiles are -1.96 and 1.96, respectively. So, the 95% CI of \\(x\\) is [-1.96, 1.96].\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnder the assumption of MLR.1 through MLR.6 (which includes the normality assumption of the error), we learned that\n\\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}} \\sim t_{n-k-1}\\)\nSo, following the general procedure we discussed in the previous slide, the A% confidence interval of \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) is\n\nlower bound: \\((100 − A)/2\\)% quantile of the \\(t_{n-k-1}\\) distribution (let’s call this \\(Q_l\\))\nupper bound: \\(100 - (100 − A)/2\\)% quantile of the \\(t_{n-k-1}\\) distribution (let’s call this \\(Q_h\\))\n\nBut, we want the A% CI of \\(\\beta_j\\), not \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\). Solving for \\(\\beta_j\\),\n\\(\\beta_j = t_{n-k-1}\\times \\widehat{se(\\widehat{\\beta}_j)} + \\widehat{\\beta}_j\\)\nSo, to get the A% CI of \\(\\beta_j\\), we scale the CI of \\(\\frac{\\widehat{\\beta}_j - \\beta_j}{\\widehat{se(\\widehat{\\beta}_j)}}\\) by \\(se(\\widehat{\\beta}_j)\\) and then shift by \\(\\widehat{\\beta}_j\\).\n\nlower bound: \\(Q_l \\times se(\\widehat{\\beta}_j) + \\widehat{\\beta}_j\\)\nupper bound: \\(Q_h \\times se(\\widehat{\\beta}_j) + \\widehat{\\beta}_j\\)\n\nNote that \\(Q_l\\) is negative and \\(Q_h\\) is negative.\n\n\n\nRegressionCollect necessary informationFind CIIn practice\n\n\nRun OLS and extract necessary information\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nApplying broom::tidy() to wage_reg,\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe are interested in getting the 90% confidence interval of the coefficient on educ \\((\\beta_{educ})\\). Under all the assumptions (MLR.1 through MLR.6), we know that in general,\n\\(\\frac{\\widehat{\\beta}_{educ} - \\beta_{educ}}{\\widehat{se(\\widehat{\\beta}_{educ})}} \\sim t_{n-k-1}\\)\nSpecifically for this regression,\n\n\\(\\widehat{\\beta}_{educ}\\) = 0.5989651\n\\(\\widehat{se(\\widehat{\\beta}_{educ})}\\) = 0.0512835\n\\(n - k - 1 = 490\\) (degrees of freedom)\n\n\n\n\n\nNow, we need to find the 5% ((100-90)/2) and 95% (100-(100-90)/2) quantile of \\(t_{522}\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSo, the 90% CI of \\(\\frac{0.599 - \\beta_{educ}}{0.051} \\sim t_{522}\\) is [-1.6477779, 1.6477779]\nBy scaling and shifting, the lower and upper bounds of the 90% CI of \\(\\beta_{educ}\\) are:\n\nlower bound: 0.599 + 0.051 \\(\\times\\) -1.6477779 = 0.5149633\nupper bound: 0.599 + 0.051 \\(\\times\\) 1.6477779 = 0.6830367\n\n\nThe distribution of \\(t_{522}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can just use broom::tidy() with conf.int = TRUE, conf.level = confidence level like below:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#linear-combination-of-multiple-coefficients-1",
    "href": "lectures/05-testing/05-testing.html#linear-combination-of-multiple-coefficients-1",
    "title": "05: Hypothesis Testing",
    "section": "Linear Combination of Multiple Coefficients",
    "text": "Linear Combination of Multiple Coefficients\n\nExampleRewrite the hypothesisMathGoing back to the exampleget \\(se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)\\)Demonstration using R\n\n\nModel\n\\[log(wage) = \\beta_0+\\beta_1 jc+ \\beta_2 univ + \\beta_3 exper + u\\]\n\n\\(jc\\): 1 if you attended 2-year college, 0 otherwise\n\\(univ\\): 1 if you attended 4-year college, 0 otherwise\n\nQuestion\nDoes the impact of education on wage is greater if you attend a 4-year college than 2-year college?\nHypothesis\nThe null and alternative hypotheses would be:\n\n\\(H_1:\\) \\(\\beta_1 &lt; \\beta_2\\)\n\\(H_0:\\) \\(\\beta_1 = \\beta_2\\)\n\n\n\nThe null and alternative hypotheses are:\n\n\\(H_1:\\) \\(\\beta_1 &lt; \\beta_2\\)\n\\(H_0:\\) \\(\\beta_1 = \\beta_2\\)\n\nRewriting them,\n\n\\(H_1:\\) \\(\\beta_1-\\beta_2 &lt; 0\\)\n\\(H_0:\\) \\(\\beta_1 - \\beta_2 =0\\)\n\nOr,\n\n\\(H_1:\\) \\(\\alpha &lt; 0\\)\n\\(H_0:\\) \\(\\alpha =0\\)\n\nwhere \\(\\alpha = \\beta_1 - \\beta_2\\)\nNote that \\(\\alpha\\) is a linear combination of \\(\\beta_1\\) and \\(\\beta_2\\).\n\n\n\n\n\n\nImportant Fact\n\n\nFor any linear combination of the OLS coefficients, denoted as \\(\\widehat{\\alpha}\\), the following holds:\n\\[\\frac{\\widehat{\\alpha}-\\alpha}{\\widehat{se(\\widehat{\\alpha})}} \\sim t_{n-k-1}\\]\nWhere \\(\\alpha\\) is the true value (it is \\(\\beta_1 - \\beta_2\\) in the example in the previous slide).\n\n\n\n\n\nSo, using the example, this means that\n\\[\\frac{\\widehat{\\alpha}-\\alpha}{\\widehat{se(\\widehat{\\alpha})}} = \\frac{\\widehat{\\beta}_1-\\widehat{\\beta}_2-(\\beta_1 - \\beta_2)}{\\widehat{se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)}} \\sim t_{n-k-1}\\]\nThis is great because we know how to do t-test!\n\n\nOur null hypothesis is \\(\\alpha = 0\\) (or \\(\\beta_1 - \\beta_2 = 0\\)).\nSo,  If  indeed the null hypothesis is true, then\n\\[\\frac{\\widehat{\\alpha}-0}{\\widehat{se(\\widehat{\\alpha})}} = \\frac{\\widehat{\\beta}_1-\\widehat{\\beta}_2-0}{\\widehat{se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)}} \\sim t_{n-k-1}\\]\nSo, all you need to do is to substitute \\(\\widehat{\\beta}_1\\), \\(\\widehat{\\beta}_2\\), \\(\\widehat{se(\\widehat{\\beta}_1 - \\widehat{\\beta}_2)}\\) into the formula and see if the value is beyond the critical value for your chosen level of statistical significance.\n\n\nBut,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)= \\sqrt{Var(\\widehat{\\beta}_1-\\widehat{\\beta}_2}) \\ne \\sqrt{Var(\\widehat{\\beta}_1)+Var(\\widehat{\\beta}_2)}\\]\nIf the following was true,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2) = \\sqrt{Var(\\widehat{\\beta}_1)+Var(\\widehat{\\beta}_2)}\\]\nthen, we could have just extracted \\(Var(\\widehat{\\beta}_1)\\) and \\(Var(\\widehat{\\beta}_2)\\) individually from the regression object on R, sum them up, and take a square root of it.\n\n\n\n\nMath aside\n\n\n\\[Var(ax+by) = a^2 Var(x) + 2abCov(x,y) + b^2 Var(y)\\]\n\n\n\n\nSo,\n\\[se(\\widehat{\\beta}_1-\\widehat{\\beta}_2)= \\sqrt{Var(\\widehat{\\beta}_1-\\widehat{\\beta}_2}) = \\sqrt{Var(\\widehat{\\beta}_1)-2Cov(\\widehat{\\beta}_1,\\widehat{\\beta}_2)+Var(\\widehat{\\beta}_2)}\\]\n\n\n\nRegressionVariance covariance matrixCalculate the t-statisticIn practice\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nDefinition\n\n\nVariance covariance matrix is a matrix where\n\n\\(VCOV_{i,i}\\): the variance of \\(i\\)th variable’s coefficient estimator\n\\(VCOV_{i,j}\\): the covariance between \\(i\\)th and \\(j\\)th variables’ estimators\n\n\n\n\n\n\nYou can get it by applying vcov() to regression results:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nFor example, \\(vcov_{sc}[2, 2]\\) is the variance of \\(\\widehat{\\beta}_{jc}\\), and \\(vcov_{sc}[2, 3]\\) is the covariance between \\(\\widehat{\\beta}_{jc}\\) and \\(\\widehat{\\beta}_{univ}\\).\n\n\n\n\nGet coefficient estimates:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCalcualt t-statistic:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nIn practice, you can just use car::linearHypothesis() to implement t-test involving a linear combination of multiple coefficients.\nSyntax\n\ncar::linearHypothesis(regression resultss, expression of the test) \n\n\nExample\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease note that it runs \\(\\chi^2\\) test. When t-statistic is squared, it follows a \\(\\chi^2\\) distribution. So, it calcualtes the square of the t-statistic and look up the \\(\\chi^2\\) distribution critical value to decide whether to reject the null or not.\nLet’s confirm this:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYes, this is the same as the \\(\\chi^2\\) value reported by car::linearHypothesis(reg_sc, \"jc=univ\")\n\n\n\nImportant\n\n\nThe test results would be the same whether you run t-test or \\(\\chi^2\\) test."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#multiple-linear-restrictions-f-test-1",
    "href": "lectures/05-testing/05-testing.html#multiple-linear-restrictions-f-test-1",
    "title": "05: Hypothesis Testing",
    "section": "Multiple Linear Restrictions: F-test",
    "text": "Multiple Linear Restrictions: F-test\n\nExampleIndividual t-testF-testSSRF-test in generalF-test stepsF-test by handF-test (easier way)\n\n\n\nModel and hypothesisQuestions\n\n\nModel\n\\[log(salary) =  \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u\\]\n\n\\(salary\\): salary in 1993\n\\(years\\): years in the league\n\\(gamesyr\\): average games played per year\n\\(bavg\\): career batting average\n\\(hrunsyr\\): home runs per year\n\\(rbisyr\\): runs batted in per year\n\nHypothesis\nOnce years in the league and games per year have been controlled for, the statistics measuring performance ( \\(bavg\\), \\(hrunsyr\\), \\(rbisyr\\)) have no effect on salary collectively.\n\\(H_0\\): \\(\\beta_3=0\\), \\(\\beta_4=0\\), and \\(\\beta_5=0\\)\n\\(H_1\\): \\(H_0\\) is not true\n\n\nHow do we test this?\n\n\\(H_0\\) holds if all of \\(\\beta_3\\), \\(\\beta_4\\), or \\(\\beta_5\\) are zero.\nConduct t-test for each coefficient individually?\n\n\n\n\n\n\n\n\nRun regressionQuestionAnswer\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWhat do you find?\n\n\nNone of the coefficients on bavg, hrunsyr, and rbisyr is statistically significantly different from 0 even at the 10% level!!\nSo, does this mean that they collectively have no impact on the salary of MLB players?\nIf you were to conclude that they do not have statistically significant impact jointly, you would turn out to be wrong!!\n\\(SSR\\) (or \\(R^2\\)) turns out to be useful for testing their impacts jointly.\n\n\n\n\n\n\nIn doing an F-test of the null hypothesis, we compare sum of squared residuals \\((SSR)\\) of two models:\n\nUnrestricted Model\n\\[log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + \\beta_3 bavg + \\beta_4 hrunsyr + \\beta_5 rbisyr + u\\]\n\nRestricted Model\n\\[log(salary) = \\beta_0 + \\beta_1 years + \\beta_2 gamesyr + u\\]\nThe coefficients on \\(bavg\\), \\(hrunsyr\\), and \\(rbisyr\\) are restricted to be 0 following the null hypothesis.\n\nQuestionAnswer\n\n\nIf the null hypothesis is indeed true, then what do you think is going to happen if you compare the \\(SSR\\) of the two models? Which one has a bigger \\(SSR\\)?\n\n\n\\(SSR\\) from the restricted model should be large because the restricted model has a smaller explanatory power than the unrestricted model.\n\n\n\n\n\n\nSSR of the unrestricted model: \\(SSR_u\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSSR of the restricted model: \\(SSR_r\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nQuestion 1Answer 1Question 2Answer 2\n\n\nWhat does \\(SSR_r - SSR_u\\) measure?\n\n\nThe contribution from the three excluded variables in explaining the dependent variable.\n\n\nIs the contribution large enough to say that the excluded variables are important?\n\n\nCannot tell at this point because we do not know the distribution of the difference!\n\n\n\n\n\n\nSetup\nConsider a following general model:\n\\[\ny = \\beta_0 +\\beta_1 x_1 + \\dots+\\beta_k x_k +u\n\\]\nSuppose we have \\(q\\) restrictions to test: that is, the null hypothesis states that \\(q\\) of the variables have zero coefficients.\n\\[H_0: \\beta_{k-q+1} =0, \\beta_{k-q+2} =0, \\dots, \\beta_k=0\\]\nWhen we impose the restrictions under \\(H_0\\), the restricted model is the following:\n\\[y = \\beta_0 +\\beta_1 x_1 + \\dots+\\beta_{k-q} x_{k-q} + u\\]\nF-statistic\n If  the null hypothesis is true, then,\n\\[F = \\frac{(SSR_r-SSR_u)/q}{SSR_u/(n-k-1)} \\sim F_{q,n-k-1}\\]\n\n\\(q\\): the number of restrictions\n\\(n-k-1\\): degrees of freedom of residuals\n\n\nQuestionAnswerQuestion 2Answer 2\n\n\nIs the above \\(F\\)-statistic always positive?\n\n\nYes, because \\(SSR_r-SSR_u\\) is always positive.\n\n\nThe greater the joint contribution of the \\(q\\) variables, the (greater or smaller) the \\(F\\)-statistic?\n\n\nGreater.\n\n\n\n\n\n\n\n\nF-distribution\n\n\n\n\n\n\n\n\n\n\nF-test steps\n\nDefine the null hypothesis\nEstimate the unrestricted and restricted models to obtains their \\(SSR\\)\nCalculate \\(F\\)-statistic\nDefine the significance level and corresponding critical value according to the F distribution with appropriate degrees of freedoms\nReject if your \\(F\\)-statistic is greater than the critical value, otherwise do not reject\n\n\n\n\n\n\n\nStep 1Step 2Steps 3 and 4\n\n\nEestimate the unrestricted and restricted models\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nCalculate F-stat\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nFind the critical value\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIs F-stat &gt; critical value?\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\nObservation\n\n\nSo, the performance variables have statistically significant impacts on salary  jointly!!\nWhat happened?\n\n\n\n\n\n\n\n\n\n\n\nHowDemonstration\n\n\nYou can use the car::linearHypothesis() function from the car package.\n\nSyntax\n\ncar::linearHypothesis(regression, hypothesis)\n\n\nregression: the name of the regression results of the unrestricted model\nhypothesis” text of null hypothesis. For example,\n\nc(\"x1 = 0\", \"x2 = 1\") means the coefficients on \\(x1\\) and \\(x2\\) are \\(0\\) and \\(1\\), respectively\n\n\nThe following code test if \\(\\beta_{bavg} = 0\\), \\(\\beta_{hrunsyr} = 0\\), and \\(\\beta_{rbisyr} = 0\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#simulation-multicollinearity-t-test-and-f-test",
    "href": "lectures/05-testing/05-testing.html#simulation-multicollinearity-t-test-and-f-test",
    "title": "05: Hypothesis Testing",
    "section": "Simulation (multicollinearity, t-test, and F-test)",
    "text": "Simulation (multicollinearity, t-test, and F-test)\n\nData generationRegressionF-testImportant\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nx1 and x2 are highly correlated with each other:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nBoth x1 and x2 are statistically insignificant individually.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe F-statistic for the hypothesis testing is:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nThe F-statistic is very high, meaning they collectively affect the dependent variable significantly.\n\n\n\n\n\n\n\nThe standard error estimates of the coefficients on x1 and x2 are very high because they are so highly correlated that your estimation of the model had such a difficult time to distinguish the their individual impacts.\nBut, collectively, they have large impacts. \\(F\\)-test was able to detect the statistical significance of their impacts  collectively."
  },
  {
    "objectID": "lectures/05-testing/05-testing.html#mlb-example",
    "href": "lectures/05-testing/05-testing.html#mlb-example",
    "title": "05: Hypothesis Testing",
    "section": "MLB example",
    "text": "MLB example\n\nCorrelation checkMultiple coefficients again\n\n\nHere is the correlation coefficients between the three variables:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nAs you can see, brunsyr and hrunsyr are highly correlated with each other.\nThey are not so highly correlated with bavg.\n\n\n\n\nThe test of a linear combination of the parameters we looked at earlier is a special case of F-test where the number of restriction is 1.\nIt can be shown that square root of \\(F_{1, df}\\) follows the \\(t_{df}\\) distribution.\nSo, we can actually use \\(F\\)-test for this type of hypothesis testing because \\(F_{1,t-n-k} \\sim t_{t-n-k}^2\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nCheck with the previous slide of the same t-test and confirm that the t-statistics we got there and here are the same.\n\n\n\n\n\n\n\n\n\n\n\n\nback to course website with lecture slides"
  }
]