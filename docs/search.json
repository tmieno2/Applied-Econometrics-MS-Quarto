[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (MS)",
    "section": "",
    "text": "This website hosts course materials for Applied Econometrics (AECN 896-04) at UNL."
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\n\n\n\n\n01-1: Univariate Regression: Introduction\n\n\n\n\n01-2: Univariate Regression: OLS Mechanics and Implementation\n\n\n\n\n01-3: Univariate Regression: OLS Small Sample Property\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Small sample property of OLS estimators",
    "text": "Small sample property of OLS estimators\n\nSmall sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\hat{\\beta_1}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS estimator",
    "text": "Unbiasedness of OLS estimator\n\nUnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\nDirection\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\hat{\\beta_1}]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\hat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\nLinear in Parameters\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\nRandom sampling\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\nVariation in covariates\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\nRoughly speaking\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\hat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\hat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\hat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\hat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\hat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\hat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\hat{\\beta}_1|x] = & E[\\hat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Unbiasedness of OLS in practice",
    "text": "Unbiasedness of OLS in practice\n\nGood empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Variance of OLS estimator",
    "text": "Variance of OLS estimator\n\nIntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\beta_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\beta_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\hat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\hat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\hat{\\beta}_1) = \\sqrt{Var(\\hat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\hat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Efficiency of OLS Estimators",
    "text": "Efficiency of OLS Estimators\n\nNature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-error-variance",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-error-variance",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the error variance",
    "text": "Estimating the error variance\n\nWhy?ProblemProposalAlgebraic property of OLSR code(Math)\n\n\nOnce you estimate \\(Var(\\hat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\hat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\hat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\hat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\hat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\hat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\hat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\nHow about \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) as an estimator of \\(\\sigma^2\\)?\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\\[\\begin{align}\n    \\sum_{i=1}^n \\hat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\hat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\nUnbiased estimator of the variance of the error term\nWe use \\(\\hat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\hat{u}_i^2\\), which satisfies \\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\hat{u}_i^2]=\\sigma^2\\)\nWe use \\(\\widehat{Var(\\hat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\hat{\\beta}_j\\), and it is defined as\n\\(\\widehat{Var(\\hat{\\beta}_1)} = \\hat{\\sigma}^2/SST_x\\)\nSince \\(se(\\hat{\\beta_1})=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\hat{\\beta_1})\\) is\n\\[\\begin{aligned}\n  \\widehat{se(\\hat{\\beta_1})} =\\sqrt{\\hat{\\sigma}^2}/\\sqrt{SST_x},\n\\end{aligned}\\]\nwhich is called  standard error of \\(\\hat{\\beta_1}\\) .\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#population-sample-and-econometrics",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#population-sample-and-econometrics",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Population, Sample, and Econometrics",
    "text": "Population, Sample, and Econometrics\n\nPopulationSample\n\n\n\n\n\n\nDefinition\n\n\nA set of \\(ALL\\) individuals, items, phenomenon, that you are interested in learning about\n\n\n\n\n\nExample\n\nSuppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.\nSuppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.\n\nImportant\nPopulation differs depending on the scope of your interest\n\nIf you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world\nIf you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.\n\n\n\n\n\n\n\nDefinition\n\n\nSample is a subset of population that you observe\n\n\n\n\n\nCase 1Case 2\n\n\n\nPopulation: you are interested in the impact of education on wage\nSample (example): data on education, income, and many other things for 300 individuals from each State\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?\n\n\n\n\n\n\n\nPopulation: you are interested in the impact of water price on irrigation by farmers in Nebraska\nSample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#simple-univariate-model",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#simple-univariate-model",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Simple univariate model",
    "text": "Simple univariate model\n\nWhat is it?What does \\(\\beta_1\\) measure?What does \\(\\beta_0\\) measure?Visualized\n\n\nConsider a phenomenon in the population that is correctly represented by the following model ( This is the model you want to learn about using sample ):\n\\[\\begin{equation}\ny=\\beta_0+\\beta_1 x + u\n\\end{equation}\\]\n\n\\(y\\): to be explained by \\(x\\) ( dependent variable)\n\\(x\\): explain \\(y\\) ( independent variable ,  covariate ,  explanatory variable )\n\\(u\\): parts of \\(y\\) that cannot be explained by \\(x\\) ( error term )\n\\(\\beta_0\\) and \\(\\beta_1\\): real numbers that gives the model a quantitative meaning ( parameters )\n\n\n\n\n\nImportant\n\n\nYou will never know the true model. You can try estimating it using sample! That is what statistics is about.\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nIf you change \\(x\\) by \\(1\\) unit while holding \\(u\\) (everything else) constant,\n\\[\\begin{align}\n  y_{before} & = \\beta_0+\\beta_1 x + u \\\\\n  y_{after} & = \\beta_0+\\beta_1 (x + 1) + u\n\\end{align}\\]\nThe difference in \\(y_{before}\\) and \\(y_{after}\\),\n\\[\\begin{align}\n  \\Delta y = \\beta_1\n\\end{align}\\]\nThat is, \\(y\\) changes by \\(\\beta_1\\).\n\n\n\n\n\nSo,\n\n\n\n\\(\\beta_1\\) is the change in \\(y\\) when \\(x\\) increases by 1\nWe call \\(\\beta_1\\) the  ceteris paribus  (with everything else fixed) causal impact of \\(x\\) on \\(y\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nWhen \\(x = 0\\) and \\(u=0\\),\n\\[\\begin{align}\ny=\\beta_0\n\\end{align}\\]\nSo, \\(\\beta_0\\) represents the intercept.\n\n\n\n\n\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): coefficient (slope)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#why-do-we-want-ceteris-paribus-causal-impact",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#why-do-we-want-ceteris-paribus-causal-impact",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Why do we want  ceteris paribus  causal impact?",
    "text": "Why do we want  ceteris paribus  causal impact?\n\nExampleWhy ceteris paribus impact?What do you observe?\n\n\nQuality of College\nYou\n\nhave been admitted to University A (better, more expensive) and B (worse, less expensive)\nare trying to decide which school to attend\nare interested in knowing a boost in your future income to make a decision\n\nYou have found the following data\n\n\nUniversityaverage incomesample sizeA130.13500B90.13500\n\n\nQuestion\nShould you assume that the observed difference of 40 is the expected boost you would get if you are to attend University A instead of B?\n\n\nLet’s say your ability score is \\(6\\) out of \\(10\\) (the higher, the better),\n\\[\\mbox{(1)}\\;\\; E[inc|A,ability=9] -E[inc|B,ability=6]\\] \\[\\mbox{(2)}\\;\\; E[inc|A,ability=6] -E[inc|B,ability=6]\\]\nWhich one would like you to know?\n\n\n\n\n\nImportant\n\n\n\nYou want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A\nYou do not want the impact of school quality to be confounded with something else\n\n\n\n\n\n\n\n\n\n\n\nAside: Conditional Expectation\n\n\n\\(E[Y|X]\\) represents expected value of \\(Y\\) conditional on \\(X\\) (For a given value of \\(X\\), the expected value of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nred line: \\(E[income|A, ability]\\)\nblue line: \\(E[income|B, ability]\\)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#example-corn-yield-and-fertilizer",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#example-corn-yield-and-fertilizer",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Example: corn yield and fertilizer",
    "text": "Example: corn yield and fertilizer\n\nModelEstimate \\(\\beta_1\\)Crucial conditionCondition satisfied?Math asides\n\n\nCorn yield and fertilizer\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\nQuestion\nWhat is in the error term?\n\n\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\n\nyou do not know \\(\\beta_0\\) and \\(\\beta_1\\), and would like to estimate them\nyou observe a series of \\(\\{yield_i,fertilizer_i\\}\\) combinations \\((i=1,\\dots,n)\\)\nyou would like to estiamte \\(\\beta_1\\), the impact of fertilizer on yield, ceteris paribus (with everything else fixed)\n\nQuestion\nHow could we possibly find the ceteris paribus impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?\n\n\nIt turns out we can identify the ceteris paribus causal impact of \\(x\\) on \\(y\\) as long as the following condition is satisfied:\n\n\n\n\nZero conditional mean\n\n\n\\(E(u|x) = 0\\)\n\n\n\n\nThis is satisfied when \\(E[u|x]=E[u]\\) and \\(E[u] = 0\\). Practically (and roughtly) speaking, this condition is satisfied if\n\n\n\nImportant\n\n\n\n the error term (\\(u\\)) is not correlated with \\(x\\) \n\nan intercept (\\(\\beta_0\\)) is included in the model (which we almost always do by default)\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer + u\n\\end{align}\\]\n\nData\nYou have collected farm-level yield-fertilizer data from 200 farmers in year 2023.\n\nQuestions\n\nWhat’s in \\(u\\)? (note that factors that do not affect yield are not part of \\(u\\))\nIs it correlated with fertilizer?\n\n\n\n\nMean independenceCorrelation and mean independence\\(E(u)=0\\)\n\n\n\n\n\n\nDefinition: Mean Independence\n\n\n\\(E[u|x]=E[u]\\)\n\n\n\n\n\nverbally: the average value of the error term (collection of all the unobservables) is the same at any value of \\(x\\), and that the common average is equal to the average of \\(u\\) over the entire population\n(almost) interchangeably: the error term is not correlated with \\(x\\)\n\n\n\nMean independence of \\(u\\) and \\(x\\) implies no correlation. But, no correlation does not imply mean independence.\n\\[\\begin{aligned}\n    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\\\\\\n    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\\\\\\n    = & E[ux] \\\\\\\\\n    = & E_x[E_u[u|x]] \\;\\; \\mbox{(iterated law of expectation)}\n\\end{aligned}\\]\nIf zero conditional mean condition \\((E(u|x)=0)\\) is satisfied,\n\\[\\begin{aligned}\n    Cov(u,x)= & E_x[0] = 0\n\\end{aligned}\\]\n\n\nExpected value of the error term is 0 \\((E(u)=0)\\).\nThis is always satisfied as long as an intercept is included in the model:\n\\[y = \\beta_0 + \\beta_1 x + u_1,\\;\\; \\mbox{where}\\;\\; E(u_1)=\\alpha\\]\nRewriting the model,\n\\[\\begin{aligned}\ny & = \\beta_0 + \\alpha + \\beta_1 x + u_1 - \\alpha \\\\\\\\\n  & = \\gamma_0 + \\beta_1 x + u_2\n\\end{aligned}\\]\nwhere, \\(\\gamma_0=\\beta_0+\\alpha\\) and \\(u_2=u_1-\\alpha\\).\nNow, \\(E[u_2]=0\\)."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#going-back-to-the-college-income-example",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#going-back-to-the-college-income-example",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Going back to the college-income example",
    "text": "Going back to the college-income example\nThe model\n\\[\nIncome = \\beta_0+\\beta_1 College\\;\\; A + u\n\\]\nwhere \\(College\\;\\; A\\) is 1 if attending college A, 0 if attending college B, and \\(u\\) is the error term that includes ability. \\(u\\) includes ability.\n\nZero conditional mean satisfied?\n\\[\nE[u(ability)|college A] = 0?\n\\]\nThat is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?\n\n\n\n\n\n\n\n\n\nThis is what it would like if college choice and ability are not correlated:"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#exercise",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#exercise",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Exercise",
    "text": "Exercise\n\nconsider a phenomenon you are interested in understanding\n\ndependent variable (variable to be explained)\nexplanatory variable (variable to explain)\n\nconstruct a simple linear model\nidentify what is in the error term\ncheck if they are correlated withe explanatory variable or not"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the variance of error",
    "text": "Estimating the variance of error\n\nWhy?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\)\n\n\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\nNote\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  }
]