[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Econometrics (MS)",
    "section": "",
    "text": "This website hosts course materials for Applied Econometrics (AECN 896-04) at UNL."
  },
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Title\n\n\n\n\n\n\n01-1: Univariate Regression: Introduction\n\n\n\n\n01-2: Univariate Regression: OLS Mechanics and Implementation\n\n\n\n\n01-3: Univariate Regression: OLS Small Sample Property\n\n\n\n\n02-Multivariate Regression\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#small-sample-property-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "Small sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\widehat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "UnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nDirection\n\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\widehat{\\beta}_1]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\widehat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\n\n\nLinear in Parameters\n\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\n\n\nRandom sampling\n\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\n\n\nVariation in covariates\n\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\n\n\nRoughly speaking\n\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\widehat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\widehat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\widehat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\widehat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\widehat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\widehat{\\beta}_1|x] = & E[\\widehat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#unbiasedness-of-ols-in-practice",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "Good empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#variance-of-ols-estimator",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "IntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\widehat{\\beta}_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\widehat{\\beta}_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\widehat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\widehat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\widehat{\\beta}_1) = \\sqrt{Var(\\widehat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#efficiency-of-ols-estimators",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "Nature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-error-variance",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-error-variance",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "Estimating the error variance",
    "text": "Estimating the error variance\n\nWhy?ProblemProposalAlgebraic property of OLSR code(Math)\n\n\nOnce you estimate \\(Var(\\hat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\hat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\hat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\hat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\nBut,\n\n\nWe observe \\(\\hat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\hat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\hat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\nHow about \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) as an estimator of \\(\\sigma^2\\)?\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\\[\\begin{align}\n    \\sum_{i=1}^n \\hat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\hat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\nUnbiased estimator of the variance of the error term\nWe use \\(\\hat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\hat{u}_i^2\\), which satisfies \\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\hat{u}_i^2]=\\sigma^2\\)\nWe use \\(\\widehat{Var(\\hat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\hat{\\beta}_j\\), and it is defined as\n\\(\\widehat{Var(\\hat{\\beta}_1)} = \\hat{\\sigma}^2/SST_x\\)\nSince \\(se(\\hat{\\beta_1})=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\hat{\\beta_1})\\) is\n\\[\\begin{aligned}\n  \\widehat{se(\\hat{\\beta_1})} =\\sqrt{\\hat{\\sigma}^2}/\\sqrt{SST_x},\n\\end{aligned}\\]\nwhich is called  standard error of \\(\\hat{\\beta_1}\\) .\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#population-sample-and-econometrics",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#population-sample-and-econometrics",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Population, Sample, and Econometrics",
    "text": "Population, Sample, and Econometrics\n\nPopulationSample\n\n\n\n\n\n\nDefinition\n\n\nA set of \\(ALL\\) individuals, items, phenomenon, that you are interested in learning about\n\n\n\n\n\nExample\n\nSuppose you are interested in the impact of eduction on income across the U.S. Then, the population is all the individuals in U.S.\nSuppose you are interested in the impact of water pricing on irrigation water demand for farmers in NE. Then, your population is all the farmers in NE.\n\nImportant\nPopulation differs depending on the scope of your interest\n\nIf you are interested in understanding the impact of COVID-19 on child education achievement at the global scale, then your population is every single kid in the world\nIf you are interested in understanding the impact of COVID-19 on child education achievement in U.S., then your population is every single kid in U.S.\n\n\n\n\n\n\n\nDefinition\n\n\nSample is a subset of population that you observe\n\n\n\n\n\nCase 1Case 2\n\n\n\nPopulation: you are interested in the impact of education on wage\nSample (example): data on education, income, and many other things for 300 individuals from each State\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?\n\n\n\n\n\n\n\nPopulation: you are interested in the impact of water price on irrigation by farmers in Nebraska\nSample (example): data on water price, irrigation water use, and many other things for 500 farmers who farm in the Upper Republican Basin (southwest corner of NE)\n\n\n\n\n\nQuestion\n\n\nIs the sample representative of the population?"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#simple-univariate-model",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#simple-univariate-model",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Simple univariate model",
    "text": "Simple univariate model\n\nWhat is it?What does \\(\\beta_1\\) measure?What does \\(\\beta_0\\) measure?Visualized\n\n\nConsider a phenomenon in the population that is correctly represented by the following model ( This is the model you want to learn about using sample ):\n\\[\\begin{equation}\ny=\\beta_0+\\beta_1 x + u\n\\end{equation}\\]\n\n\\(y\\): to be explained by \\(x\\) ( dependent variable)\n\\(x\\): explain \\(y\\) ( independent variable ,  covariate ,  explanatory variable )\n\\(u\\): parts of \\(y\\) that cannot be explained by \\(x\\) ( error term )\n\\(\\beta_0\\) and \\(\\beta_1\\): real numbers that gives the model a quantitative meaning ( parameters )\n\n\n\n\n\nImportant\n\n\nYou will never know the true model. You can try estimating it using sample! That is what statistics is about.\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nIf you change \\(x\\) by \\(1\\) unit while holding \\(u\\) (everything else) constant,\n\\[\\begin{align}\n  y_{before} & = \\beta_0+\\beta_1 x + u \\\\\n  y_{after} & = \\beta_0+\\beta_1 (x + 1) + u\n\\end{align}\\]\nThe difference in \\(y_{before}\\) and \\(y_{after}\\),\n\\[\\begin{align}\n  \\Delta y = \\beta_1\n\\end{align}\\]\nThat is, \\(y\\) changes by \\(\\beta_1\\).\n\n\n\n\n\nSo,\n\n\n\n\\(\\beta_1\\) is the change in \\(y\\) when \\(x\\) increases by 1\nWe call \\(\\beta_1\\) the  ceteris paribus  (with everything else fixed) causal impact of \\(x\\) on \\(y\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\ny=\\beta_0+\\beta_1 x + u\n\\end{align}\\]\nWhen \\(x = 0\\) and \\(u=0\\),\n\\[\\begin{align}\ny=\\beta_0\n\\end{align}\\]\nSo, \\(\\beta_0\\) represents the intercept.\n\n\n\n\n\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): coefficient (slope)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#why-do-we-want-ceteris-paribus-causal-impact",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#why-do-we-want-ceteris-paribus-causal-impact",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Why do we want  ceteris paribus  causal impact?",
    "text": "Why do we want  ceteris paribus  causal impact?\n\nExampleWhy ceteris paribus impact?What do you observe?\n\n\nQuality of College\nYou\n\nhave been admitted to University A (better, more expensive) and B (worse, less expensive)\nare trying to decide which school to attend\nare interested in knowing a boost in your future income to make a decision\n\nYou have found the following data\n\n\nUniversityaverage incomesample sizeA130.13500B90.13500\n\n\nQuestion\nShould you assume that the observed difference of 40 is the expected boost you would get if you are to attend University A instead of B?\n\n\nLet’s say your ability score is \\(6\\) out of \\(10\\) (the higher, the better),\n\\[\\mbox{(1)}\\;\\; E[inc|A,ability=9] -E[inc|B,ability=6]\\] \\[\\mbox{(2)}\\;\\; E[inc|A,ability=6] -E[inc|B,ability=6]\\]\nWhich one would like you to know?\n\n\n\n\n\nImportant\n\n\n\nYou want ability (an unobservable) to stay fixed when you change the quality of school because your innate ability is not going to miraculously increase by simply attending school A\nYou do not want the impact of school quality to be confounded with something else\n\n\n\n\n\n\n\n\n\n\n\nAside: Conditional Expectation\n\n\n\\(E[Y|X]\\) represents expected value of \\(Y\\) conditional on \\(X\\) (For a given value of \\(X\\), the expected value of \\(Y\\)).\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nred line: \\(E[income|A, ability]\\)\nblue line: \\(E[income|B, ability]\\)"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#example-corn-yield-and-fertilizer",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#example-corn-yield-and-fertilizer",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Example: corn yield and fertilizer",
    "text": "Example: corn yield and fertilizer\n\nModelEstimate \\(\\beta_1\\)Crucial conditionCondition satisfied?Math asides\n\n\nCorn yield and fertilizer\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\nQuestion\nWhat is in the error term?\n\n\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer+u\n\\end{align}\\]\n\nyou do not know \\(\\beta_0\\) and \\(\\beta_1\\), and would like to estimate them\nyou observe a series of \\(\\{yield_i,fertilizer_i\\}\\) combinations \\((i=1,\\dots,n)\\)\nyou would like to estiamte \\(\\beta_1\\), the impact of fertilizer on yield, ceteris paribus (with everything else fixed)\n\nQuestion\nHow could we possibly find the ceteris paribus impact of fertilizer on yield when we do not observe whole bunch of other factors (error term)?\n\n\nIt turns out we can identify the ceteris paribus causal impact of \\(x\\) on \\(y\\) as long as the following condition is satisfied:\n\n\n\n\nZero conditional mean\n\n\n\\(E(u|x) = 0\\)\n\n\n\n\nThis is satisfied when \\(E[u|x]=E[u]\\) and \\(E[u] = 0\\). Practically (and roughtly) speaking, this condition is satisfied if\n\n\n\nImportant\n\n\n\n the error term (\\(u\\)) is not correlated with \\(x\\) \n\nan intercept (\\(\\beta_0\\)) is included in the model (which we almost always do by default)\n\n\n\n\n\n\nModel\n\\[\\begin{align}\n  yield=\\beta_0+\\beta_1 fertilizer + u\n\\end{align}\\]\n\nData\nYou have collected farm-level yield-fertilizer data from 200 farmers in year 2023.\n\nQuestions\n\nWhat’s in \\(u\\)? (note that factors that do not affect yield are not part of \\(u\\))\nIs it correlated with fertilizer?\n\n\n\n\nMean independenceCorrelation and mean independence\\(E(u)=0\\)\n\n\n\n\n\n\nDefinition: Mean Independence\n\n\n\\(E[u|x]=E[u]\\)\n\n\n\n\n\nverbally: the average value of the error term (collection of all the unobservables) is the same at any value of \\(x\\), and that the common average is equal to the average of \\(u\\) over the entire population\n(almost) interchangeably: the error term is not correlated with \\(x\\)\n\n\n\nMean independence of \\(u\\) and \\(x\\) implies no correlation. But, no correlation does not imply mean independence.\n\\[\\begin{aligned}\n    Cov(u,x)= & E[(u-E[u])(x-E[x])] \\\\\\\\\n    = & E[ux]-E[u]E[x]-E[u]E[x]+E[u]E[x]\\\\\\\\\n    = & E[ux] \\\\\\\\\n    = & E_x[E_u[u|x]] \\;\\; \\mbox{(iterated law of expectation)}\n\\end{aligned}\\]\nIf zero conditional mean condition \\((E(u|x)=0)\\) is satisfied,\n\\[\\begin{aligned}\n    Cov(u,x)= & E_x[0] = 0\n\\end{aligned}\\]\n\n\nExpected value of the error term is 0 \\((E(u)=0)\\).\nThis is always satisfied as long as an intercept is included in the model:\n\\[y = \\beta_0 + \\beta_1 x + u_1,\\;\\; \\mbox{where}\\;\\; E(u_1)=\\alpha\\]\nRewriting the model,\n\\[\\begin{aligned}\ny & = \\beta_0 + \\alpha + \\beta_1 x + u_1 - \\alpha \\\\\\\\\n  & = \\gamma_0 + \\beta_1 x + u_2\n\\end{aligned}\\]\nwhere, \\(\\gamma_0=\\beta_0+\\alpha\\) and \\(u_2=u_1-\\alpha\\).\nNow, \\(E[u_2]=0\\)."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#going-back-to-the-college-income-example",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#going-back-to-the-college-income-example",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Going back to the college-income example",
    "text": "Going back to the college-income example\nThe model\n\\[\nIncome = \\beta_0+\\beta_1 College\\;\\; A + u\n\\]\nwhere \\(College\\;\\; A\\) is 1 if attending college A, 0 if attending college B, and \\(u\\) is the error term that includes ability. \\(u\\) includes ability.\n\nZero conditional mean satisfied?\n\\[\nE[u(ability)|college A] = 0?\n\\]\nThat is, are attending college A and ability (correlate) systematically related with each other? Or, is college choice (and acceptance of course) correlated with ability?\n\n\n\n\n\n\n\n\n\nThis is what it would like if college choice and ability are not correlated:"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#exercise",
    "href": "lectures/01-univariate-introduction/01-1-univariate_regression_x.html#exercise",
    "title": "01-1: Univariate Regression: Introduction",
    "section": "Exercise",
    "text": "Exercise\n\nconsider a phenomenon you are interested in understanding\n\ndependent variable (variable to be explained)\nexplanatory variable (variable to explain)\n\nconstruct a simple linear model\nidentify what is in the error term\ncheck if they are correlated withe explanatory variable or not"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html#estimating-the-variance-of-error",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "Why?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\n\n\nBut,\n\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\widehat{u}_i^2\\)\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\n\n\n\nNote\n\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#the-data-set-and-model",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "The data set and model",
    "text": "The data set and model\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nSet upR code to get dataData visualization\n\n\nData\nObservations of house price and lot size for 546 houses.\n\nModel\n\\[price_i = \\beta_0 + \\beta_1 lotsize_i+u_i\\]\n\n\\(price_i\\): house price ($) of house \\(i\\)\n\\(lotsize_i\\): lot size of house \\(i\\)\n\\(u_i\\): error term (everything else) of house \\(i\\)\n\n\nObjective\nEstimate the impact of lot size on house price\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#estimation-with-ols",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Estimation with OLS",
    "text": "Estimation with OLS\n\nRough ideaExamplesResidualsOLSVisualizationDerivationEstimators vs Estimates\n\n\n\nWe want to draw a line like this, the slope of which is an estimate of \\(\\beta_1\\)\nA way: Ordinary Least Squares (OLS)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nEx. 1: \\(\\widehat{\\beta}_0=20000\\), \\(\\widehat{\\beta}_1=7\\)Ex. 2: \\(\\widehat{\\beta}_0=70000\\), \\(\\widehat{\\beta}_1=3.8\\)So,\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion\n\n\n\nAmong all the possible values of \\(\\beta_0\\) and \\(\\beta_1\\), which one is the best?\nWhat criteria do we use (what does the best even mean?)\n\n\n\n\n\n\n\n\n\n\n\nFor particular values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) you pick, the modeled value of \\(y\\) for individual \\(i\\) is \\(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i\\).\nThen, the residual for individual \\(i\\) is:\n\\[\n\\widehat{u}_i =  y_i - (\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x_i)\n\\]\nThat is, residual is the observed value of the dependent variable less the value of modeled value. For different values of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\), you have a different value of residual.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nIdea of OLS (Ordinary Least Squares)\nLet’s find the value of \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the squared residuals!\n\nMathematically\nSolve the following minimization problem:\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n \\widehat{u}_i^2, \\mbox{where} \\;\\; \\widehat{u}_i=y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)\\]\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nQuestions\n\nWhy do we square the residuals, and then sum them up together? What’s gonna happen if you just sum up residuals?\nHow about taking the absolute value of residuals, and then sum them up?\n\n\n\nMinimization problem to solve\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nSteps\n\npartial differentiation of the objective function with respect to \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\nsolve for \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\)\n\n\\[Min_{\\widehat{\\beta}_0,\\widehat{\\beta}_1} \\sum_{i=1}^n [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]^2\\]\nFOC\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{align}\n\\frac{\\partial }{\\partial \\widehat{\\beta}_0}=& 2 \\sumn [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]=0 \\\\\\\\\n\\frac{\\partial }{\\partial \\widehat{\\beta}_1}=& 2 \\sumn x_i\\cdot [y_i-(\\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i)]= \\sumn x_i\\cdot \\widehat{u}_i = 0\n\\end{align}\n\\]\nOLS estimators: analytical formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2},\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}, \\\\\\\\\n  \\mbox{where} & \\;\\; \\bar{y} = \\sumn y_i/n \\;\\; \\mbox{and} \\;\\;\\bar{x} = \\sumn x_i/n\n\\end{aligned}\n\\]\n\n\nEstimators\nSpecific  rules (formula)  to use once you get the data\n\nEstimates\nNumbers you get once you plug values (your data) into the formula"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#ols-demonstration-in-r-1",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "OLS demonstration in R",
    "text": "OLS demonstration in R\n\nR code: hard wayR code: a better waypost-estimation\n\n\nOLS Estimator Formula\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n  \\widehat{\\beta}_1 & = \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}\\\\\\\\\n  \\widehat{\\beta}_0 & = \\bar{y}-\\widehat{\\beta}_1 \\bar{x}\n\\end{aligned}\n\\]\nR code\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWe can use the feols() function from the fixest package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nLots of information is stored in the regression results (here, uni_reg), which is of class list.\nApply ls() to see its elements:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimated coefficients:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPredicted values at the observation points:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nResiduals:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nYou can have a nice quick summary of the regression results with summary() function:\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#once-the-model-is-estimated",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "Once the model is estimated",
    "text": "Once the model is estimated\n\nEstimated modelPredicted values (R)New predictions (R)Exercise: The impact of lotsize\n\n\nModel to be estimated\n\\[\nprice = \\beta_0 + \\beta_1 lotsize + u\n\\]\n\nEstimated Model\nThis is the estimated version of the expected value of \\(y\\) conditional on \\(x\\).\n\\[\nprice =  3.4136\\times 10^{4} + 6.599 \\times lotsize\n\\]\nThis is called  sample regression function (SRF) , and it is an estimation of \\(E[price|lotsize]\\), the  population regression function (PRF).\n\n\n\nImportant\n\n\n\nOLS regression predicts the  expected  value of the dependent variable  conditional on the explanatory variables.\n\\(\\widehat{\\beta}_1\\) is an estimate of how a change in \\(x\\) affects the  expected  value of \\(y\\).\n\n\n\n\n\n\nYou can access the predicted values at the observed points by looking at the fitted.value element of the regression results.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nTo calculate the predicted value at arbitrary values of \\(x\\),\n\ncreate a new data.frame with values of \\(x\\) of your choice.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\napply predict() to the data.frame using the regression results.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nProblemSolution\n\n\nYour current lot size is 3000. You are thinking of expanding your lot by 1000 (with everything else fixed), which would cost you 5,000 USD. Should you do it? Use R to figure it out.\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page."
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "href": "lectures/01-univariate-introduction/01-2-univariate_ols_mechanics.html#r2-goodness-of-fit",
    "title": "01-2: Univariate Regression: OLS Mechanics and Implementation",
    "section": "\\(R^2\\): Goodness of fit",
    "text": "\\(R^2\\): Goodness of fit\n\nWhat is it?Decompose \\(y\\)Visualization\\(R^2\\) componentsDefinition of \\(R^2\\)Caveat\n\n\n\\(R^2\\) is a measure of how good your model is in predicting the dependent variable (explaining variations in the dependent variable)  compared  to just using the average of the dependent variable as the predictor.\n\n\nYou can decompose observed value of \\(y\\) into two parts: fitted value and residual\n\\[\ny_i=\\widehat{y}_i +\\widehat{u}_i, \\;\\;\\mbox{where}\\;\\; \\widehat{y}_i = \\widehat{\\beta}_0+\\widehat{\\beta}_1 x_i\n\\]\nnow, subtracting \\(\\bar{y}\\) (sample average of \\(y\\)),\n\\[\ny_i-\\bar{y}=\\widehat{y}_i-\\bar{y}+\\widehat{u}_i\n\\]\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\n\n\n\\(y_i-\\bar{y}\\): how far away the actual value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (actual deviation from the mean)\n\\(\\widehat{y_i}-\\bar{y}\\): how far away the predicted value of \\(y\\) for \\(i\\)th observation from the sample average \\(\\bar{y}\\) is (explained deviation from the mean)\n\\(\\widehat{u_i}\\): the residual for \\(i\\)th observation\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\ntotal sum of squares (SST)\n\\[\nSST\\equiv \\sum_{i=1}^{n}(y_i-\\bar{y})^2\n\\]\nexplained sum of squares (SSE) \\[\nSSE\\equiv \\sum_{i=1}^{n}(\\widehat{y}_i-\\bar{y})^2\n\\]\nresidual sum of squares (SSR) \\[\nSSR\\equiv \\sum_{i=1}^{n}\\widehat{u}_i^2\n\\]\n\n\n\n\n\n\nDefinition\n\n\n\\(R^2 = 1 - SSR/SST\\)\n\n\n\n\n\nWhere did it come from?\n\\[\\begin{align}\n& SST = SSE + SSR  \\\\\n\\Rightarrow & SSE = SST - SSR \\\\\n\\Rightarrow & SSE/SST = 1 - SSR/SST = R^2\\\\\n\\end{align}\\]\nThe value of \\(R^2\\) always lies between \\(0\\) and \\(1\\) as long as an intercept is included in the econometric model.\n\nWhat does it measure?\n\\(R^2\\) is a measure of how much improvement  in predictin the depdent variable  you’ve made by including independent variable(s) \\((y=\\beta_0+\\beta_1 x+u)\\) compared to when simply using the mean of dependent variable as the predictor \\((y=\\beta_0+u)\\).\n\n\nImportant\n\n\\(R^2\\) tells  nothing  about how well you have estimated the causal ceteris paribus impact of \\(x\\) on \\(y\\) \\((\\beta_1)\\).\nAs an economist, we typically do not care about how well we can prefict yield, rather we care about how well we have predicted \\(\\beta\\).\n\nProblem\n\nWhile we observe the dependent variable (otherwise you cannot run regression), we cannot observe \\(\\beta_1\\).\nSo, we get to check how good estimated models are in predicting the dependent variable (which we do not care), but we can  never  test whether they have estimated \\(\\beta_1\\) well.\nThis means that we need to carefully examines whether the  assumptions  necessary for good estimation of \\(\\beta_1\\) is satisfied (next topic)."
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#univariate-vs-multivariate-regression-models",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#univariate-vs-multivariate-regression-models",
    "title": "02-Multivariate Regression",
    "section": "",
    "text": "Bi-variate vs. Uni-variateExampleExampleModel (general)\n\n\nUnivariate\nThe most important assumption \\(E[u|x] = 0\\) (zero conditional mean) is almost always violated (unless you data comes from randomized experiments) because all the other variables are sitting in the error term, which can be correlated with \\(x\\).\n\nMultivariate\nMore independent variables mean less factors left in the error term, which makes the endogeneity problem  less severe\n\n\n\\[\\begin{align}\n  \\mbox{Bi-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u_2 \\\\\n  \\mbox{Uni-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + u_1 (=u_2+\\beta_2 exper)\n\\end{align}\\]\nWhat’s different?\n\nbi-variate: able to measure the effect of education on wage,  holding experience fixed  because experience is modeled explicitly ( We say \\(exper\\) is controlled for. )\nuni-variate: \\(\\widehat{\\beta_1}\\) is biased unless experience is uncorrelated with education because experience was in error term\n\n\n\nThe impact of per student spending (expend) on standardized test score (avgscore) at the high school level\n\\[\\begin{align}\navgscore= & \\beta_0+\\beta_1 expend + u_1 (=u_2+\\beta_2 avginc) \\notag \\\\\navgscore= & \\beta_0+\\beta_1 expend +\\beta_2 avginc + u_2 \\notag\n\\end{align}\\]\n\n\nMore generally,\n\\[\\begin{align}\n  y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + u\n\\end{align}\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\\(\\beta_2\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#the-crucial-condition-assumption-for-unbiasedness-of-the-ols-estimator",
    "title": "02-Multivariate Regression",
    "section": "",
    "text": "Uni-variate v.s. Bi-variateMean independence condition: example\n\n\nUni-variate\nFor \\(y = \\beta_0 + \\beta_1x + u\\),\n\\(E[u|x]=0\\)\n\nBi-variate\nFor \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\),\n\nMathematically: \\(E[u|x_1,x_2]=0\\)\nVerbally: for any values of \\(x_1\\) and \\(x_2\\), the expected value of the unobservables is zero\n\n\n\nIn the following wage model,\n\\[\\begin{align*}\nwage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\n\\end{align*}\\]\nMean independence condition is\n\\[\\begin{align}\n  E[u|educ,exper]=0\n\\end{align}\\]\nVerbally:\nThis condition would be satisfied if innate ability of students is on average unrelated to education level and experience."
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#the-model-with-k-independent-variables",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#the-model-with-k-independent-variables",
    "title": "02-Multivariate Regression",
    "section": "",
    "text": "Implementation (R)Present results(Math)\n\n\nModel\n\\[\\begin{align}\n  y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\n\\end{align}\\]\nMean independence assumption?\n\\(\\beta_{OLS}\\) (OLS estimators of \\(\\beta\\)s) is unbiased if,\n\\[\\begin{align}\n    E[u|x_1,x_2,\\dots,x_k]=0\n\\end{align}\\]\nVerbally: this condition would be satisfied if the error term is uncorrelated wtih any of the independent variables, \\(x_1,x_2,\\dots,x_k\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhen you are asked to present regression results in assignments or your final paper, use the msummary() function from the modelsummary package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nOLS\nFind the combination of \\(\\beta\\)s that minimizes the sum of squared residuals\nSo,\nDenoting the collection of \\(\\widehat{\\beta}\\)s as \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n    Min_{\\theta} \\sum_{i=1}^n \\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\widehat{\\beta_k} x_{k,i}) \\Big]^2\n\\end{align}\\]\nFind the FOCs by partially differentiating the objective function (sum of squared residuals) wrt each of \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n  \\sum_{i=1}^n(y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]\nOr more succinctly,\n\\[\\begin{align}\n  \\sum_{i=1}^n \\widehat{u}_i = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#unbiasedness-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#unbiasedness-of-ols-estimators",
    "title": "02-Multivariate Regression",
    "section": "Unbiasedness of OLS Estimators",
    "text": "Unbiasedness of OLS Estimators\n\nConditions (four)Perfect CollinearityEndogeneity (Definition)\n\n\nOLS estimators of multivariate models are unbiased under  certain  conditions\nCondition 1\nYour model is correct (Assumption \\(MLR.1\\))\n\nCondition 2\nRandom sampling (Assumption \\(MLR.2\\))\n\nConditions 3\nNo perfect collinearity (Assumption \\(MLR.3\\))\n\nConditions 4\nZero Conditional Mean (Assumption \\(MLR.4\\))\n\\[\\begin{align}\n  E[u|x_1,x_2,\\dots,x_k]=0 \\;\\;\\mbox{(Assumption MLR.4)}\n\\end{align}\\]\nIf all the conditions \\(MLR.1\\sim MLR.4\\) are satisfied, OLS estimators are unbiased.\n\\[\n\\def\\ehb{E[\\ha\n```{=tex}t{\\beta}_j]}\n\\begin{align}\n  \\ehb=\\beta_j \\;\\; ^\\forall j=0,1,\\dots,k\n\\end{align}\n```\n\\]\n\n\nNo Perfect Collinearity (\\(MLR.3\\))\nAny variable cannot be a linear function of the other variables\nExample (silly)\n\\[\\begin{align}\n  wage = \\beta_0 + \\beta_1 educ + \\beta_2 (3\\times educ) + u\n\\end{align}\\]\n( More on this later when we talk about dummy variables)\n\n\n\\[E[u|x_1,x_2,\\dots,x_k] = f(x_1,x_2,\\dots,x_k) \\ne 0\\]\nWhat could cause endogeneity problem? + functional form misspecification\n\\[\\begin{align}\n  wage = & \\beta_0 + \\beta_1 log(x_1) + \\beta_2 x_2 + u_1 \\;\\;\\mbox{(true)}\\\\\n  wage = & \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u_2 (=log(x_1)-x_1) \\;\\; \\mbox{(yours)}\n\\end{align}\\]\n\nomission of variables that are correlated with any of \\(x_1,x_2,\\dots,x_k\\) ( more on this soon )\n other sources of enfogeneity later"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#variance-of-ols-estimators",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#variance-of-ols-estimators",
    "title": "02-Multivariate Regression",
    "section": "Variance of OLS estimators",
    "text": "Variance of OLS estimators\n\nHomoskedasticityVarianceEstimating \\(\\sigma^2\\)\n\n\nCondition 5\nError term is homoeskedastic (Assumption \\(MLR.5\\))\n\\[\\begin{align}\nVar(u|x_1,\\dots,x_k)=\\sigma^2\n\\end{align}\\]\n\n\nUnder conditions \\(MLR.1\\) through \\(MLR.5\\), conditional on the sample values of the independent variables,\n\\[\\begin{align}\n    Var(\\widehat{\\beta}_j)= \\frac{\\sigma^2}{SST_j(1-R^2_j)},\n\\end{align}\\]\nwhere \\(SST_j= \\sum_{i=1}^n (x_{ji}-\\bar{x_j})^2\\) and \\(R_j^2\\) is the R-squared from regressing \\(x_j\\) on all other independent variables including an intercept. ( We will revisit this equation)\n\n\nJust like uni-variate regression, you need to estimate \\(\\sigma^2\\) if you want to estimate the variance (and standard deviation) of the OLS estimators.\nuni-variate regression\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-2}\n\\end{align}\\]\nmulti-variate regression\nA model with \\(k\\) independent variables with intercept.\n\\[\\begin{align}\n  \\widehat{\\sigma}^2=\\sum_{i=1}^N \\frac{\\widehat{u}_i^2}{n-(k+1)}\n\\end{align}\\]\nYou solved \\(k+1\\) simultaneous equations to get \\(\\widehat{\\beta}_j\\) \\((j=0,\\dots,k)\\). So, once you know the value of \\(n-k-1\\) of the residuals, you know the rest.\nThe  estimator  of the variance of the OLS estimator is therefore\n\\[\\begin{align}\n\\widehat{Var{\\widehat{\\beta}_j}} = \\frac{\\widehat{\\sigma}^2}{SST_j(1-R^2_j)}\n\\end{align}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#frischwaughlovell-theorem",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#frischwaughlovell-theorem",
    "title": "02-Multivariate Regression",
    "section": "Frisch–Waugh–Lovell Theorem",
    "text": "Frisch–Waugh–Lovell Theorem\nConsider the following simple model,\n\\[\\begin{align}\n  y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\beta_3 x_{3,i} + u_i\n\\end{align}\\]\nSuppose you are interested in estimating only \\(\\beta_1\\).\nLet’s consider the following two methods,\nMethod 1: Regular OLS\nRegress \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\) with an intercept to estimate \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) at the same time (just like you normally do)\nMethod 2: 3-step\n\nregress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\nregress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\nregress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\) \\((\\widehat{u}_y=\\alpha_1 \\widehat{u}_{x_1}+v_3)\\)\n\nFrisch-Waugh–Lovell theorem\nMethods 1 and 2 produces the same coefficient estimate on \\(x_1\\)\n\\[\\widehat{\\beta_1} = \\widehat{\\alpha_1}\\]"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#partialing-out-interpretation-from-method-2",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#partialing-out-interpretation-from-method-2",
    "title": "02-Multivariate Regression",
    "section": "Partialing out Interpretation from Method 2",
    "text": "Partialing out Interpretation from Method 2\nStep 1\nRegress \\(y\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_y\\)\n\n\\(\\widehat{u}_y\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(y\\)\n\nStep 2\nRegress \\(x_1\\) on \\(x_2\\) and \\(x_3\\) with an intercept and get residuals, which we call \\(\\widehat{u}_{x_1}\\)\n\n\\(\\widehat{u}_{x_1}\\) is void of the impact of \\(x_2\\) and \\(x_3\\) on \\(x_1\\)\n\nStep 3\nRegress \\(\\widehat{u}_y\\) on \\(\\widehat{u}_{x_1}\\), which produces an estimte of \\(\\beta_1\\) that is identical to that you can get from regressin \\(y\\) on \\(x_1\\), \\(x_2\\), and \\(x_3\\)"
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html#interpretation",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html#interpretation",
    "title": "02-Multivariate Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nRegressing \\(y\\) on all explanatory variables \\((x_1\\), \\(x_2\\), and \\(x_3)\\) in a multivariate regression is as if you are looking at the impact of a single explanatory variable with the effects of all the other effects partiled out\nIn other words, including variables beyond your variable of interest lets you  control for (remove the effect of)  other variables, avoiding confusing the impact of the variable of interest with the impact of other variables."
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "Home",
    "section": "",
    "text": "Visit the course website here"
  },
  {
    "objectID": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html",
    "href": "lectures/01-univariate-introduction/01-3-univariate_ols_property.html",
    "title": "01-3: Univariate Regression: OLS Small Sample Property",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nSmall sample property (in general)OLS?\n\n\nWhat is an estimator?\n\nA function of data that produces an estimate (actual number) of a parameter of interest once you plug in actual values of data\nOLS estimators: \\(\\widehat{\\beta}_1=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\)\n\n\nWhat is small sample property?\nProperties that hold whatever the size of observation (small or large) is  prior to  obtaining actual estimates (before getting data)\n\nPut more simply: what can you expect from the estimators before you actually get data and obtain estimates?\nDifference between small sample property and the algebraic properties we looked at earlier?\n\n\n\nOLS is just  a  way of using available information to obtain estimates. Does it have desirable properties? Why are we using it?\n\nUnbiasedness\nEfficiency\n\nAs it turns out, OLS is a very good way of using available information!!\n\n\n\n\n\n\n\n\nUnbiasednessUnbiased v.s. BiasedUnbiasedness of OLS estimatorsConditions(Math)\n\n\nWhat does  unbiased  even mean?\nLet’s first look at a simple problem of estimating the expected value of a single variable (\\(x\\)) as a start.\n\nA good estimator of an expected value of a random variable is sample mean: \\(\\frac{1}{n}\\sum_i^n x_i\\)\n\nR code: Sample Mean\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\nDirection\n\n\n\nTry running the codes multiple times and feel the tendency of the estimates.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nUnder  certain conditions , OLS estimators are unbiased. That is,\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\nE[\\widehat{\\beta}_1]=E\\Big[\\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn  (x_i-\\bar{x})^2}\\Big]=\\beta_1\n\\]\n(We do not talk about unbiasedness of \\(\\widehat{\\beta}_0\\) because we are almost never interested in the intercept. Given the limited time we have, it is not worthwhile talking about it)\n\n\n\nSLR.1SLR.2SLR.3SLR.4\n\n\n\n\n\n\n\n\nLinear in Parameters\n\n\n\nIn the population model, the dependent variable, \\(y\\), is related to the independent variable, \\(x\\), and the error (or disturbance), \\(u\\), as\n\\[\ny=\\beta_0+\\beta_1 x+u\n\\]\n\n\nNote: This definition is from the textbook by Wooldridge\n\n\n\n\n\n\n\n\nRandom sampling\n\n\n\nWe have a random sample of size \\(n\\), \\({(x_i,y_i):i=1,2,\\dots,n}\\), following the population model.\n\n\nNon-random sampling\n\nExample: You observe income-education data only for those who have income higher than \\(\\$25K\\)\nBenevolent and malevolent kinds:\n\n exogenous  sampling\n endogenous  sampling\n\nWe discuss this in more detial later\n\n\n\n\n\n\n\n\n\nVariation in covariates\n\n\n\nThe sample outcomes on \\(x\\), namely, \\({x_i,i=1,\\dots,n}\\), are not all the same value.\n\n\n\n\n\n\n\n\n\n\nZero conditional mean\n\n\n\nThe error term \\(u\\) has an expected value of zero given any value of the explanatory variable. In other words,\n\\[\nE[u|x]=0  \n\\]\n\n\nAlong with random sampling condition, this implies that\n\\[\nE[u_i|x_i]=0\n\\]\n\n\n\n\n\n\nRoughly speaking\n\n\n\nThe independent variable \\(x\\) is not correlated with \\(u\\).\n\n\n\n\n\n\n\n\n\\[\n\\def\\sumn{\\sum_{i=1}^{n}}\n\\begin{aligned}\n\\widehat{\\beta}_1 = & \\frac{\\sumn (x_i-\\bar{x})(y_i-\\bar{y})}{\\sumn (x_i-\\bar{x})^2}  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{\\sumn (x_i-\\bar{x})^2} \\;\\; \\Big[\\mbox{because }\\sumn (x_i-\\bar{x})\\bar{y}=0\\Big]\\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})y_i}{SST_x} \\;\\;\\Big[\\mbox{where,}\\;\\; SST_x=\\sumn (x_i-\\bar{x})^2\\Big]  \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})(\\beta_0+\\beta_1 x_i+u_i)}{SST_x} \\\\\\\\\n= & \\frac{\\sumn (x_i-\\bar{x})\\beta_0 +\\sumn \\beta_1(x_i-\\bar{x})x_i+\\sumn(x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = & \\frac{\\sumn  (x_i-\\bar{x})\\beta_0 + \\beta_1 \\sumn  (x_i-\\bar{x})x_i+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\mbox{Since } & \\sumn  (x_i-\\bar{x})=0\\;\\; \\mbox{and}\\\\\n    & \\sumn  (x_i-\\bar{x})x_i=\\sumn  (x_i-\\bar{x})^2=SST_x,\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n  \\widehat{\\beta}_1 = \\frac{\\beta_1 SST_x+\\sumn (x_i-\\bar{x})u_i}{SST_x}\n  = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\n\\end{aligned}\n\\]\n\\[\\widehat{\\beta}_1 = \\beta_1+(1/SST_x)\\sumn (x_i-\\bar{x})u_i\\]\nTaking, expectation of \\(\\widehat{\\beta}_1\\) conditional on \\(\\mathbf{x}=\\{x_1,\\dots,x_n\\}\\),\n\\[\n\\begin{align}\n\\Rightarrow E[\\widehat{\\beta}_1|\\mathbf{x}] = & E[\\beta_1|\\mathbf{x}]+E[(1/SST_x)\\sumn (x_i-\\bar{x})u_i|\\mathbf{x}]  \\\\\\\\\n= & \\beta_1 + (1/SST_x)\\sumn (x_i-\\bar{x}) E[u_i|\\mathbf{x}]\n\\end{align}\n\\]\nSo, if condition 4 \\((E[u_i|\\mathbf{x}]=0)\\) is satisfied,\n\\[\n\\def\\Ex{E_{x}}\n\\begin{align}\nE[\\widehat{\\beta}_1|x] = & \\beta_1 \\\\\\\\\n\\Ex[\\widehat{\\beta}_1|x] = & E[\\widehat{\\beta}_1] = \\beta_1\n\\end{align}\n\\]\n\n\n\n\n\n\n\n\nGood empiricistsUnbiasedness of OLS estimatorsLet me reiterate\n\n\nGood empiricists\n\nhave ability to judge if the above conditions are satisfied for the particular context you are working on\nhave ability to correct (if possible) for the problems associated with the violations of any of the above conditions\nknows the context well so you can make appropriate judgments\n\n\n\nReconsider the following example\n\\[\nprice=\\beta_0+\\beta_1\\times lotsize + u\n\\]\n\n\\(price\\): house price (USD)\n\\(lotsize\\): lot size\n\\(u\\): error term (everything else)\n\nQuestions\n\nWhat’s in \\(u\\)?\nDo you think \\(E[u|x]\\) is satisfied? In other words (roughly speaking), is \\(u\\) uncorrelated with \\(x\\)?\n\n\n\n\nUnbiasedness property of OLS estimators says  nothing  about the estimate that we obtain for a given sample\nIt is always possible that we could obtain an unlucky sample that would give us a point estimate far from \\(\\beta_1\\), and we can never know for sure whether this is the case.\n\n\n\n\n\n\n\n\n\nIntroductionVariance (example)Variance of OLS estimatorWhat affects \\(Var(\\widehat{\\beta}_{OLS})\\)?\n\n\n\nOLS estimators are random variables because \\(y\\), \\(x\\), and \\(u\\) are random variables (this just means that you do not know the estimates until you get samples).\nVariance of OLS estimators is a measure of how much spread in estimates (realized values) you will get.\nWe let \\(Var(\\widehat{\\beta}_{OLS})\\) denote the variance of the OLS estimators of \\(\\beta_0\\) and \\(\\beta_1\\).\n\n\n\nConsider two estimators of \\(E[x]\\):\n\\[\\begin{align}\n\\theta_{smart} = & \\frac{1}{n} \\sum x_i  \\;\\;(n=1000) \\\\\\\\\n\\theta_{naive} = & \\frac{1}{10} \\sum x_i\n\\end{align}\\]\nVariance of the estimators\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n(True) Variance of the OLS Estimator\nIf \\(Var(u|x)=\\sigma^2\\) and the four conditions (we used to prove unbiasedness of the OLS estimator) are satisfied,\n\\[\n\\begin{align}\n  Var(\\widehat{\\beta}_1) = \\frac{\\sigma^2}{\\sumn (x_i-\\bar{x})^2}=\\frac{\\sigma^2}{SST_x}\n\\end{align}\n\\]\n\n(TRUE) Standard Error of the OLS Estimator\nThe standard error of the the OLS estimator is just a square root of the variance of the OLS estimator. We use \\(se(\\widehat{\\beta}_1)\\) to denote it.\n\\[\n\\begin{aligned}\n  se(\\widehat{\\beta}_1) = \\sqrt{Var(\\widehat{\\beta}_1)} = \\frac{\\sigma}{\\sqrt{SST_x}}\n\\end{aligned}\n\\]\n\n\nVariance of the OLS estimators\n\\[Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\]\n\nWhat can you learn from this equation?\n\nthe variance of OLS estimators is smaller (larger) if the variance of error term is smaller (larger)\nthe greater (smaller) the variation in the covariate \\(x\\), the smaller (larger) the variance of OLS estimators\n\nif you are running experiments, spread the value of \\(x\\) as much as possible\nyou will rarely have this luxury\n\n\n\n\n\n\n\n\n\n\nNature of error termVisualizationHouse Price ExampleGauss-Markov TheoremNotes\n\n\nHomoskedasticity\nThe error \\(u\\) has the same variance give any value of the covariate \\(x\\) \\((Var(u|x)=\\sigma^2)\\)\n\nHeterokedasticity\nThe variance of the error \\(u\\) differs depending on the value of \\(x\\) \\((Var(u|x)=f(x))\\)\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\nGauss-Markov Theorem\n\n\n\nUnder conditions \\(SLR.1\\) through \\(SLR.4\\) and the  homoskedasticity  assumption (\\(SLR.5\\)), OLS estimators are the best linear unbiased estimators (BLUEs)\n\n\n\nIn other words,\nNo other  unbiased linear  estimators have smaller variance than the OLS estimators (desirable efficiency property of OLS)\n\n\n\nWe do  NOT  need the homoskedasticity condition to prove that OLS estimators are unbiased\nIn most applications, homoskedasticity condition is not satisfied, which has important implications on:\n\nestimation of variance (standard error) of OLS estimators\nsignificance test\n\n\n( A lot more on this issue later )\n\n\n\n\n\n\n\n\nWhy?ProblemProposalAlgebraic property of OLSUnbiased estimatorR code(Math)\n\n\nOnce you estimate \\(Var(\\widehat{\\beta}_1|x)\\), you can test the statistical significance of \\(\\widehat{\\beta}_1\\) (More on this later)\n\n\n\n\n\nWe know that \\(Var(\\widehat{\\beta}_1|x) = \\sigma^2/SST_x\\).\nYou can calculate \\(SST_x\\) because \\(x\\) is observable. So, as long as we know \\(\\sigma^2\\), which is \\(Var(u)\\) (the variance of the error term), then we know \\(Var(\\widehat{\\beta}_1|x)\\).\nSince \\(Var(u_i)=\\sigma^2=E[u_i^2] \\;\\; \\Big( Var(u_i)\\equiv E[u_i^2]-E[u_i]^2 \\Big)\\), \\(\\frac{1}{n}\\sum_{i=1}^n u_i^2\\) is an unbiased estimator of \\(Var(u_i)\\)\nUnfortunately, we don’t observe \\(u_i\\) (error)\n\n\n\n\n\n\n\n\nBut,\n\n\n\nWe observe \\(\\widehat{u_i}\\) (residuals)!! Can we use residuals instead?\n\n\n\n\n\n\n\n\n\n\n\nWe know \\(E[\\widehat{u}_i-u_i]=0\\) (see a mathematical proof here), so, why don’t we use \\(\\widehat{u}_i\\) (observable) in place of \\(u_i\\) (unobservable)?\n\n\n\n\n\n\n\n\nProposed Estimator of \\(\\sigma^2\\)\n\n\n\n\\(\\frac{1}{n}\\sum_{i=1}^n \\widehat{u}_i^2\\)\n\n\n\n\n\n\nUnfortunately, \\(\\frac{1}{n}\\sum_{i=1}^n \\hat{u}_i^2\\) is a biased estimator of \\(\\sigma^2\\)\n\n\n\n\nFOCs of the minimization problem OLS solves\n\\[\\begin{align}\n    \\sum_{i=1}^n \\widehat{u}_i=0\\;\\; \\mbox{and}\\;\\; \\sum_{i=1}^n x_i\\widehat{u}_i=0\\notag\n\\end{align}\\]\n\nthis means that once you know the value of \\(n-2\\) residuals, you can find the value of the other two by solving the above equations\nso, it’s almost as if you have \\(n-2\\) value of residuals instead of \\(n\\)\n\n\n\n\n\n\n\n\n\nUnbiased estimator of \\(\\sigma^2\\)\n\n\n\n\\(\\widehat{\\sigma}^2=\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2\\) \\(\\;\\;\\;\\;\\;\\;\\)(\\(E[\\frac{1}{n-2}\\sum_{i=1}^n \\widehat{u}_i^2]=\\sigma^2\\))\n\n\n\nHereafter we use \\(\\widehat{Var(\\widehat{\\beta}_1)}\\) to denote the variance of the OLS estimator \\(\\widehat{\\beta}_j\\), and it is defined as\n\\[\n\\widehat{Var(\\widehat{\\beta}_1)} = \\widehat{\\sigma}^2/SST_x\n\\]\n\nSince \\(se(\\widehat{\\beta}_1)=\\sigma/\\sqrt{SST_x}\\), the natural estimator of \\(se(\\widehat{\\beta_1})\\) ( standard error of \\(\\widehat{\\beta}_1\\) ) is\n\\[\n\\widehat{se(\\widehat{\\beta}_1)} =\\sqrt{\\widehat{\\sigma}^2}/\\sqrt{SST_x},\n\\]\n\n\n\n\n\n\nNote\n\n\n\nLater, we use \\(\\widehat{se(\\hat{\\beta_1})}\\) for testing.\n\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nError and Residual\n\\[\\begin{align}\n    y_i = \\beta_0+\\beta_1 x_i + u_i \\\\\n    y_i = \\hat{\\beta}_0+\\hat{\\beta}_1 x_i + \\hat{u}_i\n\\end{align}\\]\nResiduals as unbiased estimators of error\n\\[\\begin{align}\n  \\hat{u}_i & = y_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\hat{u}_i & = \\beta_0+\\beta_1 x_i + u_i -\\hat{\\beta}_0-\\hat{\\beta}_1 x_i \\\\\n  \\Rightarrow \\hat{u}_i -u_i & = (\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i \\\\\n  \\Rightarrow E[\\hat{u}_i -u_i] & = E[(\\beta_0-\\hat{\\beta}_0)+(\\beta_1-\\hat{\\beta}_1) x_i]=0\n\\end{align}\\]"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Home",
    "section": "",
    "text": "library(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(data.table)\n\nWarning: package 'data.table' was built under R version 4.2.3\n\n\n\nAttaching package: 'data.table'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\ndates_data &lt;- \n  data.table(\n    date = seq( as.Date(\"2024-01-01\"), as.Date(\"2024-12-31\"), by=\"+1 day\")\n  ) %&gt;%\n  .[, day := weekdays(date)]\n\nw_month_data &lt;- dates_data[month(date) == 8,]\n\nimp_dates &lt;- \n  rep(NA, nrow(w_month_data))\n\nlecture_ind &lt;- \n  w_month_data[, day %in% c(\"Monday\", \"Wednesday\") & day(date) &gt;= 19]\n\nlab_ind &lt;- \n  w_month_data[, day %in% c(\"Friday\") & day(date) &gt;= 19 ]\n\n# Add the events to the desired days\nimp_dates[lecture_ind] &lt;- \"Lecture\"\nimp_dates[lab_ind] &lt;- \"Lab\"\n\n# Create a calendar with a legend\n\ntemp &lt;- \n  calendR::calendR(\n    year = 2024, \n    month = 8, \n    special.days = imp_dates,\n    special.col = c(\n      \"lightcyan2\", \"tan\"),\n    weeknames = c(\n      \"Mon\", \"Tue\", \"Wed\", \"Thu\",\n      \"Fri\", \"Sat\", \"Sun\"\n    ),\n    mbg.col = \"15\",\n    months.col = \"blue\",\n    legend.pos = \"bottom\"\n  )"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall\n\n\n\nMona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#instructor",
    "href": "syllabus.html#instructor",
    "title": "Personnel",
    "section": "",
    "text": "Taro Mieno + Email: tmieno2@unl.edu + Office: 209 Filley Hall"
  },
  {
    "objectID": "syllabus.html#ta",
    "href": "syllabus.html#ta",
    "title": "Personnel",
    "section": "",
    "text": "Mona Mosavi: + Email: mmousavi2@huskers.unl.edu"
  },
  {
    "objectID": "syllabus.html#lectures-and-labs",
    "href": "syllabus.html#lectures-and-labs",
    "title": "Personnel",
    "section": "Lectures and Labs:",
    "text": "Lectures and Labs:\n\nLectures: MW 3:00 - 4:30 PM\nLabs: F 1:00 - 2:30 PM"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Personnel",
    "section": "Office Hours:",
    "text": "Office Hours:\nWednesday, 10:00 to 11:30 pm or by appointment"
  },
  {
    "objectID": "syllabus.html#lecture",
    "href": "syllabus.html#lecture",
    "title": "Personnel",
    "section": "Lecture",
    "text": "Lecture\n\nIntroduction to econometrics\nSimple univariate regression\nMonte Carlo Simulation\nMultivariate regression\nMulti-collinearity and omitted variable\nHypothesis Testing\nHetereoskedasticity and robust standard error estimation\nClustered error and bootstrap\nFunctional form and scaling\nDummy variables\nPanel data methods\nCausal Inference\nCausal Inference\nLimited dependent variable"
  },
  {
    "objectID": "syllabus.html#computer-lab-r",
    "href": "syllabus.html#computer-lab-r",
    "title": "Personnel",
    "section": "Computer Lab (R)",
    "text": "Computer Lab (R)\n\nIntroduction to R\nRmarkdown\nData wrangling\nData visualization\nResearch Flow and R"
  },
  {
    "objectID": "LabLectures.html",
    "href": "LabLectures.html",
    "title": "Home",
    "section": "",
    "text": "Visit here for R lab lecture notes."
  },
  {
    "objectID": "lectures/02-multivariate-regression/multivariate_regression.html",
    "href": "lectures/02-multivariate-regression/multivariate_regression.html",
    "title": "02-Multivariate Regression",
    "section": "",
    "text": "Please enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nBi-variate vs. Uni-variateExampleExampleModel (general)\n\n\nUnivariate\nThe most important assumption \\(E[u|x] = 0\\) (zero conditional mean) is almost always violated (unless you data comes from randomized experiments) because all the other variables are sitting in the error term, which can be correlated with \\(x\\).\n\nMultivariate\nMore independent variables mean less factors left in the error term, which makes the endogeneity problem  less severe\n\n\n\\[\\begin{align}\n  \\mbox{Bi-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u_2 \\\\\n  \\mbox{Uni-variate}\\;\\; wage = & \\beta_0 + \\beta_1 educ + u_1 (=u_2+\\beta_2 exper)\n\\end{align}\\]\nWhat’s different?\n\nbi-variate: able to measure the effect of education on wage,  holding experience fixed  because experience is modeled explicitly ( We say \\(exper\\) is controlled for. )\nuni-variate: \\(\\widehat{\\beta_1}\\) is biased unless experience is uncorrelated with education because experience was in error term\n\n\n\nThe impact of per student spending (expend) on standardized test score (avgscore) at the high school level\n\\[\\begin{align}\navgscore= & \\beta_0+\\beta_1 expend + u_1 (=u_2+\\beta_2 avginc) \\notag \\\\\navgscore= & \\beta_0+\\beta_1 expend +\\beta_2 avginc + u_2 \\notag\n\\end{align}\\]\n\n\nMore generally,\n\\[\\begin{align}\n  y=\\beta_0+\\beta_1 x_1 + \\beta_2 x_2 + u\n\\end{align}\\]\n\n\\(\\beta_0\\): intercept\n\\(\\beta_1\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\\(\\beta_2\\): measure the change in \\(y\\) with respect to \\(x_1\\), holding other factors fixed\n\n\n\n\n\n\n\n\n\nUni-variate v.s. Bi-variateMean independence condition: example\n\n\nUni-variate\nFor \\(y = \\beta_0 + \\beta_1x + u\\),\n\\(E[u|x]=0\\)\n\nBi-variate\nFor \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + u\\),\n\nMathematically: \\(E[u|x_1,x_2]=0\\)\nVerbally: for any values of \\(x_1\\) and \\(x_2\\), the expected value of the unobservables is zero\n\n\n\nIn the following wage model,\n\\[\\begin{align*}\nwage = & \\beta_0 + \\beta_1 educ + \\beta_2 exper + u\n\\end{align*}\\]\nMean independence condition is\n\\[\\begin{align}\n  E[u|educ,exper]=0\n\\end{align}\\]\nVerbally:\nThis condition would be satisfied if innate ability of students is on average unrelated to education level and experience.\n\n\n\n\n\n\n\n\nImplementation (R)Present results(Math)\n\n\nModel\n\\[\\begin{align}\n  y=\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + u\n\\end{align}\\]\nMean independence assumption?\n\\(\\beta_{OLS}\\) (OLS estimators of \\(\\beta\\)s) is unbiased if,\n\\[\\begin{align}\n    E[u|x_1,x_2,\\dots,x_k]=0\n\\end{align}\\]\nVerbally: this condition would be satisfied if the error term is uncorrelated wtih any of the independent variables, \\(x_1,x_2,\\dots,x_k\\).\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nWhen you are asked to present regression results in assignments or your final paper, use the msummary() function from the modelsummary package.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\nOLS\nFind the combination of \\(\\beta\\)s that minimizes the sum of squared residuals\nSo,\nDenoting the collection of \\(\\widehat{\\beta}\\)s as \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n    Min_{\\theta} \\sum_{i=1}^n \\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\widehat{\\beta_k} x_{k,i}) \\Big]^2\n\\end{align}\\]\nFind the FOCs by partially differentiating the objective function (sum of squared residuals) wrt each of \\(\\widehat{\\theta} (=\\{\\widehat{\\beta_0},\\widehat{\\beta_1},\\dots,\\widehat{\\beta_k}\\})\\),\n\\[\\begin{align}\n  \\sum_{i=1}^n(y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\Big[ y_i-(\\widehat{\\beta_0}+\\widehat{\\beta_1} x_{1,i} + \\widehat{\\beta_2} x_{2,i} + \\dots + \\beta_k x_{k,i}) \\Big]= & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]\nOr more succinctly,\n\\[\\begin{align}\n  \\sum_{i=1}^n \\widehat{u}_i = & 0 \\;\\; (\\widehat{\\beta}_0) \\\\\n  \\sum_{i=1}^n x_{i,1}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_1) \\\\\n  \\sum_{i=1}^n x_{i,2}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_2) \\\\\n  \\vdots \\\\\n  \\sum_{i=1}^n x_{i,k}\\widehat{u}_i = & 0  \\;\\; (\\widehat{\\beta}_k) \\\\\n\\end{align}\\]"
  }
]