---
author: Sarah Fuchs
format:
  html:
    number-sections: true
    number-depth: 1
    theme: flatly
    toc: true
execute:
  echo: true
  message: false
  warning: false
---


```{r setup_not_remove, echo=FALSE, include = F}
library(knitr)
library(here)
# opts_knit$set(root.dir = here("AssignmentsInstructor/Assignment_2"))
# setwd(here("AssignmentsInstructor/Assignment_2"))
opts_chunk$set(
  echo = TRUE,
  comment = NA,
  message = FALSE,
  warning = FALSE,
  tidy = FALSE,
  #--- figure related ---#
  fig.align = "center",
  fig.width = 5,
  fig.height = 4
  # dev='pdf'
)
```

# Instruction


Please submit your assignment (both the qmd and the resulting html files) using the following [Dropbox submission request link](https://www.dropbox.com/request/QoEGkBxBSn1xvmYhJbe1). Name your qmd file using the following convention: `lastname_assignment_2.qmd`. For example, it would be `Mieno_assignment_2.qmd` for me. 

I do not accept any hand-written answers (you will get score of zero for that problem). Leave all the R codes and results in your answer (where appropriate) for the computer exercises. If your R codes are not provided, you will get score of 0 for that question.

You are encouraged to discuss the problems with your classmates. However, I would recommend that you do so after you try all the problems yourself. You would be simply doing disservice to yourself if you resort to your friends' help immediately, depriving you of important opportunities to think carefully about what we have learned and understand them through solving the problems. 

------



<!-- Non-computer Exercises -->
# Non-computer Exercises

<!-- Problem 3 -->
## Problem 3 (Chapter 3)

The following model is a simplified version of the multiple regression model used by Biddle and Hamermesh (1990) to study the trade-off between time spent sleeping and working and to look at other factors affecting sleep:
$$sleep = \beta_0 + \beta_1 totwrk + \beta_2 educ + \beta_3 age + u$$
where $sleep$ and $totwrk$ (total work) are measured in minutes per week and $educ$ and $age$
are measured in years.

------

(i) If adults trade off sleep for work, what is the sign of $\beta_1$?



Answer: Problem 3, Part (i)

For the adult working population, there is a negative relationship between the variables of $sleep$ and $totwrk$ (total work).  If an adult trades off sleep for work, $\beta_1$ has a negative sign.

------

(ii) What signs do you think $\beta_2$ and $\beta_3$ will have?



Answer: Problem 3, Part (ii)

It's hard for me to decide what sign would be best for $\beta_3$ (age), because on one hand you could say the older you are (as an adult) the less you will be working (moving toward retirement or actively in retirement), therefore there are greater opportunities to increase time spent sleeping (positive $\beta_3$).  However, I've also been told that in retirement years and beyond (as a person gets older) some people find it difficult to sleep, therefore the more years of age an adult has, the less time sleeping experienced (negative $\beta_3$).  I can kind of see the same type of situation for educ (going either negative or positive), but I would suppose that the more years of education a person might have may result in more work responsibilities leading to more work and less sleep ($\beta_2$ being negative).


------

(iii) Using the data in SLEEP75.RAW, the estimated equation is
$$\hat{sleep} = 3,638.25 -0.148 totwrk -11.13 educ + 2.20 age $$
$$ n=706, \;\; R^2=0.113$$
If someone works five more hours per week, by how many minutes is sleep predicted to fall? Is this a large trade-off?



Answer: Problem 3, Part (iii)

$\hat{sleep} = 3,638.25 -0.148(5*60) -11.13 educ + 2.20 age$

                        -44.4 min/wk
                        
Sleep is predicted to fall 44.4 minutes/week when the adult works 5 more hours/week.

Is this a large trade-off?
Overall, I don't feel like this is a large trade-off.  If you spread the 44.4 minutes across 7 days/week, the decrease in amount of sleep averages only 6.34 less minutes/night (not exceptionally noticeable in my opinion.)


------

(iv) Discuss the sign and magnitude of the estimated coefficient on $educ$.



Answer: Problem 3, Part (iv)

The estimated coefficient on $educ$ tells us that for each additional year of education that the individual has, that person's amount of sleep will decrease by 11.13 minutes/week (this is a negative sign on $educ$.)  I kind of feel that the value of $\beta_2$, paired with the fact that we are working with a linear model, gives a bit of an unrealistic result.  I don't feel that an additional year of education would affect sleep minutes per week to that large of an extent and just continue to go up and up (or in this case less and less minutes of sleep per week.)  Eventually, with enough added years of education, a worker would end up with almost no sleep with this type of linear model.



------

(v) Would you say $totwrk$, $educ$, and $age$ explain much of the variation in $sleep$? What other factors might affect the time spent sleeping? Are these likely to be correlated
with $totwrk$? What happens to the coefficient estimator on $totwrk$ if the other factors are indeed correlated with $torwrk$?



Answer: Problem 3, Part (v)

While $totwrk$, $educ$, and $age$ may be able to explain many reasons for the variation in $sleep$, I believe there are still many other unobservables in the error term.  This statement is corroborated by the estimated equation in part(iii) showing an $R^2 = 0.113$ (model not explained exceptionally well.)  Variables suchs as kids and their activities, one's health, or an individual's lifestyle ("going out", watching TV, or hobbies) could all affect whether you get more or less sleep.  These other items, which are being held in the error term, could very easily be correlated with $totwrk$.  If not only an employer paid job, but also a task such as laundry can be considered as work in $totwrk$, kids would be correlated with $totwrk$, because more kids/people in a household means more laundry (work), which could mean less sleep (trust me, this is true as I am waiting for a load to get done at 12:30 a.m. right now, so I can put it in the dryer, so my kid has a pair of jeans to wear to school in the morning.)  If these other factors are indeed correlated with $totwrk$ but left out of the model, there could be a bias on the impact that $totwrk$ has on $sleep$.  Depending on the signs that these other factors would have in a model, the effect of $totwrk$ on $sleep$ could be inflated or deflated.

------

<!-- Problem 7 -->
## Problem 7 (Chapter 3)

Which of the following can cause OLS estimators to be biased? Explain why or why not?

(i) Heteroskedasticity.
(ii) Omitting an important variable.
(iii) A sample correlation coefficient of .95 between two independent variables both included in the model.

Answer: Problem 7, Part (i-iii)

(i) Heteroskedasticity does not cause OLS estimators to be biased.
(ii) Omitting an important variable can cause OLS estimators to be biased when $x_1$ and $x_2$ are correlated.
(iii) A correlation coefficient of .95 between two independent variables both included in the model can cause a great amount of bias, since the correlation is very high (almost perfect positive correlation), although it would be worse if one variable was left out of the model.

------

## Problem 9 (Chapter 3)

The following equation describes the median housing price in a community in terms of amount of pollution ($nox$ for nitrous oxide) and the average number of rooms in houses in the community ($rooms$):

$$log(price)=\beta_0+\beta_1log(nox)+\beta_2 rooms+u$$

------

(i) What are the probable signs of $\beta_1$ and $\beta_2$? What is the interpretation of $\beta_1$? Explain.


Answer: Problem 9, Part (i)

Probable signs: $\beta_1: -$ (negative)         $\space \space \beta_2: +$  (positive)

$\beta_1$ tells us the elasticity of $price$ (the $y$ variable - price of a house) with respect to $nox$ (the $x_1$ variable - the amount of pollution in the community where the house is located).  For example, with a 1 increment increase (1% increase) in $nox$ (the amount of pollution), $price$ (the price of the house in question) will increase by the value of $\beta_1$, ceteris paribus (everything else fixed.)

------

(ii) Why might $nox$ [or more precisely, log($nox$)] and $rooms$ be negatively correlated? If this is the case, does the simple regression of log($price$) on log($nox$) produce an upward or a downward biased estimator of $\beta_1$? (Hint: look at the slides on the direction of omitted variable bias)



Answer: Problem 9, Part (ii)

Generally, the more rooms a house has the higher the sale price or worth of that house.  From this statement, we can guess that $log(nox)$ and $rooms$ may be negatively correlated, because it doesn't make sense that somebody would want to pay for or build a large house (higher price) with many rooms (that adds to the price of a house) to live in an area with a lot of pollution.)  Unfortunately, people who can't afford much and have a house with far less rooms would most likely be the ones subject to a higher pollution (higher $nox$, less $rooms$).

If $log(nox)$ and $rooms$ indeed are negatively correlated [$corr(nox, rooms)<0$], and a simple regression is run with only $log(price)$ on $log(nox)$, leaving the positive coefficient of $rooms$ out of the regression would most likely give an upward bias of estimator $\beta_1$ (lacks the positive effect of $rooms$ resulting in an inflated negative impact of $log(nox)$). 


------

(iii) Using the data in HPRICE2.RAW, the following equations were estimated:
$$\hat{log(price)}=11.71-1.043log(nox) $$
$$\hat{log(price)}=9.23-0.718log(nox)+ 0.306 rooms$$
Is the relationship between the simple and multiple regression estimates of the elasticity of price with respect to $nox$ what you would have predicted, given your answer in part (ii)? Does this mean that $-0.718$ is definitely closer to the true elasticity than $-1.043$?

Answer: Problem 9, Part (iii)

The differences between the simple and multiple regression estimates is what I would expect.  By adding $rooms$ to the model (if $log(nox)$ and $rooms$ are correlated) this takes away the bias of omitting the variable $rooms$ (as seen in the first estimate) which possibly brings the estimate for $log(nox)$ down to a more unbiased/un-inflated true elasticity.

------

<!-- Computer Exercises -->

# Computer Exercises



## Problem C6 (Chapter 3)

Use the data set in **wage2.rds** for this problem. As usual, be sure all of the following regressions contain an intercept (Hint: this question is about omitted variable bias). 

------



------

(i) Run a simple regression of $IQ$ on $educ$ to obtain the slope coefficient, say, $\tilde{\sigma}_1$.



Answer: Problem C6, Part (i)

```{r}
library(tidyverse)
library(haven)
library(fixest)
library(modelsummary)

getwd()
setwd("C:/Users/msfuc/OneDrive/Documents/Econometrics/Assignment 2")



wage2 <- readRDS("wage2.rds")

wage2

```
```{r}
library(modelsummary)


#--run OLS, IQ~educ--#
wage2_reg <- fixest::feols(IQ~educ, data = wage2)

wage2_reg

msummary(wage2_reg)

```
#--slope coefficient equals--#

$\tilde\sigma_1 = 3.53383$

------


(ii) Run the simple regression of log($wage$) on $educ$, and obtain the slope coefficient, $\tilde{\beta}_1$.



Answer: Problem C6, Part (ii)

```{r}

#--run OLS, log(wage)~educ--#

wage2_reg2 <- fixest::feols(log(wage)~educ, data = wage2)

wage2_reg2

msummary(wage2_reg2)

```

#--slope coefficient equals--#

$\tilde\beta_1 = 0.059839$


------

(iii) Run the multiple regression of log($wage$) on $educ$ and $IQ$, and obtain the slope coefficients, $\hat{\beta}_1$ and $\hat{\beta}_2$, respectively.



Answer: Problem C6, Part (iii)

```{r}
#--run OLS, log(wage)~educ + IQ--#

wage2_reg3 <- fixest::feols(log(wage)~educ + IQ, data = wage2)

wage2_reg3

msummary(wage2_reg3)


```

#--slope coefficient equals--#

$\hat\beta_1 = 0.039120$       

$\hat\beta_2 = 0.005863$

------

(iv) Verify that $\tilde{\beta}_1=\hat{\beta}_1$ + $\hat{\beta}_2\tilde{\sigma}_1$.




Answer: Problem C6, Part (iv)

$E[\beta_1] = \beta_1 + \beta_2\sigma_1$ where $\beta_2\sigma_1$ is the bias

```{r}
0.039120+(0.005863*3.53383)

```

------

(v) What does $\hat{\beta}_2\tilde{\sigma}_1$ represent?



Answer: Problem C6, Part (v)

$\hat\beta_2\tilde\sigma_1$ represents the bias when $x_2$ ($IQ$) is omitted and $y$ is only regressed on $x_1$ ($educ$).


## Problem C8 (Chapter 3)

Use the data in **discrim.dta** to answer this question (there are NAs in the data set, so remove them first). These are ZIP code–level data on prices for various items at fast-food restaurants, along with characteristics of the zip code population, in New Jersey and Pennsylvania. The idea is to see whether fast-food restaurants charge higher prices in areas with a larger concentration of blacks.

Here are the definitions of variables:

+ $prpblck$: proportion of black people
+ $income$: median income (in $\$$)
+ $lincome$: logged $income$
+ $psoda$: average price of soda (in $\$$)
+ $prppov$: proportion of people in poverty
+ $hseval$: median value of owner-occupied housing units (in $\$$)
+ $hseval$: logged $hseval$

--------

#Preparing Data
```{r}
discrim <- haven::read_dta("discrim.dta")

class(discrim)

#--select variables and remove NA--#

discrim_data <-
  dplyr::select(discrim, state, prpblck, income, lincome, psoda, prppov, hseval, lhseval)%>%
  na.omit(discrim)

discrim_data


```
  


--------

(i) Find the average values of $prpblck$ and $income$ in the sample, along with their standard deviations. Use `dplyr::summarize()` function.



Answer: Problem C8, Part (i)

```{r}

#--mean and std dev: prpblck, income--#

discrim_data %>%
  dplyr::group_by(state) %>%
  dplyr::summarize(
    mean_prpblck = mean(prpblck, na.rm = TRUE),
    std_dev_prpblck = sd(prpblck, na.rm = TRUE),
    mean_income = mean(income, na.rm = TRUE),
    std_dev_income = sd(income, na.rm = TRUE)
  )


```


--------

(ii) Consider a model to explain the price of soda, $psoda$, in terms of the proportion of the population that is black and median income:
$$psoda=\beta_0+\beta_1 prpblck + \beta_2 income +u $$
Estimate this model by OLS and report the results, including the sample size and R-squared. Interpret the coefficient on $prpblck$. Do you think it is economically large?



Answer: Problem C8, Part (ii)

```{r}

#--OLS regression--#

discrim_data_reg <- fixest::feols(psoda ~ prpblck + income, data = discrim_data)

discrim_data_reg

msummary(discrim_data_reg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```

$psoda = 0.9563196 + (0.1149882 * prpblck) + (0.0000016 * income) + u$

$observations = 401$  $\space \space R^2 = 0.059518$

Interpretation of $prpblck$:  for every 1 increment increase in $prpblck$ (proportion/percentage of black people in the population) the price of soda ($psoda$) will increase by approximately 11% (or $0.11).

Economically large?: I do feel like an $0.11 increase in the price of a soda per 1% increase in a specific population demographic seems large, but I'm also cheap and have a hard time paying $10+ for a case of Coke or Pepsi. -- I remember the "good ol' days" when a 24 pack was half that price!

<br>


--------

(iii) Compare the estimate from part (ii) with the simple regression estimate from $psoda$ on $prpblck$. Is the discrimination effect larger or smaller when you control for income?


Answer: Problem C8, Part (iii)
```{r}
simplereg <- lm(psoda ~ prpblck, data = discrim_data)
simplereg
msummary(simplereg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```

```{r}

#--simple regression, psoda on prpblck--#

discrim_simplereg <- fixest::feols(psoda ~ prpblck, data = discrim_data)

discrim_simplereg

msummary(discrim_simplereg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```

When controlling for income, the discrimination effect is larger (0.1149882 vs 0.064927; about 11% vs 6%).

<br>

--------

(iv) A model with a constant price elasticity with respect to income may be more appropriate. Report estimates of the model:
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) +u $$
If $prpblck$ increases by .20 (20 percentage points), what is the estimated percentage change in $psoda$?


Answer: Problem C8, Part (iv)

```{r}

#-- price elasticity wrt income--#

price_elasticity_reg <- fixest::feols(log(psoda) ~ log(income) + prpblck, data = discrim_data)

price_elasticity_reg

msummary(price_elasticity_reg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```

The regression results for a model with constant price elasticity with respect to income gives a $\beta_1$ estimate of 0.121580, which tells us that for every 1 unit change in $prpblck$, there will be a 12.158% change in $psoda$.  Therefore, if $prpblck$ changes by .2 units (20 percentage points rather than 1 whole unit), the percentage change in $psoda$ is equal to $(\beta_1 * \Delta prpblck) = (.121580*.20) = \uparrow by\space\space .024316 \space (\approx 2.4\% \space increase)$.


<br>


--------

(v) Now add the variable $prppov$ to the regression in part (iv). What happens to $\beta_{prpblck}$?


Answer: Problem C8, Part (v)


```{r}

#--regression, add poverty--#

discrim_poverty_reg <- fixest::feols(log(psoda) ~ prpblck + lincome + prppov, data = discrim_data)

discrim_poverty_reg

msummary(discrim_poverty_reg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")
```

What happens to $\beta_{prpblck}$?    When the poverty level of the community was taken into consideration and variable $prppov$ is added to the model, the magnitude which $prpblck$ affects the price of soda changes.  Now, $\beta prpblck$ decreases from 0.121580 previously to 0.072807 now that $prppov$ was added to the model.  Therefore, $prpblck$ has less of an affect on $psoda$ than it did before.



<br>


--------

(vi) Find the correlation between log($income$) and $prppov$. Is it roughly what you expected?

Answer: Problem C8, Part (vi)

```{r}
#--correlation between log(income) and prppov--#

cor(discrim_data$lincome, discrim_data$prppov)

```
The correlation between $log(income)$ and $prppov$ is -0.8402069.  This makes sense to me and is about what I would have expected, since the greater the percentage of people living in poverty within a community, the lower the  median income in the area will be. 

<br>


--------

(vii) Evaluate the following statement: "Because log($income$) and $prppov$ are so highly correlated, they have no business being in the same regression."

Answer: Problem C8, Part (vii)

Since they ARE so highly correlated, they SHOULD be in the same regression, because otherwise you will experience omitted variable bias.

------

## Problem C9 (Chapter 4)

Use the data in **discrim.dta** (you have imported this data already in the previous problem) to answer this question.

------

(i) Use OLS to estimate the model
$$log(psoda)=\beta_0+\beta_1 prpblck + \beta_2 log(income) + \beta_2 prppov + u $$
and report the results in the usual form. Is $\beta_1$ statistically different from zero at the $5\%$ level against a two-sided alternative? What about at the $1\%$ level?

Answer: Problem C9, Part (i)

```{r}

#--regression, psoda on prpblck, log(income), prppov--#

discrim_povertyC9_reg <- feols(log(psoda) ~ prpblck + lincome + prppov, data = discrim_data)

discrim_povertyC9_reg

msummary(discrim_povertyC9_reg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```

#--calculate 2-sided t-value for prpblck--#

beta_prpblck <- 
  discrim_povertyC9_reg %>%
  filter(term == "prpblck") %>%
  pull(estimate)

se_prpblck <-
  discrim_povertyC9_reg %>%
  filter(term == "prpblck") %>%
  pull(std.error)

t_valueC9 <- (beta_prpblck) / (se_prpblck)

t_valueC9

#I am aware that this code is not working.  I can't figure out how to extract the "estimate" and "std.error" for "prpblck" from my regression results (apparently when using fixest). For this reason, I am manually "grabbing" these numbers to calculate the t-value for the 2-sided analysis.#


```{r}
.072807/.030676


```
t-value = 2.373419


```{r}
#--degrees of freedom--#

df_C9reg <- degrees_freedom(discrim_povertyC9_reg, "resid")

df_C9reg

```
Degrees of Freedom = 397

```{r}

#--2-sided critical value--#

critical_valueC9neg <- qt(0.025, df = df_C9reg)
critical_valueC9neg

critical_valueC9pos <- qt(0.975, df = df_C9reg)
critical_valueC9pos

```

Critical Value at 5% Level = +/-1.965957

Absolute value of $t_-valueC9$ equals 2.373419, which is greater than the critical value of $critical_-valueC9$ which is +/-1.965957, so we do reject the null hypothesis, therefore $\beta_1$ is statistically significant at the 5% level.

```{r}

#--critical value, 1% level--#

critical_value1percentneg <- qt(0.005, df = df_C9reg)
critical_value1percentneg

critical_value1percentpos <- qt(0.995, df = df_C9reg)
critical_value1percentpos


```
Critical Value at 1% Level = +/-2.58827

Absolute value of $t_-valueC9$ equals 2.373419, which is less than the critical value of $critical_-value1percent$ which is +/-2.58827, so we do not reject the null hypothesis, therefore $\beta_1$ is not statistically significant at the 1% level.


------

(ii) What is the correlation between log($income$) and $prppov$ (use **cor()** function in R to find correlation coefficient)? Is each variable statistically significant at the $5\%$ level? Report the two-sided p-values.


Answer: Problem C9, Part (ii)

```{r}
#--correlation between log(income) and prppov--#

cor(discrim_data$lincome, discrim_data$prppov)

```
Correlation between $log(income)$ and $prppov$ = -0.8402069

```{r}

#t-value log(income)--#

0.136955/0.026755

#t-value prppov--#

0.380360/0.132790


```
t-value for $log(income)$:  5.118856

t-value for $prppov$:  2.864372

$log(income)$: t-value > critical value (5.118856 > 1.96) reject null, is statistically significant at 5% level

$prppov$:  t-value > critical value (2.864372 > 1.96) reject null, is statistically significant at 5% level

------

(iii) To the regression in part (i), add the variable log($hseval$). Interpret its coefficient and report the two-sided $p$-value for $H_0$: $\beta_{log(hseval)}=0$

Answer: Problem C9, Part (iii)

```{r}

#--regression, psoda on prpblck, log(income), prppov, hseval--#

discrim_lhseval_reg <- fixest::feols(log(psoda) ~ prpblck + lincome + prppov + lhseval, data = discrim_data)

discrim_lhseval_reg

msummary(discrim_lhseval_reg, stars = TRUE, gof_omit = "IC, Log, Adj, F, Pseudo, Within")

```
```{r}
#--degrees of freedom, beta-4--#

df_C9hsevalreg <- degrees_freedom(discrim_lhseval_reg, "resid")

df_C9hsevalreg

```
Degrees of Freedom = 396

```{r}

#--2-sided critical value--#

critical_valueC9hsevalneg <- qt(0.025, df = df_C9hsevalreg)
critical_valueC9hsevalneg

critical_valueC9hsevalpos <- qt(0.975, df = df_C9hsevalreg)
critical_valueC9hsevalpos

```


```{r}

#t-value log(hseval)--#

0.121306/0.017684


```
t-value for $log(hseval)$:  6.859647

$log(hseval)$: t-value > critical value (6.859647 > 1.96) reject null, is statistically significant at 5% level (and at 1% level)

The coefficient for $log(hseval)$ equals 0.121306.  When $log(hseval)$ changes by 1%, then $\beta_{log(hseval)}$ will change $log(psoda)$ by 1%. If $log(hseval)$ changes by 1%, then $\beta_{log(hseval)}$ (0.121306) will change $log(psoda)$ by approximately .12%.

<br>



------

(iv) In the regression in part (iii), what happens to the individual statistical significance of log($income$) and $prppov$? Are these variables jointly significant? (Compute a p-value.) What do you make of your answers? Use `car::linearHypothesis()` to run the F-test.


Answer: Problem C9, Part (iv)


```{r}

#--individual statistical significance, log(income), prppov--#

#--t_value_log(income)--#

-0.052990/0.037526

#--t_value_prppov--#

0.052123/0.134499

```
t-value for $log(income)$:  -1.412088

t-value for $prppov$:  0.3875345

When adding $log(hseval)$ to the model, the individual statistical significance of both $log(income)$ and $prppov$ changes with the t-values for both decreasing ($log(income): -1.412088, \space prppov: 0.387534$).  With this change, neither variable individually is statistically significant at the 5% level and the null hypothesis IS NOT rejected.

Are these variables jointly significant?

```{r}

#--library "car" package--#

library(car)



#--fit multiple linear regression model--#

hseval_reg <- lm(log(psoda) ~ prpblck + lincome + prppov + lhseval, data = discrim_data)

#--p-value test--#

pvalue_test <- linearHypothesis(hseval_reg, "lincome + prppov=0")

pvalue_test

```

Joint p-value:  0.9958

The joint p-value is 0.9958, which indicates that we fail to reject the null hypothesis.  This leaves us with the conclusion that $log(income)$ and $prppov$ are not jointly statistically significant at the 5% level.


<br>

------

(v) Given the results of the previous regressions, which one would you report as most reliable in determining whether the racial makeup of a zip code influences local fast-food prices?


Answer: Problem C9, Part (v)

While both of the last two regressions yielded insignificant results (at the 5% significance level), and the joint statistical results were somewhat closer to significance than the first regression results, I would probably not use the results where variables were tested for joint statistical significance. This is because even though two variables may show joint statistical significance, this doesn't mean that all variables in the model will be significant.



Wow. This assignment was long and a bit challenging...I guess that's how a graduate class is supposed to be.
